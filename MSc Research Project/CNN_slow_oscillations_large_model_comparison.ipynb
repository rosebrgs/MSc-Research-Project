{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53912b6f-0832-4e91-b068-27583c9a7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista\n",
    "import ipywidgets\n",
    "import ipyevents\n",
    "import pyvistaqt\n",
    "import yasa\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import stft\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "\n",
    "import pywt\n",
    "import cv2\n",
    "\n",
    "SEED = 15\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7692555-7c48-480d-a32f-6fe1f0fbcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1dd96-bb6c-47de-9642-b7df2c49f262",
   "metadata": {},
   "source": [
    "## CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23cf70f-8364-46b1-9471-022dd77508a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_downsampled(input_shape=(300,1)):\n",
    "\n",
    "    # linear embedding layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Three convolutional blocks (like having three pattern detectors)\n",
    "\n",
    "    # First convolution block, kernel size of 5\n",
    "    padded1 = tf.keras.layers.ZeroPadding1D(padding=2)(input_layer)\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=10, kernel_size=5, strides=1, padding='valid')(padded1)\n",
    "    # each filter learns a different type of short-time feature\n",
    "    # stride of 1, moves one step at a time\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv1)\n",
    "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # K = 2\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "\n",
    "    # Second convolution block, kernel size of 11\n",
    "    padded2 = tf.keras.layers.ZeroPadding1D(padding=5)(input_layer)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=10, kernel_size=11, strides=1, padding='valid')(padded2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv2)\n",
    "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "\n",
    "    # Third convolution block, kernel size of 21\n",
    "    padded3 = tf.keras.layers.ZeroPadding1D(padding=10)(input_layer)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=10, kernel_size=21, strides=1, padding='valid')(padded3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv3)\n",
    "    conv3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "\n",
    "    # Concatenate the outputs of all blocks\n",
    "    concatenated = tf.keras.layers.Concatenate()([conv1, conv2, conv3])\n",
    "\n",
    "    # GRU Layer\n",
    "    gru = tf.keras.layers.GRU(64)(concatenated)\n",
    "\n",
    "    # Fully connected (dense) layer\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(gru)\n",
    "    # add a Dropout layer to prevent overfitting\n",
    "    #dense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    # Two softmax outputs for dual-task classification\n",
    "    #output_task1 = tf.keras.layers.Dense(2, activation='softmax', name='task1')(dense)\n",
    "    #output_task2 = tf.keras.layers.Dense(2, activation='softmax', name='task2')(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create the model\n",
    "    #model = tf.keras.models.Model(inputs=input_layer, outputs=[output_task1, output_task2])\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    #model.compile(optimizer='adam', loss={'task1': 'categorical_crossentropy', 'task2': 'categorical_crossentropy'}, metrics={'task1': 'accuracy', 'task2': 'accuracy'})\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174edab5-b401-4d56-9832-dcd6bfa3ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_filtered():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered = tf.keras.Input(shape=(300, 1), name='filtered_input') \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered = conv_branch(input_filtered)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered = tf.keras.layers.GRU(64)(branch_filtered)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4fbf7e-a06b-4f40-8092-8255df739ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_freq():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered = tf.keras.Input(shape=(300, 1), name='filtered_input')\n",
    "    input_stft = tf.keras.Input(shape=(13, 1), name='stft_input')  \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered = conv_branch(input_filtered)\n",
    "    branch_stft = conv_branch(input_stft)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered = tf.keras.layers.GRU(64)(branch_filtered)\n",
    "    gru_stft = tf.keras.layers.GRU(64)(branch_stft)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered, gru_stft])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered, input_stft], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74010b3-521c-44b2-b095-9b56d318cf3d",
   "metadata": {},
   "source": [
    "## Slow oscillation detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3c8bcc-7f78-48f3-99ab-9bdf104e5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_slow_oscillations_times(combined_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "    #signs = np.sign(current_data)\n",
    "    #pos_to_neg = np.where((signs[:-1] > 0) & (signs[1:] < 0))[0]\n",
    "    # detect +1 to -1\n",
    "    #neg_to_pos = np.where((signs[:-1] <  0) & (signs[1:] > 0))[0]\n",
    "    # detect -1 to +1\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices = []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings)-1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "\n",
    "        # find the negative to positive crossing in between\n",
    "        #mid_crossings = neg_to_pos[(neg_to_pos > start_idx) & (neg_to_pos < end_idx)]\n",
    "\n",
    "        #if len(mid_crossings) != 1:\n",
    "            #continue\n",
    "\n",
    "        #mid_idx = mid_crossings [0]\n",
    "\n",
    "        #duration = (end_idx - start_idx) / sfreq\n",
    "        #if not (0.8 <= duration <= 2.0):\n",
    "  \n",
    "        \n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    # keep lowest negative peaks (under the 25th percentile)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "    # keep largest peak-to-peak amplitude (over 75th percentile)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation\n",
    "\n",
    "def detect_slow_oscillations_peaks(combined_raw, do_filter=True, do_downsample=True, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    slow_oscillations_peaks = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices =  []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings) - 1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            slow_oscillations_peaks.append((negative_peak, positive_peak))\n",
    "\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cea46-853b-4428-bb10-633e542db5d6",
   "metadata": {},
   "source": [
    "## Epochs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d211ff33-e059-4360-97e5-861d5104e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_length_epochs(raw, duration=3.0, overlap=0.0, preload=True, reject_by_annotation=False):\n",
    "\n",
    "    return mne.make_fixed_length_epochs(\n",
    "        raw,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=preload,\n",
    "        reject_by_annotation=reject_by_annotation\n",
    "    )\n",
    "# function mne.make_fixed_length_epochs takes into account the sampling frequency of the data\n",
    "\n",
    "\n",
    "def label_so_epochs_moderate(epochs, so_starts, so_ends, epoch_length_sec=3.0):\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "    for so_start, so_end in zip(so_starts, so_ends):\n",
    "        so_duration = so_end - so_start\n",
    "        required_overlap = 0.5 * so_duration  \n",
    "        # only label 1 if epoch contains 80% of the SO duration\n",
    "\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "\n",
    "            # Calculate overlap between SO and epoch\n",
    "            overlap_start = max(so_start, epoch_start)\n",
    "            overlap_end = min(so_end, epoch_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration >= required_overlap:\n",
    "                epoch_labels[i] = 1\n",
    "\n",
    "    return epoch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fae6b-c6f3-4d41-9d9e-94bf8b069555",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4b0a26-649b-4b9b-ba8f-6b6e306e667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for split_1...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 32700095 =    180.000 ... 65400.190 secs\n",
      "Ready.\n",
      "Reading 0 ... 32610095  =      0.000 ... 65220.190 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 7845026 =   2940.000 ... 15690.052 secs\n",
      "Ready.\n",
      "Reading 0 ... 6375026  =      0.000 ... 12750.052 secs...\n",
      "Loaded train and test data for split_1\n",
      "Loading data for split_2...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32985091 =   2940.000 ... 65970.182 secs\n",
      "Ready.\n",
      "Reading 0 ... 31515091  =      0.000 ... 63030.182 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 7560030 =    180.000 ... 15120.060 secs\n",
      "Ready.\n",
      "Reading 0 ... 7470030  =      0.000 ... 14940.060 secs...\n",
      "Loaded train and test data for split_2\n",
      "Loading data for split_3...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32475107 =   2940.000 ... 64950.214 secs\n",
      "Ready.\n",
      "Reading 0 ... 31005107  =      0.000 ... 62010.214 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 300000 ... 8280014 =    600.000 ... 16560.028 secs\n",
      "Ready.\n",
      "Reading 0 ... 7980014  =      0.000 ... 15960.028 secs...\n",
      "Loaded train and test data for split_3\n",
      "Loading data for split_4...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32100090 =   2940.000 ... 64200.180 secs\n",
      "Ready.\n",
      "Reading 0 ... 30630090  =      0.000 ... 61260.180 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 825000 ... 9180031 =   1650.000 ... 18360.062 secs\n",
      "Ready.\n",
      "Reading 0 ... 8355031  =      0.000 ... 16710.062 secs...\n",
      "Loaded train and test data for split_4\n",
      "Loading data for split_5...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 31650104 =   2940.000 ... 63300.208 secs\n",
      "Ready.\n",
      "Reading 0 ... 30180104  =      0.000 ... 60360.208 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 165000 ... 8970017 =    330.000 ... 17940.034 secs\n",
      "Ready.\n",
      "Reading 0 ... 8805017  =      0.000 ... 17610.034 secs...\n",
      "Loaded train and test data for split_5\n"
     ]
    }
   ],
   "source": [
    "# for 5-fold validation\n",
    "# load the all the files needed that were pre-processed before\n",
    "# from train_1_raw and test_1_raw to train_5_raw and test_5_raw\n",
    "split_files = {\n",
    "    f'split_{i}': {\n",
    "        'train': fr\"C:\\EEG DATA\\combined_sets\\train_{i}_large_raw.fif\",\n",
    "        'test': fr\"C:\\EEG DATA\\combined_sets\\test_{i}_large_raw.fif\"\n",
    "    } for i in range(1, 6) \n",
    "}\n",
    "\n",
    "raw_splits = {}\n",
    "for split_name, files in split_files.items():\n",
    "    print(f\"Loading data for {split_name}...\")\n",
    "    try:\n",
    "        train_raw = mne.io.read_raw_fif(files['train'], preload=True)\n",
    "        test_raw = mne.io.read_raw_fif(files['test'], preload=True)\n",
    "        raw_splits[split_name] = {'train': train_raw, 'test': test_raw}\n",
    "        print(f\"Loaded train and test data for {split_name}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found for {split_name}: {e}\")\n",
    "        # error in case the file does not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {split_name}: {e}\")\n",
    "        # errors in case not loading data\n",
    "\n",
    "        # error statements useful if running this notebook on another laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9b435-9aff-48ce-a113-3ee82572009d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bbfa1-7748-42fa-9987-266c1992dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Split: split_1 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: raw on split_1 ---\n",
      "Training data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Test data shapes: (4250, 300, 1), labels=(4250,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 190ms/step - accuracy: 0.7542 - loss: 0.5254 - val_accuracy: 0.8816 - val_loss: 0.3326\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 182ms/step - accuracy: 0.8243 - loss: 0.3936 - val_accuracy: 0.8954 - val_loss: 0.3727\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 182ms/step - accuracy: 0.8037 - loss: 0.4252 - val_accuracy: 0.8680 - val_loss: 0.3014\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 181ms/step - accuracy: 0.8689 - loss: 0.2961 - val_accuracy: 0.8811 - val_loss: 0.2765\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8741 - loss: 0.2783 - val_accuracy: 0.8848 - val_loss: 0.2564\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8717 - loss: 0.2749 - val_accuracy: 0.8997 - val_loss: 0.2347\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 188ms/step - accuracy: 0.8867 - loss: 0.2556 - val_accuracy: 0.9055 - val_loss: 0.2259\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 180ms/step - accuracy: 0.8901 - loss: 0.2459 - val_accuracy: 0.9059 - val_loss: 0.2216\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.8942 - loss: 0.2404 - val_accuracy: 0.9101 - val_loss: 0.2160\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8979 - loss: 0.2355 - val_accuracy: 0.9128 - val_loss: 0.2124\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.9000 - loss: 0.2320 - val_accuracy: 0.9131 - val_loss: 0.2092\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.9010 - loss: 0.2285 - val_accuracy: 0.9151 - val_loss: 0.2063\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.9025 - loss: 0.2253 - val_accuracy: 0.9165 - val_loss: 0.2037\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.9033 - loss: 0.2223 - val_accuracy: 0.9177 - val_loss: 0.2011\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.9049 - loss: 0.2195 - val_accuracy: 0.9190 - val_loss: 0.1988\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.9069 - loss: 0.2163 - val_accuracy: 0.9197 - val_loss: 0.1965\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 180ms/step - accuracy: 0.9081 - loss: 0.2136 - val_accuracy: 0.9209 - val_loss: 0.1942\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 182ms/step - accuracy: 0.9107 - loss: 0.2108 - val_accuracy: 0.9218 - val_loss: 0.1918\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 182ms/step - accuracy: 0.9121 - loss: 0.2082 - val_accuracy: 0.9211 - val_loss: 0.1903\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.9136 - loss: 0.2058 - val_accuracy: 0.9234 - val_loss: 0.1895\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.2689, Accuracy: 0.8814\n",
      "F1 Score for raw on split_1: 0.5625\n",
      "Precision for raw on split_1: 0.8244\n",
      "Recall for raw on split_1: 0.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-Jul-25 08:12:34 | WARNING | From C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model: raw_and_filtered on split_1 ---\n",
      "Training data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1)}, labels=(21740,)\n",
      "Test data shapes: {'raw_input': (4250, 300, 1), 'filtered_input': (4250, 300, 1)}, labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 383ms/step - accuracy: 0.7698 - loss: 0.4950 - val_accuracy: 0.8268 - val_loss: 0.3416\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 366ms/step - accuracy: 0.8329 - loss: 0.3531 - val_accuracy: 0.8857 - val_loss: 0.2848\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 382ms/step - accuracy: 0.8412 - loss: 0.3292 - val_accuracy: 0.8818 - val_loss: 0.2490\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 380ms/step - accuracy: 0.8961 - loss: 0.2404 - val_accuracy: 0.9179 - val_loss: 0.1854\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 399ms/step - accuracy: 0.9053 - loss: 0.2237 - val_accuracy: 0.9328 - val_loss: 0.1586\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 394ms/step - accuracy: 0.9121 - loss: 0.2102 - val_accuracy: 0.9400 - val_loss: 0.1458\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 377ms/step - accuracy: 0.9179 - loss: 0.1973 - val_accuracy: 0.9453 - val_loss: 0.1373\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 376ms/step - accuracy: 0.9233 - loss: 0.1878 - val_accuracy: 0.9503 - val_loss: 0.1274\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 379ms/step - accuracy: 0.9266 - loss: 0.1812 - val_accuracy: 0.9558 - val_loss: 0.1168\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 368ms/step - accuracy: 0.9300 - loss: 0.1733 - val_accuracy: 0.9600 - val_loss: 0.1111\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 375ms/step - accuracy: 0.9322 - loss: 0.1665 - val_accuracy: 0.9621 - val_loss: 0.1078\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 360ms/step - accuracy: 0.9369 - loss: 0.1607 - val_accuracy: 0.9572 - val_loss: 0.1142\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 375ms/step - accuracy: 0.9375 - loss: 0.1610 - val_accuracy: 0.9414 - val_loss: 0.1473\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 372ms/step - accuracy: 0.9361 - loss: 0.1673 - val_accuracy: 0.9556 - val_loss: 0.1124\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 376ms/step - accuracy: 0.9408 - loss: 0.1544 - val_accuracy: 0.9611 - val_loss: 0.1061\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 377ms/step - accuracy: 0.9428 - loss: 0.1512 - val_accuracy: 0.9634 - val_loss: 0.1033\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 370ms/step - accuracy: 0.9438 - loss: 0.1475 - val_accuracy: 0.9678 - val_loss: 0.0956\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 370ms/step - accuracy: 0.9461 - loss: 0.1430 - val_accuracy: 0.9662 - val_loss: 0.0958\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 379ms/step - accuracy: 0.9457 - loss: 0.1436 - val_accuracy: 0.9662 - val_loss: 0.0950\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 378ms/step - accuracy: 0.9467 - loss: 0.1390 - val_accuracy: 0.9660 - val_loss: 0.0979\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.1764, Accuracy: 0.9261\n",
      "F1 Score for raw_and_filtered on split_1: 0.7646\n",
      "Precision for raw_and_filtered on split_1: 0.8870\n",
      "Recall for raw_and_filtered on split_1: 0.6719\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered_and_stft on split_1 ---\n",
      "Training data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1), 'stft_input': (21740, 13, 1)}, labels=(21740,)\n",
      "Test data shapes: {'raw_input': (4250, 300, 1), 'filtered_input': (4250, 300, 1), 'stft_input': (4250, 13, 1)}, labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 556ms/step - accuracy: 0.8196 - loss: 0.4170 - val_accuracy: 0.7339 - val_loss: 0.4367\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 418ms/step - accuracy: 0.8575 - loss: 0.3224 - val_accuracy: 0.8779 - val_loss: 0.3010\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 415ms/step - accuracy: 0.8731 - loss: 0.2842 - val_accuracy: 0.8908 - val_loss: 0.2510\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 415ms/step - accuracy: 0.8898 - loss: 0.2559 - val_accuracy: 0.9126 - val_loss: 0.2118\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 414ms/step - accuracy: 0.9013 - loss: 0.2367 - val_accuracy: 0.9151 - val_loss: 0.1929\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 412ms/step - accuracy: 0.9076 - loss: 0.2168 - val_accuracy: 0.9223 - val_loss: 0.1776\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 411ms/step - accuracy: 0.9146 - loss: 0.2060 - val_accuracy: 0.9342 - val_loss: 0.1594\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 417ms/step - accuracy: 0.9229 - loss: 0.1947 - val_accuracy: 0.9322 - val_loss: 0.1574\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 419ms/step - accuracy: 0.9267 - loss: 0.1873 - val_accuracy: 0.9457 - val_loss: 0.1319\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 419ms/step - accuracy: 0.9291 - loss: 0.1757 - val_accuracy: 0.9494 - val_loss: 0.1300\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 417ms/step - accuracy: 0.9352 - loss: 0.1661 - val_accuracy: 0.9485 - val_loss: 0.1278\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 423ms/step - accuracy: 0.9359 - loss: 0.1611 - val_accuracy: 0.9457 - val_loss: 0.1355\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 407ms/step - accuracy: 0.9381 - loss: 0.1561 - val_accuracy: 0.9483 - val_loss: 0.1294\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 425ms/step - accuracy: 0.9434 - loss: 0.1481 - val_accuracy: 0.9499 - val_loss: 0.1325\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 423ms/step - accuracy: 0.9436 - loss: 0.1419 - val_accuracy: 0.9478 - val_loss: 0.1332\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 423ms/step - accuracy: 0.9472 - loss: 0.1341 - val_accuracy: 0.9519 - val_loss: 0.1321\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.2223, Accuracy: 0.9153\n",
      "F1 Score for raw_and_filtered_and_stft on split_1: 0.7321\n",
      "Precision for raw_and_filtered_and_stft on split_1: 0.8410\n",
      "Recall for raw_and_filtered_and_stft on split_1: 0.6482\n",
      "\n",
      "--- Processing Split: split_2 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 31 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 31 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4980 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4980 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4980 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4980 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: raw on split_2 ---\n",
      "Training data shapes: (21010, 300, 1), labels=(21010,)\n",
      "Test data shapes: (4980, 300, 1), labels=(4980,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 254ms/step - accuracy: 0.7937 - loss: 0.4979 - val_accuracy: 0.8903 - val_loss: 0.3428\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 221ms/step - accuracy: 0.8067 - loss: 0.4018 - val_accuracy: 0.8810 - val_loss: 0.2836\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 217ms/step - accuracy: 0.8596 - loss: 0.3088 - val_accuracy: 0.8855 - val_loss: 0.2743\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 216ms/step - accuracy: 0.8742 - loss: 0.2864 - val_accuracy: 0.8977 - val_loss: 0.2696\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 219ms/step - accuracy: 0.8764 - loss: 0.2790 - val_accuracy: 0.8855 - val_loss: 0.2676\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 218ms/step - accuracy: 0.8780 - loss: 0.2774 - val_accuracy: 0.8877 - val_loss: 0.2738\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 211ms/step - accuracy: 0.8799 - loss: 0.2802 - val_accuracy: 0.8934 - val_loss: 0.2600\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 209ms/step - accuracy: 0.8803 - loss: 0.2675 - val_accuracy: 0.9031 - val_loss: 0.2507\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 221ms/step - accuracy: 0.8805 - loss: 0.2687 - val_accuracy: 0.9008 - val_loss: 0.2481\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 219ms/step - accuracy: 0.8829 - loss: 0.2621 - val_accuracy: 0.9039 - val_loss: 0.2449\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 222ms/step - accuracy: 0.8856 - loss: 0.2605 - val_accuracy: 0.9034 - val_loss: 0.2453\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 221ms/step - accuracy: 0.8905 - loss: 0.2552 - val_accuracy: 0.9086 - val_loss: 0.2347\n",
      "Epoch 13/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 209ms/step - accuracy: 0.8921 - loss: 0.2492 - val_accuracy: 0.9112 - val_loss: 0.2329\n",
      "Epoch 14/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 214ms/step - accuracy: 0.8928 - loss: 0.2491 - val_accuracy: 0.9136 - val_loss: 0.2264\n",
      "Epoch 15/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 213ms/step - accuracy: 0.8983 - loss: 0.2414 - val_accuracy: 0.9134 - val_loss: 0.2276\n",
      "Epoch 16/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 213ms/step - accuracy: 0.9003 - loss: 0.2397 - val_accuracy: 0.9139 - val_loss: 0.2230\n",
      "Epoch 17/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 214ms/step - accuracy: 0.9004 - loss: 0.2361 - val_accuracy: 0.9148 - val_loss: 0.2120\n",
      "Epoch 18/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 214ms/step - accuracy: 0.9024 - loss: 0.2305 - val_accuracy: 0.9150 - val_loss: 0.2149\n",
      "Epoch 19/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 216ms/step - accuracy: 0.9041 - loss: 0.2299 - val_accuracy: 0.9146 - val_loss: 0.2105\n",
      "Epoch 20/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 215ms/step - accuracy: 0.9051 - loss: 0.2263 - val_accuracy: 0.9172 - val_loss: 0.2081\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.2432, Accuracy: 0.8998\n",
      "F1 Score for raw on split_2: 0.7163\n",
      "Precision for raw on split_2: 0.8289\n",
      "Recall for raw on split_2: 0.6306\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered on split_2 ---\n",
      "Training data shapes: {'raw_input': (21010, 300, 1), 'filtered_input': (21010, 300, 1)}, labels=(21010,)\n",
      "Test data shapes: {'raw_input': (4980, 300, 1), 'filtered_input': (4980, 300, 1)}, labels=(4980,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 474ms/step - accuracy: 0.7983 - loss: 0.4683 - val_accuracy: 0.8889 - val_loss: 0.3510\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 388ms/step - accuracy: 0.8163 - loss: 0.4316 - val_accuracy: 0.8877 - val_loss: 0.2277\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 363ms/step - accuracy: 0.8701 - loss: 0.2740 - val_accuracy: 0.9143 - val_loss: 0.1866\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 352ms/step - accuracy: 0.8871 - loss: 0.2416 - val_accuracy: 0.9212 - val_loss: 0.1910\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 360ms/step - accuracy: 0.8929 - loss: 0.2351 - val_accuracy: 0.9243 - val_loss: 0.1743\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 361ms/step - accuracy: 0.8975 - loss: 0.2346 - val_accuracy: 0.9212 - val_loss: 0.1859\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 361ms/step - accuracy: 0.8981 - loss: 0.2247 - val_accuracy: 0.9269 - val_loss: 0.1670\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 348ms/step - accuracy: 0.9034 - loss: 0.2189 - val_accuracy: 0.9307 - val_loss: 0.1601\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 361ms/step - accuracy: 0.9059 - loss: 0.2152 - val_accuracy: 0.9341 - val_loss: 0.1537\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 372ms/step - accuracy: 0.9090 - loss: 0.2109 - val_accuracy: 0.9353 - val_loss: 0.1499\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 374ms/step - accuracy: 0.9101 - loss: 0.2075 - val_accuracy: 0.9365 - val_loss: 0.1473\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 367ms/step - accuracy: 0.9112 - loss: 0.2041 - val_accuracy: 0.9410 - val_loss: 0.1411\n",
      "Epoch 13/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 376ms/step - accuracy: 0.9132 - loss: 0.2002 - val_accuracy: 0.9424 - val_loss: 0.1383\n",
      "Epoch 14/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 377ms/step - accuracy: 0.9158 - loss: 0.1960 - val_accuracy: 0.9434 - val_loss: 0.1344\n",
      "Epoch 15/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 381ms/step - accuracy: 0.9179 - loss: 0.1929 - val_accuracy: 0.9434 - val_loss: 0.1314\n",
      "Epoch 16/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 384ms/step - accuracy: 0.9205 - loss: 0.1893 - val_accuracy: 0.9438 - val_loss: 0.1333\n",
      "Epoch 17/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 381ms/step - accuracy: 0.9213 - loss: 0.1864 - val_accuracy: 0.9453 - val_loss: 0.1308\n",
      "Epoch 18/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 388ms/step - accuracy: 0.9231 - loss: 0.1826 - val_accuracy: 0.9455 - val_loss: 0.1333\n",
      "Epoch 19/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 380ms/step - accuracy: 0.9251 - loss: 0.1792 - val_accuracy: 0.9465 - val_loss: 0.1349\n",
      "Epoch 20/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 376ms/step - accuracy: 0.9260 - loss: 0.1764 - val_accuracy: 0.9465 - val_loss: 0.1384\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.2294, Accuracy: 0.9050\n",
      "F1 Score for raw_and_filtered on split_2: 0.7078\n",
      "Precision for raw_and_filtered on split_2: 0.9242\n",
      "Recall for raw_and_filtered on split_2: 0.5736\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered_and_stft on split_2 ---\n",
      "Training data shapes: {'raw_input': (21010, 300, 1), 'filtered_input': (21010, 300, 1), 'stft_input': (21010, 13, 1)}, labels=(21010,)\n",
      "Test data shapes: {'raw_input': (4980, 300, 1), 'filtered_input': (4980, 300, 1), 'stft_input': (4980, 13, 1)}, labels=(4980,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2578s\u001b[0m 20s/step - accuracy: 0.7843 - loss: 0.4528 - val_accuracy: 0.8353 - val_loss: 0.3862\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 415ms/step - accuracy: 0.8472 - loss: 0.3259 - val_accuracy: 0.9034 - val_loss: 0.2430\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 352ms/step - accuracy: 0.8828 - loss: 0.2611 - val_accuracy: 0.9307 - val_loss: 0.1715\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 308ms/step - accuracy: 0.8899 - loss: 0.2410 - val_accuracy: 0.9222 - val_loss: 0.1846\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 291ms/step - accuracy: 0.8977 - loss: 0.2274 - val_accuracy: 0.9253 - val_loss: 0.1735\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 267ms/step - accuracy: 0.9045 - loss: 0.2184 - val_accuracy: 0.9317 - val_loss: 0.1652\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 271ms/step - accuracy: 0.9054 - loss: 0.2133 - val_accuracy: 0.9346 - val_loss: 0.1559\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 310ms/step - accuracy: 0.9107 - loss: 0.2058 - val_accuracy: 0.9346 - val_loss: 0.1522\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 291ms/step - accuracy: 0.9141 - loss: 0.1979 - val_accuracy: 0.9365 - val_loss: 0.1472\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 265ms/step - accuracy: 0.9184 - loss: 0.1905 - val_accuracy: 0.9386 - val_loss: 0.1457\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 270ms/step - accuracy: 0.9224 - loss: 0.1817 - val_accuracy: 0.9398 - val_loss: 0.1461\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 276ms/step - accuracy: 0.9277 - loss: 0.1728 - val_accuracy: 0.9403 - val_loss: 0.1439\n",
      "Epoch 13/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 285ms/step - accuracy: 0.9316 - loss: 0.1641 - val_accuracy: 0.9436 - val_loss: 0.1409\n",
      "Epoch 14/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 270ms/step - accuracy: 0.9343 - loss: 0.1545 - val_accuracy: 0.9410 - val_loss: 0.1463\n",
      "Epoch 15/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 277ms/step - accuracy: 0.9394 - loss: 0.1457 - val_accuracy: 0.9438 - val_loss: 0.1435\n",
      "Epoch 16/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 280ms/step - accuracy: 0.9429 - loss: 0.1364 - val_accuracy: 0.9462 - val_loss: 0.1417\n",
      "Epoch 17/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 260ms/step - accuracy: 0.9469 - loss: 0.1261 - val_accuracy: 0.9448 - val_loss: 0.1530\n",
      "Epoch 18/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 256ms/step - accuracy: 0.9517 - loss: 0.1178 - val_accuracy: 0.9426 - val_loss: 0.1702\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.2477, Accuracy: 0.9026\n",
      "F1 Score for raw_and_filtered_and_stft on split_2: 0.6955\n",
      "Precision for raw_and_filtered_and_stft on split_2: 0.9327\n",
      "Recall for raw_and_filtered_and_stft on split_2: 0.5546\n",
      "\n",
      "--- Processing Split: split_3 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 108 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 15 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 108 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 15 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Not setting metadata\n",
      "20670 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20670 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5320 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5320 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "20670 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20670 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5320 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5320 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: raw on split_3 ---\n",
      "Training data shapes: (20670, 300, 1), labels=(20670,)\n",
      "Test data shapes: (5320, 300, 1), labels=(5320,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 185ms/step - accuracy: 0.7921 - loss: 0.4960 - val_accuracy: 0.8609 - val_loss: 0.3822\n",
      "Epoch 2/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 179ms/step - accuracy: 0.8227 - loss: 0.3648 - val_accuracy: 0.8290 - val_loss: 0.3141\n",
      "Epoch 3/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.8333 - loss: 0.3179 - val_accuracy: 0.8696 - val_loss: 0.2893\n",
      "Epoch 4/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 160ms/step - accuracy: 0.8676 - loss: 0.2880 - val_accuracy: 0.8561 - val_loss: 0.2943\n",
      "Epoch 5/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.8737 - loss: 0.2672 - val_accuracy: 0.8556 - val_loss: 0.2922\n",
      "Epoch 6/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.8785 - loss: 0.2594 - val_accuracy: 0.8694 - val_loss: 0.2644\n",
      "Epoch 7/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.8826 - loss: 0.2522 - val_accuracy: 0.8628 - val_loss: 0.2747\n",
      "Epoch 8/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.8821 - loss: 0.2487 - val_accuracy: 0.8732 - val_loss: 0.2680\n",
      "Epoch 9/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 148ms/step - accuracy: 0.8838 - loss: 0.2475 - val_accuracy: 0.8788 - val_loss: 0.2589\n",
      "Epoch 10/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 150ms/step - accuracy: 0.8866 - loss: 0.2408 - val_accuracy: 0.8800 - val_loss: 0.2548\n",
      "Epoch 11/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.8881 - loss: 0.2365 - val_accuracy: 0.8834 - val_loss: 0.2562\n",
      "Epoch 12/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.8914 - loss: 0.2340 - val_accuracy: 0.8911 - val_loss: 0.2417\n",
      "Epoch 13/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.8947 - loss: 0.2293 - val_accuracy: 0.8921 - val_loss: 0.2411\n",
      "Epoch 14/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.8946 - loss: 0.2295 - val_accuracy: 0.8953 - val_loss: 0.2405\n",
      "Epoch 15/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.9008 - loss: 0.2222 - val_accuracy: 0.9076 - val_loss: 0.2223\n",
      "Epoch 16/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.9045 - loss: 0.2172 - val_accuracy: 0.9093 - val_loss: 0.2217\n",
      "Epoch 17/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9054 - loss: 0.2134 - val_accuracy: 0.9103 - val_loss: 0.2169\n",
      "Epoch 18/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.9095 - loss: 0.2068 - val_accuracy: 0.9119 - val_loss: 0.2115\n",
      "Epoch 19/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.9100 - loss: 0.2029 - val_accuracy: 0.9149 - val_loss: 0.2084\n",
      "Epoch 20/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.9116 - loss: 0.2005 - val_accuracy: 0.9078 - val_loss: 0.2225\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.2960, Accuracy: 0.8643\n",
      "F1 Score for raw on split_3: 0.7472\n",
      "Precision for raw on split_3: 0.6428\n",
      "Recall for raw on split_3: 0.8921\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered on split_3 ---\n",
      "Training data shapes: {'raw_input': (20670, 300, 1), 'filtered_input': (20670, 300, 1)}, labels=(20670,)\n",
      "Test data shapes: {'raw_input': (5320, 300, 1), 'filtered_input': (5320, 300, 1)}, labels=(5320,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 345ms/step - accuracy: 0.8092 - loss: 0.4623 - val_accuracy: 0.7794 - val_loss: 0.4353\n",
      "Epoch 2/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 290ms/step - accuracy: 0.8194 - loss: 0.3653 - val_accuracy: 0.8445 - val_loss: 0.3294\n",
      "Epoch 3/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 289ms/step - accuracy: 0.8732 - loss: 0.2712 - val_accuracy: 0.9095 - val_loss: 0.2037\n",
      "Epoch 4/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 274ms/step - accuracy: 0.8985 - loss: 0.2317 - val_accuracy: 0.9163 - val_loss: 0.1945\n",
      "Epoch 5/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 296ms/step - accuracy: 0.9020 - loss: 0.2181 - val_accuracy: 0.9323 - val_loss: 0.1683\n",
      "Epoch 6/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 295ms/step - accuracy: 0.8899 - loss: 0.2526 - val_accuracy: 0.9296 - val_loss: 0.1672\n",
      "Epoch 7/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 275ms/step - accuracy: 0.9046 - loss: 0.2077 - val_accuracy: 0.9352 - val_loss: 0.1616\n",
      "Epoch 8/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 275ms/step - accuracy: 0.9112 - loss: 0.2046 - val_accuracy: 0.9357 - val_loss: 0.1598\n",
      "Epoch 9/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 276ms/step - accuracy: 0.9137 - loss: 0.2008 - val_accuracy: 0.9390 - val_loss: 0.1539\n",
      "Epoch 10/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 287ms/step - accuracy: 0.9172 - loss: 0.1942 - val_accuracy: 0.9398 - val_loss: 0.1514\n",
      "Epoch 11/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 290ms/step - accuracy: 0.9179 - loss: 0.1922 - val_accuracy: 0.9267 - val_loss: 0.1756\n",
      "Epoch 12/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 278ms/step - accuracy: 0.9189 - loss: 0.1910 - val_accuracy: 0.9448 - val_loss: 0.1481\n",
      "Epoch 13/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 277ms/step - accuracy: 0.9209 - loss: 0.1884 - val_accuracy: 0.9386 - val_loss: 0.1561\n",
      "Epoch 14/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 279ms/step - accuracy: 0.9209 - loss: 0.1884 - val_accuracy: 0.9412 - val_loss: 0.1513\n",
      "Epoch 15/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 282ms/step - accuracy: 0.9217 - loss: 0.1813 - val_accuracy: 0.9461 - val_loss: 0.1407\n",
      "Epoch 16/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 276ms/step - accuracy: 0.9245 - loss: 0.1776 - val_accuracy: 0.9473 - val_loss: 0.1392\n",
      "Epoch 17/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 286ms/step - accuracy: 0.9259 - loss: 0.1738 - val_accuracy: 0.9424 - val_loss: 0.1493\n",
      "Epoch 18/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 274ms/step - accuracy: 0.9251 - loss: 0.1732 - val_accuracy: 0.9478 - val_loss: 0.1340\n",
      "Epoch 19/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 277ms/step - accuracy: 0.9289 - loss: 0.1667 - val_accuracy: 0.9490 - val_loss: 0.1287\n",
      "Epoch 20/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 270ms/step - accuracy: 0.9309 - loss: 0.1631 - val_accuracy: 0.9523 - val_loss: 0.1239\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.2508, Accuracy: 0.8959\n",
      "F1 Score for raw_and_filtered on split_3: 0.7990\n",
      "Precision for raw_and_filtered on split_3: 0.7058\n",
      "Recall for raw_and_filtered on split_3: 0.9206\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered_and_stft on split_3 ---\n",
      "Training data shapes: {'raw_input': (20670, 300, 1), 'filtered_input': (20670, 300, 1), 'stft_input': (20670, 13, 1)}, labels=(20670,)\n",
      "Test data shapes: {'raw_input': (5320, 300, 1), 'filtered_input': (5320, 300, 1), 'stft_input': (5320, 13, 1)}, labels=(5320,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 362ms/step - accuracy: 0.8141 - loss: 0.4310 - val_accuracy: 0.8582 - val_loss: 0.3256\n",
      "Epoch 2/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 313ms/step - accuracy: 0.8696 - loss: 0.2820 - val_accuracy: 0.8280 - val_loss: 0.3194\n",
      "Epoch 3/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 297ms/step - accuracy: 0.8801 - loss: 0.2474 - val_accuracy: 0.9124 - val_loss: 0.2021\n",
      "Epoch 4/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - accuracy: 0.9026 - loss: 0.2138 - val_accuracy: 0.9349 - val_loss: 0.1658\n",
      "Epoch 5/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 293ms/step - accuracy: 0.9082 - loss: 0.2023 - val_accuracy: 0.9371 - val_loss: 0.1560\n",
      "Epoch 6/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 290ms/step - accuracy: 0.9132 - loss: 0.1932 - val_accuracy: 0.9403 - val_loss: 0.1498\n",
      "Epoch 7/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 291ms/step - accuracy: 0.9182 - loss: 0.1857 - val_accuracy: 0.9400 - val_loss: 0.1487\n",
      "Epoch 8/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 289ms/step - accuracy: 0.9222 - loss: 0.1796 - val_accuracy: 0.9398 - val_loss: 0.1482\n",
      "Epoch 9/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 291ms/step - accuracy: 0.9254 - loss: 0.1743 - val_accuracy: 0.9422 - val_loss: 0.1445\n",
      "Epoch 10/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 311ms/step - accuracy: 0.9285 - loss: 0.1679 - val_accuracy: 0.9439 - val_loss: 0.1406\n",
      "Epoch 11/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 301ms/step - accuracy: 0.9337 - loss: 0.1602 - val_accuracy: 0.9473 - val_loss: 0.1388\n",
      "Epoch 12/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 298ms/step - accuracy: 0.9369 - loss: 0.1530 - val_accuracy: 0.9480 - val_loss: 0.1374\n",
      "Epoch 13/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 292ms/step - accuracy: 0.9406 - loss: 0.1462 - val_accuracy: 0.9475 - val_loss: 0.1357\n",
      "Epoch 14/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 295ms/step - accuracy: 0.9440 - loss: 0.1394 - val_accuracy: 0.9461 - val_loss: 0.1346\n",
      "Epoch 15/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 299ms/step - accuracy: 0.9462 - loss: 0.1323 - val_accuracy: 0.9487 - val_loss: 0.1351\n",
      "Epoch 16/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 286ms/step - accuracy: 0.9491 - loss: 0.1257 - val_accuracy: 0.9490 - val_loss: 0.1380\n",
      "Epoch 17/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 324ms/step - accuracy: 0.9510 - loss: 0.1202 - val_accuracy: 0.9463 - val_loss: 0.1424\n",
      "Epoch 18/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 306ms/step - accuracy: 0.9537 - loss: 0.1124 - val_accuracy: 0.9468 - val_loss: 0.1481\n",
      "Epoch 19/20\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 287ms/step - accuracy: 0.9583 - loss: 0.1035 - val_accuracy: 0.9429 - val_loss: 0.1564\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.2573, Accuracy: 0.8959\n",
      "F1 Score for raw_and_filtered_and_stft on split_3: 0.7966\n",
      "Precision for raw_and_filtered_and_stft on split_3: 0.7101\n",
      "Recall for raw_and_filtered_and_stft on split_3: 0.9072\n",
      "\n",
      "--- Processing Split: split_4 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 91 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 32 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 91 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 32 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Not setting metadata\n",
      "20420 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20420 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5570 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5570 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "20420 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20420 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5570 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5570 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: raw on split_4 ---\n",
      "Training data shapes: (20420, 300, 1), labels=(20420,)\n",
      "Test data shapes: (5570, 300, 1), labels=(5570,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 125ms/step - accuracy: 0.7921 - loss: 0.5105 - val_accuracy: 0.8790 - val_loss: 0.3493\n",
      "Epoch 2/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.8027 - loss: 0.4122 - val_accuracy: 0.8786 - val_loss: 0.3137\n",
      "Epoch 3/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.8403 - loss: 0.3467 - val_accuracy: 0.8952 - val_loss: 0.2509\n",
      "Epoch 4/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.8767 - loss: 0.2816 - val_accuracy: 0.8972 - val_loss: 0.2414\n",
      "Epoch 5/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.8829 - loss: 0.2707 - val_accuracy: 0.9003 - val_loss: 0.2375\n",
      "Epoch 6/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - accuracy: 0.8855 - loss: 0.2638 - val_accuracy: 0.8996 - val_loss: 0.2356\n",
      "Epoch 7/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.8875 - loss: 0.2558 - val_accuracy: 0.8981 - val_loss: 0.2246\n",
      "Epoch 8/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.8898 - loss: 0.2535 - val_accuracy: 0.9060 - val_loss: 0.2256\n",
      "Epoch 9/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - accuracy: 0.8906 - loss: 0.2471 - val_accuracy: 0.9082 - val_loss: 0.2172\n",
      "Epoch 10/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.8920 - loss: 0.2407 - val_accuracy: 0.9094 - val_loss: 0.2097\n",
      "Epoch 11/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.8932 - loss: 0.2353 - val_accuracy: 0.9111 - val_loss: 0.2061\n",
      "Epoch 12/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - accuracy: 0.8965 - loss: 0.2306 - val_accuracy: 0.9138 - val_loss: 0.2034\n",
      "Epoch 13/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.8981 - loss: 0.2262 - val_accuracy: 0.9143 - val_loss: 0.2008\n",
      "Epoch 14/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 106ms/step - accuracy: 0.8992 - loss: 0.2228 - val_accuracy: 0.9155 - val_loss: 0.1990\n",
      "Epoch 15/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - accuracy: 0.9007 - loss: 0.2196 - val_accuracy: 0.9163 - val_loss: 0.1972\n",
      "Epoch 16/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9026 - loss: 0.2164 - val_accuracy: 0.9182 - val_loss: 0.1962\n",
      "Epoch 17/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9053 - loss: 0.2136 - val_accuracy: 0.9199 - val_loss: 0.1956\n",
      "Epoch 18/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 107ms/step - accuracy: 0.9075 - loss: 0.2105 - val_accuracy: 0.9207 - val_loss: 0.1941\n",
      "Epoch 19/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 106ms/step - accuracy: 0.9083 - loss: 0.2073 - val_accuracy: 0.9216 - val_loss: 0.1935\n",
      "Epoch 20/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.9092 - loss: 0.2043 - val_accuracy: 0.9229 - val_loss: 0.1919\n",
      "Training finished.\n",
      "Evaluating on split_4's test data...\n",
      "Loss: 0.1855, Accuracy: 0.9172\n",
      "F1 Score for raw on split_4: 0.7863\n",
      "Precision for raw on split_4: 0.7845\n",
      "Recall for raw on split_4: 0.7881\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered on split_4 ---\n",
      "Training data shapes: {'raw_input': (20420, 300, 1), 'filtered_input': (20420, 300, 1)}, labels=(20420,)\n",
      "Test data shapes: {'raw_input': (5570, 300, 1), 'filtered_input': (5570, 300, 1)}, labels=(5570,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 333ms/step - accuracy: 0.7431 - loss: 0.5221 - val_accuracy: 0.7840 - val_loss: 0.3761\n",
      "Epoch 2/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 263ms/step - accuracy: 0.7829 - loss: 0.4180 - val_accuracy: 0.8763 - val_loss: 0.3027\n",
      "Epoch 3/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 252ms/step - accuracy: 0.8402 - loss: 0.3792 - val_accuracy: 0.8827 - val_loss: 0.2668\n",
      "Epoch 4/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 263ms/step - accuracy: 0.8712 - loss: 0.2816 - val_accuracy: 0.9141 - val_loss: 0.2006\n",
      "Epoch 5/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 261ms/step - accuracy: 0.8937 - loss: 0.2490 - val_accuracy: 0.9199 - val_loss: 0.1904\n",
      "Epoch 6/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 255ms/step - accuracy: 0.9010 - loss: 0.2308 - val_accuracy: 0.9226 - val_loss: 0.1768\n",
      "Epoch 7/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 256ms/step - accuracy: 0.8996 - loss: 0.2269 - val_accuracy: 0.9327 - val_loss: 0.1631\n",
      "Epoch 8/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 253ms/step - accuracy: 0.9055 - loss: 0.2149 - val_accuracy: 0.9334 - val_loss: 0.1598\n",
      "Epoch 9/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 264ms/step - accuracy: 0.9096 - loss: 0.2100 - val_accuracy: 0.9322 - val_loss: 0.1674\n",
      "Epoch 10/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 261ms/step - accuracy: 0.9074 - loss: 0.2255 - val_accuracy: 0.9390 - val_loss: 0.1439\n",
      "Epoch 11/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 252ms/step - accuracy: 0.9113 - loss: 0.2055 - val_accuracy: 0.9395 - val_loss: 0.1420\n",
      "Epoch 12/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 246ms/step - accuracy: 0.9135 - loss: 0.2007 - val_accuracy: 0.9405 - val_loss: 0.1399\n",
      "Epoch 13/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 270ms/step - accuracy: 0.9144 - loss: 0.1968 - val_accuracy: 0.9395 - val_loss: 0.1416\n",
      "Epoch 14/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 248ms/step - accuracy: 0.9167 - loss: 0.1942 - val_accuracy: 0.9393 - val_loss: 0.1364\n",
      "Epoch 15/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 259ms/step - accuracy: 0.9188 - loss: 0.1908 - val_accuracy: 0.9474 - val_loss: 0.1310\n",
      "Epoch 16/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 248ms/step - accuracy: 0.9203 - loss: 0.1868 - val_accuracy: 0.9466 - val_loss: 0.1270\n",
      "Epoch 17/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 249ms/step - accuracy: 0.9208 - loss: 0.1836 - val_accuracy: 0.9483 - val_loss: 0.1285\n",
      "Epoch 18/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 255ms/step - accuracy: 0.9255 - loss: 0.1821 - val_accuracy: 0.9525 - val_loss: 0.1178\n",
      "Epoch 19/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 259ms/step - accuracy: 0.9239 - loss: 0.1772 - val_accuracy: 0.9542 - val_loss: 0.1133\n",
      "Epoch 20/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 253ms/step - accuracy: 0.9266 - loss: 0.1753 - val_accuracy: 0.9567 - val_loss: 0.1107\n",
      "Training finished.\n",
      "Evaluating on split_4's test data...\n",
      "Loss: 0.1589, Accuracy: 0.9323\n",
      "F1 Score for raw_and_filtered on split_4: 0.8209\n",
      "Precision for raw_and_filtered on split_4: 0.8397\n",
      "Recall for raw_and_filtered on split_4: 0.8030\n",
      "\n",
      "--- Evaluating Model: raw_and_filtered_and_stft on split_4 ---\n",
      "Training data shapes: {'raw_input': (20420, 300, 1), 'filtered_input': (20420, 300, 1), 'stft_input': (20420, 13, 1)}, labels=(20420,)\n",
      "Test data shapes: {'raw_input': (5570, 300, 1), 'filtered_input': (5570, 300, 1), 'stft_input': (5570, 13, 1)}, labels=(5570,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 336ms/step - accuracy: 0.8055 - loss: 0.4340 - val_accuracy: 0.8891 - val_loss: 0.3204\n",
      "Epoch 2/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 292ms/step - accuracy: 0.8397 - loss: 0.3548 - val_accuracy: 0.8923 - val_loss: 0.2807\n",
      "Epoch 3/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 296ms/step - accuracy: 0.8743 - loss: 0.2773 - val_accuracy: 0.9070 - val_loss: 0.2338\n",
      "Epoch 4/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 275ms/step - accuracy: 0.8802 - loss: 0.2565 - val_accuracy: 0.9197 - val_loss: 0.1921\n",
      "Epoch 5/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 292ms/step - accuracy: 0.8986 - loss: 0.2294 - val_accuracy: 0.9182 - val_loss: 0.1864\n",
      "Epoch 6/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 281ms/step - accuracy: 0.9035 - loss: 0.2177 - val_accuracy: 0.9275 - val_loss: 0.1682\n",
      "Epoch 7/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 275ms/step - accuracy: 0.9094 - loss: 0.2070 - val_accuracy: 0.9327 - val_loss: 0.1611\n",
      "Epoch 8/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 286ms/step - accuracy: 0.9167 - loss: 0.1983 - val_accuracy: 0.9314 - val_loss: 0.1618\n",
      "Epoch 9/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 272ms/step - accuracy: 0.9158 - loss: 0.1969 - val_accuracy: 0.9322 - val_loss: 0.1631\n",
      "Epoch 10/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 287ms/step - accuracy: 0.9209 - loss: 0.1878 - val_accuracy: 0.9310 - val_loss: 0.1677\n",
      "Epoch 11/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 270ms/step - accuracy: 0.9239 - loss: 0.1797 - val_accuracy: 0.9319 - val_loss: 0.1713\n",
      "Epoch 12/20\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 294ms/step - accuracy: 0.9266 - loss: 0.1744 - val_accuracy: 0.9275 - val_loss: 0.1791\n",
      "Training finished.\n",
      "Evaluating on split_4's test data...\n",
      "Loss: 0.2036, Accuracy: 0.9072\n",
      "F1 Score for raw_and_filtered_and_stft on split_4: 0.7464\n",
      "Precision for raw_and_filtered_and_stft on split_4: 0.7902\n",
      "Recall for raw_and_filtered_and_stft on split_4: 0.7072\n",
      "\n",
      "--- Processing Split: split_5 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 105 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 18 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 105 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Filtering raw data in 18 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "Not setting metadata\n",
      "20120 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20120 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5870 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5870 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "20120 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 20120 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5870 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5870 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: raw on split_5 ---\n",
      "Training data shapes: (20120, 300, 1), labels=(20120,)\n",
      "Test data shapes: (5870, 300, 1), labels=(5870,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 44/126\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.6943 - loss: 0.5802 "
     ]
    }
   ],
   "source": [
    "# we want to evaluate the models on all these scores\n",
    "model_metrics = {\n",
    "    'raw': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'raw_and_filtered': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'raw_and_filtered_and_stft': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# will evaluate the three models\n",
    "# model 1: raw\n",
    "# model 2: raw + filtered\n",
    "# model 3: raw + filtered + STFT frequency\n",
    "models_to_evaluate = {\n",
    "    'raw': build_cnn_model_downsampled,\n",
    "    'raw_and_filtered': build_multi_input_cnn_model_filtered,\n",
    "    'raw_and_filtered_and_stft': build_multi_input_cnn_model_freq\n",
    "}\n",
    "\n",
    "# then we go through each split\n",
    "# so create a for loop\n",
    "for split_name, raw_data in raw_splits.items():\n",
    "    print(f\"\\n--- Processing Split: {split_name} ---\")\n",
    "    # when running the code for a long time, allows you to know at which stage it's at\n",
    "    train_raw = raw_data['train']\n",
    "    test_raw = raw_data['test']\n",
    "\n",
    "    # this is for each split\n",
    "\n",
    "    # Slow oscillation detection on raw data for raw/three-input model labels\n",
    "    so_train_times_raw_downsampled = detect_slow_oscillations_times(train_raw, do_filter=True, do_downsample=True)\n",
    "    so_test_times_raw_downsampled = detect_slow_oscillations_times(test_raw, do_filter=True, do_downsample=True)\n",
    "    so_starts_train_raw_downsampled, so_ends_train_raw_downsampled = zip(*so_train_times_raw_downsampled) if so_train_times_raw_downsampled else ([], [])\n",
    "    so_starts_test_raw_downsampled, so_ends_test_raw_downsampled = zip(*so_test_times_raw_downsampled) if so_test_times_raw_downsampled else ([], [])\n",
    "    # Downsample raw data for one input\n",
    "    train_raw_downsampled = train_raw.copy().resample(100)\n",
    "    test_raw_downsampled = test_raw.copy().resample(100)\n",
    "\n",
    "    # Filtered data for filtered input and downsample\n",
    "    train_filtered_downsampled = train_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "    test_filtered_downsampled = test_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "    train_filtered = train_filtered_downsampled.resample(100)\n",
    "    test_filtered = test_filtered_downsampled.resample(100)\n",
    "    # resample because already copied before\n",
    "\n",
    "    # so detection for model 2\n",
    "    #so_train_times_filtered_downsampled = detect_slow_oscillations_times(train_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "    #so_test_times_filtered_downsampled = detect_slow_oscillations_times(test_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "    # since filtering and downsampling before, do not filter and downsample again in function\n",
    "\n",
    "    #so_starts_train_filtered_downsampled, so_ends_train_filtered_downsampled = zip(*so_train_times_filtered_downsampled) if so_train_times_filtered_downsampled else([],[])\n",
    "    #so_starts_test_filtered_downsampled, so_ends_test_filtered_downsampled = zip(*so_test_times_filtered_downsampled) if so_test_times_filtered_downsampled else([],[])\n",
    "\n",
    "\n",
    "    # create fixed length epochs, are of 3 seconds each\n",
    "    epochs_train_raw_downsampled = create_fixed_length_epochs(train_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "    epochs_test_raw_downsampled = create_fixed_length_epochs(test_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "    epochs_train_filtered_downsampled = create_fixed_length_epochs(train_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "    epochs_test_filtered_downsampled = create_fixed_length_epochs(test_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "\n",
    "    # STFT input for model 3\n",
    "    # created on epochs from raw downsampled data\n",
    "    epochs_train_stft_downsampled = np.squeeze(np.array(epochs_train_raw_downsampled))\n",
    "    epochs_test_stft_downsampled = np.squeeze(np.array(epochs_test_raw_downsampled))\n",
    "\n",
    "    fs = train_raw_downsampled.info['sfreq']\n",
    "    nperseg = 50\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    X_train_stft_transformed = []\n",
    "    for epoch in epochs_train_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_train_stft_transformed.append(spectrogram)\n",
    "    X_train_stft_transformed = np.array(X_train_stft_transformed)\n",
    "\n",
    "    X_test_stft_transformed = []\n",
    "    for epoch in epochs_test_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_test_stft_transformed.append(spectrogram)\n",
    "    X_test_stft_transformed = np.array(X_test_stft_transformed)\n",
    "\n",
    "    # only keep the frequency dimension of STFT\n",
    "    X_train_stft_freq = np.mean(X_train_stft_transformed, axis=1)\n",
    "    X_test_stft_freq = np.mean(X_test_stft_transformed, axis=1)\n",
    "    X_train_stft_freq = X_train_stft_freq[..., np.newaxis] \n",
    "    # to have correct input size for CNN, adds channel dimension\n",
    "    X_test_stft_freq = X_test_stft_freq[..., np.newaxis] \n",
    "\n",
    "    # normalize per epoch\n",
    "    X_train_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_train_stft_freq\n",
    "    ])\n",
    "    X_test_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_test_stft_freq\n",
    "    ])\n",
    "\n",
    "\n",
    "    # reshape the epochs for model 1 and model 2\n",
    "    X_train_raw = np.array(epochs_train_raw_downsampled).reshape(len(epochs_train_raw_downsampled), -1, 1)\n",
    "    X_test_raw = np.array(epochs_test_raw_downsampled).reshape(len(epochs_test_raw_downsampled), -1, 1)\n",
    "\n",
    "    X_train_filtered = np.array(epochs_train_filtered_downsampled).reshape(len(epochs_train_filtered_downsampled), -1, 1)\n",
    "    X_test_filtered = np.array(epochs_test_filtered_downsampled).reshape(len(epochs_test_filtered_downsampled), -1, 1)\n",
    "\n",
    "\n",
    "    # still in the same split\n",
    "    # now iterate through the models\n",
    "    for model_name, build_model_func in models_to_evaluate.items():\n",
    "        print(f\"\\n--- Evaluating Model: {model_name} on {split_name} ---\")\n",
    "\n",
    "        # here define X and y sets\n",
    "        # y set defined by assigning labels\n",
    "        if model_name == 'raw':\n",
    "            X_train_input = X_train_raw\n",
    "            X_test_input = X_test_raw\n",
    "            y_train = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "            y_test = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "            input_shape = (X_train_input.shape[1], X_train_input.shape[2])\n",
    "\n",
    "        elif model_name == 'raw_and_filtered':\n",
    "             X_train_input = {\n",
    "                 'raw_input': X_train_raw,\n",
    "                 'filtered_input': X_train_filtered\n",
    "             }\n",
    "             X_test_input = {\n",
    "                 'raw_input': X_test_raw,\n",
    "                 'filtered_input': X_test_filtered\n",
    "             }\n",
    "             y_train = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "             y_test = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "             input_shape = None\n",
    "            # when input shape = None, infers it itself\n",
    "\n",
    "        elif model_name == 'raw_and_filtered_and_stft':\n",
    "            X_train_input = {\n",
    "                'raw_input': X_train_raw,\n",
    "                'filtered_input': X_train_filtered,\n",
    "                'stft_input': X_train_stft_freq_norm \n",
    "            }\n",
    "            X_test_input = {\n",
    "                'raw_input': X_test_raw,\n",
    "                'filtered_input': X_test_filtered,\n",
    "                'stft_input': X_test_stft_freq_norm \n",
    "            }\n",
    "            # Labels for the three-input model come from the raw downsampled data\n",
    "            y_train = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "            y_test = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "\n",
    "            input_shape = None \n",
    "\n",
    "        print(f\"Training data shapes: { {k: v.shape for k, v in X_train_input.items()} if isinstance(X_train_input, dict) else X_train_input.shape}, labels={y_train.shape}\")\n",
    "        print(f\"Test data shapes: { {k: v.shape for k, v in X_test_input.items()} if isinstance(X_test_input, dict) else X_test_input.shape}, labels={y_test.shape}\")\n",
    "        # to check whether a dictionary or not \n",
    "        # because it is a dictionary for the three-input model\n",
    "        # but not for the other models\n",
    "\n",
    "\n",
    "        # build the models\n",
    "\n",
    "        print(\"Building and compiling model...\")\n",
    "        if model_name in ['raw']:\n",
    "             model = build_model_func(input_shape)\n",
    "        else:\n",
    "            model = build_model_func()\n",
    "\n",
    "\n",
    "        # define early stopping\n",
    "        # if validation loss does not change after 5 epochs\n",
    "        # stop training\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        # keep 20% of training set as validation\n",
    "        # this is useful to detect overfitting\n",
    "        history = model.fit(\n",
    "            X_train_input,\n",
    "            y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=20, # Adjust epochs as needed\n",
    "            batch_size=128, # Adjust batch size as needed\n",
    "            callbacks=[early_stop], # Optional: Use early stopping\n",
    "        )\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # this evaluates the model on test data of split (unseen data)\n",
    "        # these are the predictions\n",
    "        print(f\"Evaluating on {split_name}'s test data...\")\n",
    "        loss, accuracy = model.evaluate(X_test_input, y_test, verbose=0)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        # useful to compare to accuracy and loss of training and validation set\n",
    "        # to detect any overfitting\n",
    "\n",
    "        # now that have the predictions can calculate the F1 score\n",
    "        # and also accuracy and recall\n",
    "        y_pred_proba = model.predict(X_test_input, verbose=0)\n",
    "        y_pred_labels = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        split_f1 = f1_score(y_test, y_pred_labels)\n",
    "        split_precision = precision_score(y_test, y_pred_labels)\n",
    "        split_recall = recall_score(y_test, y_pred_labels)\n",
    "        print(f\"F1 Score for {model_name} on {split_name}: {split_f1:.4f}\")\n",
    "        print(f\"Precision for {model_name} on {split_name}: {split_precision:.4f}\")\n",
    "        print(f\"Recall for {model_name} on {split_name}: {split_recall:.4f}\")\n",
    "\n",
    "        # store all the metrics\n",
    "        # then move on to next step\n",
    "        model_metrics[model_name]['f1_scores'].append(split_f1)\n",
    "        model_metrics[model_name]['precision_scores'].append(split_precision)\n",
    "        model_metrics[model_name]['recall_scores'].append(split_recall)\n",
    "\n",
    "        # clears tensorflow \n",
    "        # this is to free up memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n--- Evaluation finished for all models across all splits ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c467797-0f32-4c23-889d-121f66f5fdf5",
   "metadata": {},
   "source": [
    "## Display average metrics and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467a81e-5d36-4808-a9a5-08c7694ae3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display F1, precision and recall in a data frame\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    average_f1 = np.mean(metrics['f1_scores'])\n",
    "    std_f1 = np.std(metrics['f1_scores'])\n",
    "\n",
    "    average_precision = np.mean(metrics['precision_scores'])\n",
    "    std_precision = np.std(metrics['precision_scores'])\n",
    "\n",
    "    average_recall = np.mean(metrics['recall_scores'])\n",
    "    std_recall = np.std(metrics['recall_scores'])\n",
    "\n",
    "    # the row is appended as a dict\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"F1 Score (mean ± std)\": f\"{average_f1:.4f} ± {std_f1:.4f}\",\n",
    "        \"Precision (mean ± std)\": f\"{average_precision:.4f} ± {std_precision:.4f}\",\n",
    "        \"Recall (mean ± std)\": f\"{average_recall:.4f} ± {std_recall:.4f}\",\n",
    "    })\n",
    "\n",
    "# use pandas to create the data frame\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# add a title and print the table\n",
    "print(\"\\n--- Average Metrics Across Splits For SO Detection ---\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# add statistics\n",
    "\n",
    "# definition\n",
    "def compare_models(f1_a, f1_b, model_a_name, model_b_name, alpha=0.05, n_comparisons=1):\n",
    "    # first alpha is adjusted for multiple comparisons\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"\\nBonferroni corrected alpha: {corrected_alpha:.4f} (original alpha={alpha} / {n_comparisons} comparisons)\")\n",
    "\n",
    "    # differences computed\n",
    "    diff = np.array(f1_b) - np.array(f1_a)\n",
    "\n",
    "    # plot distribution to assess normality\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(diff, kde=True, bins=8)\n",
    "    plt.title(f'Distribution of F1 Score Differences: {model_b_name} - {model_a_name}')\n",
    "    plt.xlabel('F1 Score Difference')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # shapiro wilk test to assess normality\n",
    "    # determines which type of t-test to do\n",
    "    w_stat, p_norm = shapiro(diff)\n",
    "    print(f\"Shapiro-Wilk normality test: W = {w_stat:.4f}, p = {p_norm:.4f}\")\n",
    "\n",
    "    # then perform t-test (paired t-test or wilcoxon signed-rank test)\n",
    "    print(f\"\\n--- Statistical Comparison: {model_b_name} vs {model_a_name} ---\")\n",
    "    if p_norm > corrected_alpha:\n",
    "        print(\"Paired t-test (normal distribution)\")\n",
    "        t_stat, p_val = ttest_rel(f1_b, f1_a)\n",
    "        print(f\"t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")\n",
    "    else:\n",
    "        print(\"Wilcoxon signed-rank test (non-normal distribution)\")\n",
    "        w_stat, p_val = wilcoxon(f1_b, f1_a)\n",
    "        print(f\"W-statistic = {w_stat:.4f}, p-value = {p_val:.4f}\")\n",
    "\n",
    "    # then compare with the Bonferroni corrected alpha\n",
    "    if p_val < corrected_alpha:\n",
    "        print(f\"Significant difference at corrected alpha = {corrected_alpha:.4f}\")\n",
    "    else:\n",
    "        print(f\"No significant difference at corrected alpha = {corrected_alpha:.4f}\")\n",
    "\n",
    "# then do the three model comparisons\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered']['f1_scores'],\n",
    "    model_a_name='raw',\n",
    "    model_b_name='raw_and_filtered',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered_and_stft']['f1_scores'],\n",
    "    model_a_name='raw',\n",
    "    model_b_name='raw_and_filtered',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw_and_filtered']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered_and_stft']['f1_scores'],\n",
    "    model_a_name='raw_and_filtered',\n",
    "    model_b_name='raw_and_filtered_and_stft',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b5d8b-55b1-453b-90d3-1275f9ef7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to get nicer visualisations\n",
    "\n",
    "# have a table only with F1 scores\n",
    "model_names = list(model_metrics.keys())\n",
    "average_f1s = [round(np.mean(model_metrics[name]['f1_scores']), 2) for name in model_names]\n",
    "std_f1s = [round(np.std(model_metrics[name]['f1_scores']), 2) for name in model_names]\n",
    "\n",
    "summary_data = {\n",
    "    'Model': model_names,\n",
    "    'Mean F1 Score': average_f1s,\n",
    "    'F1 Score Std Dev': std_f1s\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n--- Summary of F1 Scores Across 5 Folds for SO Detection ---\")\n",
    "display(summary_df)\n",
    "\n",
    "# create bar plot with F1 score for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(summary_df['Model'], summary_df['Mean F1 Score'],\n",
    "               yerr=summary_df['F1 Score Std Dev'], capsize=5,\n",
    "               color=['lightcoral', 'lightgreen', 'skyblue'])\n",
    "\n",
    "# the F1 values are on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', va='bottom', ha='center', fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Mean F1 Score\")\n",
    "plt.title(\"Comparison of Model Performance on SO Detection for 5 participants\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
