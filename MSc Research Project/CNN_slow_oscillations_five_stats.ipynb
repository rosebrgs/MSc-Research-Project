{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53912b6f-0832-4e91-b068-27583c9a7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista\n",
    "import ipywidgets\n",
    "import ipyevents\n",
    "import pyvistaqt\n",
    "import yasa\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import stft\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "import pywt\n",
    "import cv2\n",
    "\n",
    "SEED = 15\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7692555-7c48-480d-a32f-6fe1f0fbcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1dd96-bb6c-47de-9642-b7df2c49f262",
   "metadata": {},
   "source": [
    "### CNN one input Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23cf70f-8364-46b1-9471-022dd77508a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_downsampled(input_shape=(300,1)):\n",
    "\n",
    "    # linear embedding layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Three convolutional blocks (like having three pattern detectors)\n",
    "\n",
    "    # First convolution block, kernel size of 5\n",
    "    padded1 = tf.keras.layers.ZeroPadding1D(padding=2)(input_layer)\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=10, kernel_size=5, strides=1, padding='valid')(padded1)\n",
    "    # each filter learns a different type of short-time feature\n",
    "    # stride of 1, moves one step at a time\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv1)\n",
    "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # K = 2\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "\n",
    "    # Second convolution block, kernel size of 11\n",
    "    padded2 = tf.keras.layers.ZeroPadding1D(padding=5)(input_layer)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=10, kernel_size=11, strides=1, padding='valid')(padded2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv2)\n",
    "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "\n",
    "    # Third convolution block, kernel size of 21\n",
    "    padded3 = tf.keras.layers.ZeroPadding1D(padding=10)(input_layer)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=10, kernel_size=21, strides=1, padding='valid')(padded3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv3)\n",
    "    conv3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "\n",
    "    # Concatenate the outputs of all blocks\n",
    "    concatenated = tf.keras.layers.Concatenate()([conv1, conv2, conv3])\n",
    "\n",
    "    # GRU Layer\n",
    "    gru = tf.keras.layers.GRU(64)(concatenated)\n",
    "\n",
    "    # Fully connected (dense) layer\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(gru)\n",
    "    # add a Dropout layer to prevent overfitting\n",
    "    #dense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    # Two softmax outputs for dual-task classification\n",
    "    #output_task1 = tf.keras.layers.Dense(2, activation='softmax', name='task1')(dense)\n",
    "    #output_task2 = tf.keras.layers.Dense(2, activation='softmax', name='task2')(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create the model\n",
    "    #model = tf.keras.models.Model(inputs=input_layer, outputs=[output_task1, output_task2])\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    #model.compile(optimizer='adam', loss={'task1': 'categorical_crossentropy', 'task2': 'categorical_crossentropy'}, metrics={'task1': 'accuracy', 'task2': 'accuracy'})\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4fbf7e-a06b-4f40-8092-8255df739ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_freq():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered = tf.keras.Input(shape=(300, 1), name='filtered_input')\n",
    "    input_stft = tf.keras.Input(shape=(13, 1), name='stft_input')  \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered = conv_branch(input_filtered)\n",
    "    branch_stft = conv_branch(input_stft)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered = tf.keras.layers.GRU(64)(branch_filtered)\n",
    "    gru_stft = tf.keras.layers.GRU(64)(branch_stft)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered, gru_stft])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered, input_stft], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74010b3-521c-44b2-b095-9b56d318cf3d",
   "metadata": {},
   "source": [
    "### Slow oscillation detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3c8bcc-7f78-48f3-99ab-9bdf104e5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_slow_oscillations_times(combined_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "    #signs = np.sign(current_data)\n",
    "    #pos_to_neg = np.where((signs[:-1] > 0) & (signs[1:] < 0))[0]\n",
    "    # detect +1 to -1\n",
    "    #neg_to_pos = np.where((signs[:-1] <  0) & (signs[1:] > 0))[0]\n",
    "    # detect -1 to +1\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices = []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings)-1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "\n",
    "        # find the negative to positive crossing in between\n",
    "        #mid_crossings = neg_to_pos[(neg_to_pos > start_idx) & (neg_to_pos < end_idx)]\n",
    "\n",
    "        #if len(mid_crossings) != 1:\n",
    "            #continue\n",
    "\n",
    "        #mid_idx = mid_crossings [0]\n",
    "\n",
    "        #duration = (end_idx - start_idx) / sfreq\n",
    "        #if not (0.8 <= duration <= 2.0):\n",
    "  \n",
    "        \n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    # keep lowest negative peaks (under the 25th percentile)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "    # keep largest peak-to-peak amplitude (over 75th percentile)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation\n",
    "\n",
    "def detect_slow_oscillations_peaks(combined_raw, do_filter=True, do_downsample=True, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    slow_oscillations_peaks = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices =  []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings) - 1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            slow_oscillations_peaks.append((negative_peak, positive_peak))\n",
    "\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cea46-853b-4428-bb10-633e542db5d6",
   "metadata": {},
   "source": [
    "### Epochs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d211ff33-e059-4360-97e5-861d5104e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_length_epochs(raw, duration=3.0, overlap=0.0, preload=True, reject_by_annotation=False):\n",
    "\n",
    "    return mne.make_fixed_length_epochs(\n",
    "        raw,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=preload,\n",
    "        reject_by_annotation=reject_by_annotation\n",
    "    )\n",
    "# function mne.make_fixed_length_epochs takes into account the sampling frequency of the data\n",
    "\n",
    "\n",
    "def label_so_epochs_moderate(epochs, so_starts, so_ends, epoch_length_sec=3.0):\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "    for so_start, so_end in zip(so_starts, so_ends):\n",
    "        so_duration = so_end - so_start\n",
    "        required_overlap = 0.5 * so_duration  \n",
    "        # only label 1 if epoch contains 80% of the SO duration\n",
    "\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "\n",
    "            # Calculate overlap between SO and epoch\n",
    "            overlap_start = max(so_start, epoch_start)\n",
    "            overlap_end = min(so_end, epoch_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration >= required_overlap:\n",
    "                epoch_labels[i] = 1\n",
    "\n",
    "    return epoch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fae6b-c6f3-4d41-9d9e-94bf8b069555",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4b0a26-649b-4b9b-ba8f-6b6e306e667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for split_1...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 32700095 =    180.000 ... 65400.190 secs\n",
      "Ready.\n",
      "Reading 0 ... 32610095  =      0.000 ... 65220.190 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 7845026 =   2940.000 ... 15690.052 secs\n",
      "Ready.\n",
      "Reading 0 ... 6375026  =      0.000 ... 12750.052 secs...\n",
      "Loaded train and test data for split_1\n",
      "Loading data for split_2...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32985091 =   2940.000 ... 65970.182 secs\n",
      "Ready.\n",
      "Reading 0 ... 31515091  =      0.000 ... 63030.182 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 32700095 =    180.000 ... 65400.190 secs\n",
      "Ready.\n",
      "Reading 0 ... 32610095  =      0.000 ... 65220.190 secs...\n",
      "Loaded train and test data for split_2\n",
      "Loading data for split_3...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 82755237 =   2940.000 ... 165510.474 secs\n",
      "Ready.\n",
      "Reading 0 ... 81285237  =      0.000 ... 162570.474 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 300000 ... 8280014 =    600.000 ... 16560.028 secs\n",
      "Ready.\n",
      "Reading 0 ... 7980014  =      0.000 ... 15960.028 secs...\n",
      "Loaded train and test data for split_3\n",
      "Loading data for split_4...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 132150366 =   2940.000 ... 264300.732 secs\n",
      "Ready.\n",
      "Reading 0 ... 130680366  =      0.000 ... 261360.732 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 825000 ... 9180031 =   1650.000 ... 18360.062 secs\n",
      "Ready.\n",
      "Reading 0 ... 8355031  =      0.000 ... 16710.062 secs...\n",
      "Loaded train and test data for split_4\n",
      "Loading data for split_5...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 181095509 =   2940.000 ... 362191.018 secs\n",
      "Ready.\n",
      "Reading 0 ... 179625509  =      0.000 ... 359251.018 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 165000 ... 8970017 =    330.000 ... 17940.034 secs\n",
      "Ready.\n",
      "Reading 0 ... 8805017  =      0.000 ... 17610.034 secs...\n",
      "Loaded train and test data for split_5\n"
     ]
    }
   ],
   "source": [
    "# for 5-fold validation\n",
    "# load the all the files needed that were pre-processed before\n",
    "# from train_1_raw and test_1_raw to train_5_raw and test_5_raw\n",
    "split_files = {\n",
    "    f'split_{i}': {\n",
    "        'train': fr\"C:\\EEG DATA\\combined_sets\\train_{i}_raw.fif\",\n",
    "        'test': fr\"C:\\EEG DATA\\combined_sets\\test_{i}_raw.fif\"\n",
    "    } for i in range(1, 6) \n",
    "}\n",
    "\n",
    "raw_splits = {}\n",
    "for split_name, files in split_files.items():\n",
    "    print(f\"Loading data for {split_name}...\")\n",
    "    try:\n",
    "        train_raw = mne.io.read_raw_fif(files['train'], preload=True)\n",
    "        test_raw = mne.io.read_raw_fif(files['test'], preload=True)\n",
    "        raw_splits[split_name] = {'train': train_raw, 'test': test_raw}\n",
    "        print(f\"Loaded train and test data for {split_name}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found for {split_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {split_name}: {e}\")\n",
    "        # errors in case dictionaries not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5bbfa1-7748-42fa-9987-266c1992dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Split: split_1 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: one_input_cnn_raw on split_1 ---\n",
      "Training data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Test data shapes: (4250, 300, 1), labels=(4250,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.5630 - loss: 0.6829 - val_accuracy: 0.5584 - val_loss: 0.6818\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.6277 - loss: 0.6515 - val_accuracy: 0.6789 - val_loss: 0.6132\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.6702 - loss: 0.6035 - val_accuracy: 0.6391 - val_loss: 0.6110\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.6152 - loss: 0.6469 - val_accuracy: 0.5345 - val_loss: 0.6915\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.5911 - loss: 0.6700 - val_accuracy: 0.5964 - val_loss: 0.6575\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.6353 - loss: 0.6422 - val_accuracy: 0.6095 - val_loss: 0.6597\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.6527 - loss: 0.6225 - val_accuracy: 0.6693 - val_loss: 0.6105\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.7023 - loss: 0.5745 - val_accuracy: 0.6014 - val_loss: 0.6472\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.6609 - loss: 0.6227 - val_accuracy: 0.6355 - val_loss: 0.6518\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 106ms/step - accuracy: 0.7397 - loss: 0.5329 - val_accuracy: 0.7397 - val_loss: 0.5217\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 105ms/step - accuracy: 0.7800 - loss: 0.4829 - val_accuracy: 0.6884 - val_loss: 0.5870\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 0.7280 - loss: 0.5342 - val_accuracy: 0.7236 - val_loss: 0.5285\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 105ms/step - accuracy: 0.7749 - loss: 0.4770 - val_accuracy: 0.7705 - val_loss: 0.4696\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.7674 - loss: 0.4911 - val_accuracy: 0.6755 - val_loss: 0.5919\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7794 - loss: 0.4674 - val_accuracy: 0.7532 - val_loss: 0.5543\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.8151 - loss: 0.3966 - val_accuracy: 0.8172 - val_loss: 0.3977\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 108ms/step - accuracy: 0.8278 - loss: 0.3721 - val_accuracy: 0.8344 - val_loss: 0.3603\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - accuracy: 0.8370 - loss: 0.3565 - val_accuracy: 0.8441 - val_loss: 0.3489\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.8425 - loss: 0.3464 - val_accuracy: 0.8496 - val_loss: 0.3335\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 0.8422 - loss: 0.3482 - val_accuracy: 0.8360 - val_loss: 0.3546\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.3464, Accuracy: 0.8409\n",
      "F1 Score for one_input_cnn_raw on split_1: 0.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-Jul-25 17:34:14 | WARNING | From C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model: one_input_cnn_filtered on split_1 ---\n",
      "Training data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Test data shapes: (4250, 300, 1), labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 107ms/step - accuracy: 0.6297 - loss: 0.6495 - val_accuracy: 0.7054 - val_loss: 0.7377\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.7370 - loss: 0.5306 - val_accuracy: 0.8300 - val_loss: 0.4304\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.8414 - loss: 0.3727 - val_accuracy: 0.8528 - val_loss: 0.3258\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 102ms/step - accuracy: 0.8856 - loss: 0.2690 - val_accuracy: 0.8949 - val_loss: 0.2446\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 103ms/step - accuracy: 0.8924 - loss: 0.2535 - val_accuracy: 0.9154 - val_loss: 0.2125\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.9070 - loss: 0.2229 - val_accuracy: 0.9151 - val_loss: 0.1952\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.9063 - loss: 0.2203 - val_accuracy: 0.9234 - val_loss: 0.1849\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 99ms/step - accuracy: 0.9226 - loss: 0.1949 - val_accuracy: 0.9296 - val_loss: 0.1770\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.9221 - loss: 0.2018 - val_accuracy: 0.9324 - val_loss: 0.1666\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 103ms/step - accuracy: 0.9292 - loss: 0.1773 - val_accuracy: 0.9368 - val_loss: 0.1615\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.9345 - loss: 0.1711 - val_accuracy: 0.9351 - val_loss: 0.1634\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 99ms/step - accuracy: 0.9344 - loss: 0.1698 - val_accuracy: 0.9356 - val_loss: 0.1649\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 101ms/step - accuracy: 0.9370 - loss: 0.1659 - val_accuracy: 0.9386 - val_loss: 0.1588\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 102ms/step - accuracy: 0.9399 - loss: 0.1570 - val_accuracy: 0.8606 - val_loss: 0.3063\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.8941 - loss: 0.2453 - val_accuracy: 0.9351 - val_loss: 0.1657\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.9339 - loss: 0.1682 - val_accuracy: 0.9400 - val_loss: 0.1602\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.9388 - loss: 0.1599 - val_accuracy: 0.9420 - val_loss: 0.1559\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.9387 - loss: 0.1578 - val_accuracy: 0.9324 - val_loss: 0.1682\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.9384 - loss: 0.1613 - val_accuracy: 0.9441 - val_loss: 0.1573\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.9405 - loss: 0.1549 - val_accuracy: 0.9420 - val_loss: 0.1525\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.1210, Accuracy: 0.9532\n",
      "F1 Score for one_input_cnn_filtered on split_1: 0.9497\n",
      "\n",
      "--- Evaluating Model: three_input_cnn_freq on split_1 ---\n",
      "Training data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1), 'stft_input': (21740, 13, 1)}, labels=(21740,)\n",
      "Test data shapes: {'raw_input': (4250, 300, 1), 'filtered_input': (4250, 300, 1), 'stft_input': (4250, 13, 1)}, labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 328ms/step - accuracy: 0.6393 - loss: 0.6306 - val_accuracy: 0.7466 - val_loss: 0.6745\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 298ms/step - accuracy: 0.6841 - loss: 0.5973 - val_accuracy: 0.6014 - val_loss: 0.6602\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 311ms/step - accuracy: 0.7189 - loss: 0.5551 - val_accuracy: 0.6966 - val_loss: 0.5891\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 303ms/step - accuracy: 0.7327 - loss: 0.5329 - val_accuracy: 0.8454 - val_loss: 0.3482\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 297ms/step - accuracy: 0.8486 - loss: 0.3399 - val_accuracy: 0.8908 - val_loss: 0.2571\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 296ms/step - accuracy: 0.8885 - loss: 0.2532 - val_accuracy: 0.9117 - val_loss: 0.2105\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 295ms/step - accuracy: 0.9160 - loss: 0.2045 - val_accuracy: 0.9232 - val_loss: 0.1872\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 288ms/step - accuracy: 0.9267 - loss: 0.1838 - val_accuracy: 0.9236 - val_loss: 0.1838\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 286ms/step - accuracy: 0.9306 - loss: 0.1760 - val_accuracy: 0.9271 - val_loss: 0.1789\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 291ms/step - accuracy: 0.9322 - loss: 0.1743 - val_accuracy: 0.9322 - val_loss: 0.1723\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 293ms/step - accuracy: 0.9370 - loss: 0.1631 - val_accuracy: 0.9381 - val_loss: 0.1611\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 284ms/step - accuracy: 0.9394 - loss: 0.1565 - val_accuracy: 0.9391 - val_loss: 0.1563\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 258ms/step - accuracy: 0.9398 - loss: 0.1511 - val_accuracy: 0.9400 - val_loss: 0.1552\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 260ms/step - accuracy: 0.9431 - loss: 0.1460 - val_accuracy: 0.9395 - val_loss: 0.1551\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 295ms/step - accuracy: 0.9458 - loss: 0.1410 - val_accuracy: 0.9393 - val_loss: 0.1555\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 287ms/step - accuracy: 0.9491 - loss: 0.1357 - val_accuracy: 0.9381 - val_loss: 0.1556\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 272ms/step - accuracy: 0.9515 - loss: 0.1298 - val_accuracy: 0.9379 - val_loss: 0.1562\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 268ms/step - accuracy: 0.9522 - loss: 0.1236 - val_accuracy: 0.9384 - val_loss: 0.1592\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 278ms/step - accuracy: 0.9557 - loss: 0.1171 - val_accuracy: 0.9386 - val_loss: 0.1610\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.1294, Accuracy: 0.9499\n",
      "F1 Score for three_input_cnn_freq on split_1: 0.9456\n",
      "\n",
      "--- Processing Split: split_2 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: one_input_cnn_raw on split_2 ---\n",
      "Training data shapes: (21010, 300, 1), labels=(21010,)\n",
      "Test data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 163ms/step - accuracy: 0.5603 - loss: 0.6851 - val_accuracy: 0.5676 - val_loss: 0.6803\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.6094 - loss: 0.6627 - val_accuracy: 0.5369 - val_loss: 0.6957\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 218ms/step - accuracy: 0.5570 - loss: 0.6875 - val_accuracy: 0.5781 - val_loss: 0.6784\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 217ms/step - accuracy: 0.6063 - loss: 0.6559 - val_accuracy: 0.6840 - val_loss: 0.5726\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 212ms/step - accuracy: 0.7452 - loss: 0.5060 - val_accuracy: 0.6790 - val_loss: 0.6044\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 213ms/step - accuracy: 0.7647 - loss: 0.4789 - val_accuracy: 0.7587 - val_loss: 0.4877\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 205ms/step - accuracy: 0.7999 - loss: 0.4278 - val_accuracy: 0.7968 - val_loss: 0.4288\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 205ms/step - accuracy: 0.8085 - loss: 0.4087 - val_accuracy: 0.8113 - val_loss: 0.3934\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 201ms/step - accuracy: 0.8162 - loss: 0.3996 - val_accuracy: 0.8248 - val_loss: 0.3900\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 198ms/step - accuracy: 0.8221 - loss: 0.3869 - val_accuracy: 0.8115 - val_loss: 0.4090\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.8242 - loss: 0.3798 - val_accuracy: 0.8329 - val_loss: 0.3691\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 178ms/step - accuracy: 0.8362 - loss: 0.3590 - val_accuracy: 0.8444 - val_loss: 0.3574\n",
      "Epoch 13/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.8421 - loss: 0.3477 - val_accuracy: 0.8370 - val_loss: 0.3711\n",
      "Epoch 14/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 178ms/step - accuracy: 0.8382 - loss: 0.3563 - val_accuracy: 0.8375 - val_loss: 0.3643\n",
      "Epoch 15/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - accuracy: 0.8425 - loss: 0.3460 - val_accuracy: 0.8505 - val_loss: 0.3299\n",
      "Epoch 16/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 176ms/step - accuracy: 0.8547 - loss: 0.3222 - val_accuracy: 0.8608 - val_loss: 0.3235\n",
      "Epoch 17/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - accuracy: 0.8582 - loss: 0.3160 - val_accuracy: 0.8634 - val_loss: 0.3098\n",
      "Epoch 18/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - accuracy: 0.8673 - loss: 0.3039 - val_accuracy: 0.8693 - val_loss: 0.3014\n",
      "Epoch 19/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 176ms/step - accuracy: 0.8669 - loss: 0.3023 - val_accuracy: 0.8505 - val_loss: 0.3355\n",
      "Epoch 20/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 177ms/step - accuracy: 0.8654 - loss: 0.3107 - val_accuracy: 0.8641 - val_loss: 0.3256\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.3090, Accuracy: 0.8629\n",
      "F1 Score for one_input_cnn_raw on split_2: 0.8479\n",
      "\n",
      "--- Evaluating Model: one_input_cnn_filtered on split_2 ---\n",
      "Training data shapes: (21010, 300, 1), labels=(21010,)\n",
      "Test data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - accuracy: 0.5918 - loss: 0.6676 - val_accuracy: 0.5942 - val_loss: 0.6450\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.7977 - loss: 0.4959 - val_accuracy: 0.6894 - val_loss: 0.5984\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7415 - loss: 0.5329 - val_accuracy: 0.8596 - val_loss: 0.3559\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.8537 - loss: 0.3407 - val_accuracy: 0.8936 - val_loss: 0.2537\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.8948 - loss: 0.2501 - val_accuracy: 0.9131 - val_loss: 0.2188\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.9128 - loss: 0.2087 - val_accuracy: 0.9208 - val_loss: 0.1980\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.9248 - loss: 0.1854 - val_accuracy: 0.9272 - val_loss: 0.1874\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.9297 - loss: 0.1737 - val_accuracy: 0.9241 - val_loss: 0.1948\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - accuracy: 0.9327 - loss: 0.1686 - val_accuracy: 0.9236 - val_loss: 0.1913\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.9348 - loss: 0.1625 - val_accuracy: 0.9260 - val_loss: 0.1905\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.9391 - loss: 0.1569 - val_accuracy: 0.9234 - val_loss: 0.1925\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9404 - loss: 0.1543 - val_accuracy: 0.9255 - val_loss: 0.1908\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.2112, Accuracy: 0.9105\n",
      "F1 Score for one_input_cnn_filtered on split_2: 0.9030\n",
      "\n",
      "--- Evaluating Model: three_input_cnn_freq on split_2 ---\n",
      "Training data shapes: {'raw_input': (21010, 300, 1), 'filtered_input': (21010, 300, 1), 'stft_input': (21010, 13, 1)}, labels=(21010,)\n",
      "Test data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1), 'stft_input': (21740, 13, 1)}, labels=(21740,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 321ms/step - accuracy: 0.6263 - loss: 0.6394 - val_accuracy: 0.6490 - val_loss: 0.6215\n",
      "Epoch 2/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 299ms/step - accuracy: 0.8256 - loss: 0.4227 - val_accuracy: 0.7649 - val_loss: 0.4037\n",
      "Epoch 3/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 297ms/step - accuracy: 0.8645 - loss: 0.3269 - val_accuracy: 0.8589 - val_loss: 0.3128\n",
      "Epoch 4/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 291ms/step - accuracy: 0.8995 - loss: 0.2500 - val_accuracy: 0.8986 - val_loss: 0.2513\n",
      "Epoch 5/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 286ms/step - accuracy: 0.9032 - loss: 0.2473 - val_accuracy: 0.8470 - val_loss: 0.3721\n",
      "Epoch 6/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 280ms/step - accuracy: 0.8676 - loss: 0.3237 - val_accuracy: 0.9041 - val_loss: 0.2353\n",
      "Epoch 7/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 282ms/step - accuracy: 0.9147 - loss: 0.2142 - val_accuracy: 0.9103 - val_loss: 0.2190\n",
      "Epoch 8/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 283ms/step - accuracy: 0.9160 - loss: 0.2160 - val_accuracy: 0.7311 - val_loss: 0.5022\n",
      "Epoch 9/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 290ms/step - accuracy: 0.8051 - loss: 0.4111 - val_accuracy: 0.8903 - val_loss: 0.2506\n",
      "Epoch 10/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 287ms/step - accuracy: 0.9077 - loss: 0.2283 - val_accuracy: 0.9127 - val_loss: 0.2119\n",
      "Epoch 11/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 285ms/step - accuracy: 0.9203 - loss: 0.2008 - val_accuracy: 0.9188 - val_loss: 0.2020\n",
      "Epoch 12/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 288ms/step - accuracy: 0.9278 - loss: 0.1842 - val_accuracy: 0.9208 - val_loss: 0.2010\n",
      "Epoch 13/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 287ms/step - accuracy: 0.9333 - loss: 0.1742 - val_accuracy: 0.8948 - val_loss: 0.2740\n",
      "Epoch 14/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 288ms/step - accuracy: 0.9191 - loss: 0.1987 - val_accuracy: 0.9124 - val_loss: 0.2120\n",
      "Epoch 15/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 279ms/step - accuracy: 0.9366 - loss: 0.1685 - val_accuracy: 0.9196 - val_loss: 0.2045\n",
      "Epoch 16/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 348ms/step - accuracy: 0.9398 - loss: 0.1601 - val_accuracy: 0.9210 - val_loss: 0.2047\n",
      "Epoch 17/20\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1847s\u001b[0m 14s/step - accuracy: 0.9415 - loss: 0.1545 - val_accuracy: 0.9231 - val_loss: 0.2042\n",
      "Training finished.\n",
      "Evaluating on split_2's test data...\n",
      "Loss: 0.2313, Accuracy: 0.9024\n",
      "F1 Score for three_input_cnn_freq on split_2: 0.8959\n",
      "\n",
      "--- Processing Split: split_3 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 238 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 15 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 238 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 15 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Not setting metadata\n",
      "54190 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 54190 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5320 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5320 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "54190 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 54190 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "5320 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 5320 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: one_input_cnn_raw on split_3 ---\n",
      "Training data shapes: (54190, 300, 1), labels=(54190,)\n",
      "Test data shapes: (5320, 300, 1), labels=(5320,)\n",
      "Building and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 189ms/step - accuracy: 0.6018 - loss: 0.6609 - val_accuracy: 0.6055 - val_loss: 0.6608\n",
      "Epoch 2/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 175ms/step - accuracy: 0.7148 - loss: 0.5570 - val_accuracy: 0.8013 - val_loss: 0.4425\n",
      "Epoch 3/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 174ms/step - accuracy: 0.8281 - loss: 0.4013 - val_accuracy: 0.8503 - val_loss: 0.3562\n",
      "Epoch 4/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 171ms/step - accuracy: 0.8658 - loss: 0.3120 - val_accuracy: 0.8710 - val_loss: 0.3142\n",
      "Epoch 5/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.8853 - loss: 0.2729 - val_accuracy: 0.8748 - val_loss: 0.2939\n",
      "Epoch 6/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 171ms/step - accuracy: 0.8944 - loss: 0.2535 - val_accuracy: 0.8948 - val_loss: 0.2542\n",
      "Epoch 7/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9001 - loss: 0.2408 - val_accuracy: 0.8966 - val_loss: 0.2543\n",
      "Epoch 8/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 172ms/step - accuracy: 0.9045 - loss: 0.2314 - val_accuracy: 0.8980 - val_loss: 0.2430\n",
      "Epoch 9/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9058 - loss: 0.2265 - val_accuracy: 0.9011 - val_loss: 0.2305\n",
      "Epoch 10/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9061 - loss: 0.2227 - val_accuracy: 0.9095 - val_loss: 0.2143\n",
      "Epoch 11/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 169ms/step - accuracy: 0.9072 - loss: 0.2202 - val_accuracy: 0.9086 - val_loss: 0.2150\n",
      "Epoch 12/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 171ms/step - accuracy: 0.9114 - loss: 0.2155 - val_accuracy: 0.9094 - val_loss: 0.2132\n",
      "Epoch 13/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 172ms/step - accuracy: 0.9148 - loss: 0.2095 - val_accuracy: 0.9085 - val_loss: 0.2114\n",
      "Epoch 14/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9159 - loss: 0.2056 - val_accuracy: 0.9112 - val_loss: 0.2106\n",
      "Epoch 15/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9165 - loss: 0.2030 - val_accuracy: 0.9172 - val_loss: 0.1931\n",
      "Epoch 16/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9201 - loss: 0.1946 - val_accuracy: 0.9151 - val_loss: 0.1967\n",
      "Epoch 17/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 169ms/step - accuracy: 0.9211 - loss: 0.1925 - val_accuracy: 0.9183 - val_loss: 0.1894\n",
      "Epoch 18/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 171ms/step - accuracy: 0.9223 - loss: 0.1902 - val_accuracy: 0.9148 - val_loss: 0.2045\n",
      "Epoch 19/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 169ms/step - accuracy: 0.9227 - loss: 0.1899 - val_accuracy: 0.9215 - val_loss: 0.1868\n",
      "Epoch 20/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 170ms/step - accuracy: 0.9250 - loss: 0.1830 - val_accuracy: 0.9220 - val_loss: 0.1879\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.1962, Accuracy: 0.9190\n",
      "F1 Score for one_input_cnn_raw on split_3: 0.8997\n",
      "\n",
      "--- Evaluating Model: one_input_cnn_filtered on split_3 ---\n",
      "Training data shapes: (54190, 300, 1), labels=(54190,)\n",
      "Test data shapes: (5320, 300, 1), labels=(5320,)\n",
      "Building and compiling model...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 188ms/step - accuracy: 0.6741 - loss: 0.5824 - val_accuracy: 0.8797 - val_loss: 0.3038\n",
      "Epoch 2/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 186ms/step - accuracy: 0.8632 - loss: 0.3338 - val_accuracy: 0.8600 - val_loss: 0.3148\n",
      "Epoch 3/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 185ms/step - accuracy: 0.8768 - loss: 0.2805 - val_accuracy: 0.9206 - val_loss: 0.1917\n",
      "Epoch 4/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 184ms/step - accuracy: 0.9173 - loss: 0.1987 - val_accuracy: 0.9342 - val_loss: 0.1624\n",
      "Epoch 5/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 185ms/step - accuracy: 0.9291 - loss: 0.1783 - val_accuracy: 0.9396 - val_loss: 0.1534\n",
      "Epoch 6/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 182ms/step - accuracy: 0.9334 - loss: 0.1671 - val_accuracy: 0.9443 - val_loss: 0.1442\n",
      "Epoch 7/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 184ms/step - accuracy: 0.9357 - loss: 0.1607 - val_accuracy: 0.9374 - val_loss: 0.1792\n",
      "Epoch 8/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 184ms/step - accuracy: 0.9364 - loss: 0.1679 - val_accuracy: 0.9470 - val_loss: 0.1369\n",
      "Epoch 9/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 182ms/step - accuracy: 0.9370 - loss: 0.1565 - val_accuracy: 0.9481 - val_loss: 0.1338\n",
      "Epoch 10/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 179ms/step - accuracy: 0.9385 - loss: 0.1532 - val_accuracy: 0.9466 - val_loss: 0.1371\n",
      "Epoch 11/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 182ms/step - accuracy: 0.9376 - loss: 0.1537 - val_accuracy: 0.9475 - val_loss: 0.1304\n",
      "Epoch 12/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 183ms/step - accuracy: 0.9394 - loss: 0.1492 - val_accuracy: 0.9492 - val_loss: 0.1272\n",
      "Epoch 13/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 182ms/step - accuracy: 0.9403 - loss: 0.1472 - val_accuracy: 0.9502 - val_loss: 0.1256\n",
      "Epoch 14/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 183ms/step - accuracy: 0.9406 - loss: 0.1449 - val_accuracy: 0.9499 - val_loss: 0.1250\n",
      "Epoch 15/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 179ms/step - accuracy: 0.9412 - loss: 0.1427 - val_accuracy: 0.9504 - val_loss: 0.1243\n",
      "Epoch 16/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 177ms/step - accuracy: 0.9416 - loss: 0.1413 - val_accuracy: 0.9502 - val_loss: 0.1244\n",
      "Epoch 17/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 176ms/step - accuracy: 0.9421 - loss: 0.1396 - val_accuracy: 0.9502 - val_loss: 0.1248\n",
      "Epoch 18/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 177ms/step - accuracy: 0.9431 - loss: 0.1382 - val_accuracy: 0.9491 - val_loss: 0.1243\n",
      "Epoch 19/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 175ms/step - accuracy: 0.9432 - loss: 0.1367 - val_accuracy: 0.9497 - val_loss: 0.1239\n",
      "Epoch 20/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 174ms/step - accuracy: 0.9438 - loss: 0.1353 - val_accuracy: 0.9498 - val_loss: 0.1235\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.1239, Accuracy: 0.9496\n",
      "F1 Score for one_input_cnn_filtered on split_3: 0.9399\n",
      "\n",
      "--- Evaluating Model: three_input_cnn_freq on split_3 ---\n",
      "Training data shapes: {'raw_input': (54190, 300, 1), 'filtered_input': (54190, 300, 1), 'stft_input': (54190, 13, 1)}, labels=(54190,)\n",
      "Test data shapes: {'raw_input': (5320, 300, 1), 'filtered_input': (5320, 300, 1), 'stft_input': (5320, 13, 1)}, labels=(5320,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 483ms/step - accuracy: 0.6694 - loss: 0.5929 - val_accuracy: 0.8120 - val_loss: 0.4051\n",
      "Epoch 2/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 440ms/step - accuracy: 0.8420 - loss: 0.3454 - val_accuracy: 0.8968 - val_loss: 0.2359\n",
      "Epoch 3/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 437ms/step - accuracy: 0.8971 - loss: 0.2330 - val_accuracy: 0.9222 - val_loss: 0.1842\n",
      "Epoch 4/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 441ms/step - accuracy: 0.9039 - loss: 0.2417 - val_accuracy: 0.9318 - val_loss: 0.1683\n",
      "Epoch 5/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 444ms/step - accuracy: 0.9262 - loss: 0.1824 - val_accuracy: 0.9326 - val_loss: 0.1646\n",
      "Epoch 6/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 443ms/step - accuracy: 0.9301 - loss: 0.1726 - val_accuracy: 0.9407 - val_loss: 0.1498\n",
      "Epoch 7/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 440ms/step - accuracy: 0.9342 - loss: 0.1623 - val_accuracy: 0.9434 - val_loss: 0.1409\n",
      "Epoch 8/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 437ms/step - accuracy: 0.9367 - loss: 0.1549 - val_accuracy: 0.9478 - val_loss: 0.1350\n",
      "Epoch 9/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 443ms/step - accuracy: 0.9391 - loss: 0.1504 - val_accuracy: 0.9484 - val_loss: 0.1325\n",
      "Epoch 10/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 440ms/step - accuracy: 0.9394 - loss: 0.1494 - val_accuracy: 0.9462 - val_loss: 0.1326\n",
      "Epoch 11/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 376ms/step - accuracy: 0.9409 - loss: 0.1451 - val_accuracy: 0.9460 - val_loss: 0.1302\n",
      "Epoch 12/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 311ms/step - accuracy: 0.9406 - loss: 0.1476 - val_accuracy: 0.9172 - val_loss: 0.2027\n",
      "Epoch 13/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 320ms/step - accuracy: 0.9254 - loss: 0.1835 - val_accuracy: 0.9412 - val_loss: 0.1486\n",
      "Epoch 14/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 298ms/step - accuracy: 0.9377 - loss: 0.1566 - val_accuracy: 0.9389 - val_loss: 0.1469\n",
      "Epoch 15/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 297ms/step - accuracy: 0.9416 - loss: 0.1498 - val_accuracy: 0.9430 - val_loss: 0.1397\n",
      "Epoch 16/20\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 294ms/step - accuracy: 0.9434 - loss: 0.1456 - val_accuracy: 0.9447 - val_loss: 0.1377\n",
      "Training finished.\n",
      "Evaluating on split_3's test data...\n",
      "Loss: 0.1405, Accuracy: 0.9492\n",
      "F1 Score for three_input_cnn_freq on split_3: 0.9393\n",
      "\n",
      "--- Processing Split: split_4 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 367 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m test_raw \u001b[38;5;241m=\u001b[39m raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# this is for each split\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Spindle detection on raw data for raw/three-input model labels\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m spindles_train_times_raw \u001b[38;5;241m=\u001b[39m detect_spindles_times(train_raw, do_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, do_downsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m spindles_test_times_raw \u001b[38;5;241m=\u001b[39m detect_spindles_times(test_raw, do_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, do_downsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m spindles_starts_train_raw, spindles_ends_train_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mspindles_train_times_raw) \u001b[38;5;28;01mif\u001b[39;00m spindles_train_times_raw \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mdetect_spindles_times\u001b[1;34m(eeg_raw, do_filter, do_downsample, downsample_rate)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2. Downsample at 100 Hz (100 samples per second)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_downsample:\n\u001b[1;32m---> 13\u001b[0m     data\u001b[38;5;241m.\u001b[39mresample(downsample_rate)\n\u001b[0;32m     15\u001b[0m sfreq \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[0;32m     16\u001b[0m channel_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_data()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m<decorator-gen-193>:12\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(self, sfreq, npad, window, stim_picks, n_jobs, events, pad, method, verbose)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\mne\\io\\base.py:1438\u001b[0m, in \u001b[0;36mBaseRaw.resample\u001b[1;34m(self, sfreq, npad, window, stim_picks, n_jobs, events, pad, method, verbose)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreload:\n\u001b[0;32m   1437\u001b[0m     data_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[:, offsets[ri] : offsets[ri \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m-> 1438\u001b[0m     new_data[:, this_sl] \u001b[38;5;241m=\u001b[39m resample(data_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# In empirical testing, it was faster to resample all channels\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;66;03m# (above) and then replace the stim channels than it was to\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m     \u001b[38;5;66;03m# only resample the proper subset of channels and then use\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;66;03m# np.insert() to restore the stims.\u001b[39;00m\n\u001b[0;32m   1443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stim_picks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m<decorator-gen-18>:12\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, up, down, axis, window, n_jobs, pad, npad, method, verbose)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\mne\\filter.py:1880\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1878\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(pad\u001b[38;5;241m=\u001b[39mpad, window\u001b[38;5;241m=\u001b[39mwindow, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[0;32m   1879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1880\u001b[0m     y \u001b[38;5;241m=\u001b[39m _resample_fft(x, npad\u001b[38;5;241m=\u001b[39mnpad, ratio\u001b[38;5;241m=\u001b[39mratio, final_len\u001b[38;5;241m=\u001b[39mfinal_len, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1882\u001b[0m     up, down, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _prep_polyphase(\n\u001b[0;32m   1883\u001b[0m         ratio, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], final_len, window\n\u001b[0;32m   1884\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\mne\\filter.py:1976\u001b[0m, in \u001b[0;36m_resample_fft\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1974\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(x_flat), new_len \u001b[38;5;241m-\u001b[39m to_removes\u001b[38;5;241m.\u001b[39msum()), dtype\u001b[38;5;241m=\u001b[39mx_flat\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi, x_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x_flat):\n\u001b[1;32m-> 1976\u001b[0m         y[xi] \u001b[38;5;241m=\u001b[39m _fft_resample(x_, new_len, npads, to_removes, cuda_dict, pad)\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1978\u001b[0m     y \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m   1979\u001b[0m         p_fun(x_, new_len, npads, to_removes, cuda_dict, pad) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x_flat\n\u001b[0;32m   1980\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\mne\\cuda.py:354\u001b[0m, in \u001b[0;36m_fft_resample\u001b[1;34m(x, new_len, npads, to_removes, cuda_dict, pad)\u001b[0m\n\u001b[0;32m    352\u001b[0m     x_fft[nyq : nyq \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shorter \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    353\u001b[0m x_fft \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cuda_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 354\u001b[0m y \u001b[38;5;241m=\u001b[39m cuda_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mirfft\u001b[39m\u001b[38;5;124m\"\u001b[39m](x_fft, new_len)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# now let's trim it back to the correct size (if there was padding)\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (to_removes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[0m, in \u001b[0;36m_ScipyBackend.__ua_function__\u001b[1;34m(method, args, kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:97\u001b[0m, in \u001b[0;36mirfft\u001b[1;34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mirfft\u001b[39m(x, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m           overwrite_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, plan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _execute_1D(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mirfft\u001b[39m\u001b[38;5;124m'\u001b[39m, _pocketfft\u001b[38;5;241m.\u001b[39mirfft, x, n\u001b[38;5;241m=\u001b[39mn, axis\u001b[38;5;241m=\u001b[39maxis, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m     98\u001b[0m                        overwrite_x\u001b[38;5;241m=\u001b[39moverwrite_x, workers\u001b[38;5;241m=\u001b[39mworkers, plan\u001b[38;5;241m=\u001b[39mplan)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[0m, in \u001b[0;36m_execute_1D\u001b[1;34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pocketfft_func(x, n\u001b[38;5;241m=\u001b[39mn, axis\u001b[38;5;241m=\u001b[39maxis, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m     33\u001b[0m                           overwrite_x\u001b[38;5;241m=\u001b[39moverwrite_x, workers\u001b[38;5;241m=\u001b[39mworkers, plan\u001b[38;5;241m=\u001b[39mplan)\n\u001b[0;32m     35\u001b[0m norm \u001b[38;5;241m=\u001b[39m _validate_fft_args(workers, plan, norm)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:95\u001b[0m, in \u001b[0;36mc2r\u001b[1;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[0;32m     92\u001b[0m     tmp, _ \u001b[38;5;241m=\u001b[39m _fix_shape_1d(tmp, (n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, axis)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Note: overwrite_x is not utilized\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pfft\u001b[38;5;241m.\u001b[39mc2r(tmp, (axis,), n, forward, norm, \u001b[38;5;28;01mNone\u001b[39;00m, workers)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we want to evaluate the models on all these scores\n",
    "model_metrics = {\n",
    "    'one_input_cnn_raw': {\n",
    "        'f1_scores': [],\n",
    "        'accuracy_scores': [],\n",
    "        'loss_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'one_input_cnn_filtered': {\n",
    "        'f1_scores': [],\n",
    "        'accuracy_scores': [],\n",
    "        'loss_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'three_input_cnn_freq': {\n",
    "        'f1_scores': [],\n",
    "        'accuracy_scores': [],\n",
    "        'loss_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# List of models to evaluate\n",
    "models_to_evaluate = {\n",
    "    'one_input_cnn_raw': build_cnn_model_downsampled,\n",
    "    'one_input_cnn_filtered': build_cnn_model_downsampled,\n",
    "    # this has the same model architecture but different input data\n",
    "    'three_input_cnn_freq': build_multi_input_cnn_model_freq\n",
    "}\n",
    "\n",
    "# then we go through each split\n",
    "# so create a for loop\n",
    "for split_name, raw_data in raw_splits.items():\n",
    "    print(f\"\\n--- Processing Split: {split_name} ---\")\n",
    "\n",
    "    train_raw = raw_data['train']\n",
    "    test_raw = raw_data['test']\n",
    "\n",
    "    # this is for each split\n",
    "\n",
    "    # Slow oscillation detection on raw data for raw/three-input model labels\n",
    "    so_train_times_raw_downsampled = detect_slow_oscillations_times(train_raw, do_filter=True, do_downsample=True)\n",
    "    so_test_times_raw_downsampled = detect_slow_oscillations_times(test_raw, do_filter=True, do_downsample=True)\n",
    "    so_starts_train_raw_downsampled, so_ends_train_raw_downsampled = zip(*so_train_times_raw_downsampled) if so_train_times_raw_downsampled else ([], [])\n",
    "    so_starts_test_raw_downsampled, so_ends_test_raw_downsampled = zip(*so_test_times_raw_downsampled) if so_test_times_raw_downsampled else ([], [])\n",
    "    # Downsample raw data for one input\n",
    "    train_raw_downsampled = train_raw.copy().resample(100)\n",
    "    test_raw_downsampled = test_raw.copy().resample(100)\n",
    "\n",
    "    # Filtered data for filtered input and downsample\n",
    "    train_filtered_downsampled = train_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "    test_filtered_downsampled = test_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "    train_filtered_downsampled.resample(100)\n",
    "    test_filtered_downsampled.resample(100)\n",
    "    # resample because already copied before\n",
    "\n",
    "    # SO detection on filtered and downsampled data for filtered model labels\n",
    "    so_train_times_filtered_downsampled = detect_slow_oscillations_times(train_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "    so_test_times_filtered_downsampled = detect_slow_oscillations_times(test_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "    # since filtering and downsampling before, do not filter and downsample again in function\n",
    "\n",
    "    so_starts_train_filtered_downsampled, so_ends_train_filtered_downsampled = zip(*so_train_times_filtered_downsampled) if so_train_times_filtered_downsampled else([],[])\n",
    "    so_starts_test_filtered_downsampled, so_ends_test_filtered_downsampled = zip(*so_test_times_filtered_downsampled) if so_test_times_filtered_downsampled else([],[])\n",
    "\n",
    "\n",
    "    # then fixed length epochs (using downsampled raw and downsampled and filtered data)\n",
    "    epochs_train_raw_downsampled = create_fixed_length_epochs(train_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "    epochs_test_raw_downsampled = create_fixed_length_epochs(test_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "    epochs_train_filtered_downsampled = create_fixed_length_epochs(train_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "    epochs_test_filtered_downsampled = create_fixed_length_epochs(test_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "\n",
    "    # Prepare STFT input (using downsampled raw data)\n",
    "    epochs_train_stft_downsampled = np.squeeze(np.array(epochs_train_raw_downsampled))\n",
    "    epochs_test_stft_downsampled = np.squeeze(np.array(epochs_test_raw_downsampled))\n",
    "\n",
    "    fs = train_raw_downsampled.info['sfreq']\n",
    "    nperseg = 50\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    X_train_stft_transformed = []\n",
    "    for epoch in epochs_train_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_train_stft_transformed.append(spectrogram)\n",
    "    X_train_stft_transformed = np.array(X_train_stft_transformed)\n",
    "\n",
    "    X_test_stft_transformed = []\n",
    "    for epoch in epochs_test_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_test_stft_transformed.append(spectrogram)\n",
    "    X_test_stft_transformed = np.array(X_test_stft_transformed)\n",
    "\n",
    "    # Reduce STFT dimension to frequency axis only\n",
    "    X_train_stft_freq = np.mean(X_train_stft_transformed, axis=1)\n",
    "    X_test_stft_freq = np.mean(X_test_stft_transformed, axis=1)\n",
    "    X_train_stft_freq = X_train_stft_freq[..., np.newaxis] # Add channel dimension\n",
    "    X_test_stft_freq = X_test_stft_freq[..., np.newaxis] # Add channel dimension\n",
    "\n",
    "    # Normalize STFT frequency data\n",
    "    X_train_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_train_stft_freq\n",
    "    ])\n",
    "    X_test_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_test_stft_freq\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Prepare raw and filtered epoch data (reshaping)\n",
    "    X_train_raw = np.array(epochs_train_raw_downsampled).reshape(len(epochs_train_raw_downsampled), -1, 1)\n",
    "    X_test_raw = np.array(epochs_test_raw_downsampled).reshape(len(epochs_test_raw_downsampled), -1, 1)\n",
    "\n",
    "    X_train_filtered = np.array(epochs_train_filtered_downsampled).reshape(len(epochs_train_filtered_downsampled), -1, 1)\n",
    "    X_test_filtered = np.array(epochs_test_filtered_downsampled).reshape(len(epochs_test_filtered_downsampled), -1, 1)\n",
    "\n",
    "\n",
    "    # still in the same split\n",
    "    # now iterate through the models\n",
    "    for model_name, build_model_func in models_to_evaluate.items():\n",
    "        print(f\"\\n--- Evaluating Model: {model_name} on {split_name} ---\")\n",
    "\n",
    "        # Prepare data and labels based on model input requirements\n",
    "        if model_name == 'one_input_cnn_raw':\n",
    "            X_train_input = X_train_raw\n",
    "            X_test_input = X_test_raw\n",
    "            y_train = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "            y_test = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "            input_shape = (X_train_input.shape[1], X_train_input.shape[2])\n",
    "\n",
    "        elif model_name == 'one_input_cnn_filtered':\n",
    "             X_train_input = X_train_filtered\n",
    "             X_test_input = X_test_filtered\n",
    "             y_train = label_so_epochs_moderate(epochs_train_filtered_downsampled, so_starts_train_filtered_downsampled, so_ends_train_filtered_downsampled)\n",
    "             y_test = label_so_epochs_moderate(epochs_test_filtered_downsampled, so_starts_test_filtered_downsampled, so_ends_test_filtered_downsampled)\n",
    "             input_shape = (X_train_input.shape[1], X_train_input.shape[2])\n",
    "\n",
    "        elif model_name == 'three_input_cnn_freq':\n",
    "            X_train_input = {\n",
    "                'raw_input': X_train_raw,\n",
    "                'filtered_input': X_train_filtered,\n",
    "                'stft_input': X_train_stft_freq_norm \n",
    "            }\n",
    "            X_test_input = {\n",
    "                'raw_input': X_test_raw,\n",
    "                'filtered_input': X_test_filtered,\n",
    "                'stft_input': X_test_stft_freq_norm \n",
    "            }\n",
    "            # Labels for the three-input model come from the raw downsampled data\n",
    "            y_train = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "            y_test = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "\n",
    "            input_shape = None # Shape is handled by the model definition for multi-input\n",
    "\n",
    "        print(f\"Training data shapes: { {k: v.shape for k, v in X_train_input.items()} if isinstance(X_train_input, dict) else X_train_input.shape}, labels={y_train.shape}\")\n",
    "        print(f\"Test data shapes: { {k: v.shape for k, v in X_test_input.items()} if isinstance(X_test_input, dict) else X_test_input.shape}, labels={y_test.shape}\")\n",
    "        # to check whether a dictionary or not \n",
    "        # because it is a dictionary for the three-input model\n",
    "        # but not for the other models\n",
    "\n",
    "\n",
    "        # build the models\n",
    "\n",
    "        print(\"Building and compiling model...\")\n",
    "        if model_name in ['one_input_cnn_raw', 'one_input_cnn_filtered']:\n",
    "             model = build_model_func(input_shape)\n",
    "        else:\n",
    "            model = build_model_func()\n",
    "\n",
    "\n",
    "        # Define early stopping\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        # Use a validation split from the training data if desired, or train on the full training set\n",
    "        # For cross-validation on pre-defined splits, it's common to train on the full training set\n",
    "        history = model.fit(\n",
    "            X_train_input,\n",
    "            y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=20, # Adjust epochs as needed\n",
    "            batch_size=128, # Adjust batch size as needed\n",
    "            callbacks=[early_stop], # Optional: Use early stopping\n",
    "        )\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # Evaluate the model on the test data of the current split\n",
    "        print(f\"Evaluating on {split_name}'s test data...\")\n",
    "        loss, accuracy = model.evaluate(X_test_input, y_test, verbose=0)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Get predictions and calculate F1 score\n",
    "        y_pred_proba = model.predict(X_test_input, verbose=0)\n",
    "        y_pred_labels = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        \n",
    "        split_f1 = f1_score(y_test, y_pred_labels)\n",
    "        split_precision = precision_score(y_test, y_pred_labels)\n",
    "        split_recall = recall_score(y_test, y_pred_labels)\n",
    "        print(f\"F1 Score for {model_name} on {split_name}: {split_f1:.4f}\")\n",
    "        print(f\"Precision for {model_name} on {split_name}: {split_precision:.4f}\")\n",
    "        print(f\"Recall for {model_name} on {split_name}: {split_recall:.4f}\")\n",
    "\n",
    "        # Store the metrics for the current model type and split\n",
    "        model_metrics[model_name]['f1_scores'].append(split_f1)\n",
    "        model_metrics[model_name]['accuracy_scores'].append(accuracy)\n",
    "        model_metrics[model_name]['loss_scores'].append(loss)\n",
    "        model_metrics[model_name]['precision_scores'].append(split_precision)\n",
    "        model_metrics[model_name]['recall_scores'].append(split_recall)\n",
    "\n",
    "        # Clear TensorFlow session to free up memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n--- Evaluation finished for all models across all splits ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0467a81e-5d36-4808-a9a5-08c7694ae3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Average Metrics Across Splits ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'precision_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m std_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m average_precision \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m std_precision \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m average_recall \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'precision_scores'"
     ]
    }
   ],
   "source": [
    "# Step 6: Calculate and display average metrics for all models across splits\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    average_f1 = np.mean(metrics['f1_scores'])\n",
    "    std_f1 = np.std(metrics['f1_scores'])\n",
    "\n",
    "    average_accuracy = np.mean(metrics['accuracy_scores'])\n",
    "    std_accuracy = np.std(metrics['accuracy_scores'])\n",
    "\n",
    "    average_loss = np.mean(metrics['loss_scores'])\n",
    "    std_loss = np.std(metrics['loss_scores'])\n",
    "\n",
    "    average_precision = np.mean(metrics['precision_scores'])\n",
    "    std_precision = np.std(metrics['precision_scores'])\n",
    "\n",
    "    average_recall = np.mean(metrics['recall_scores'])\n",
    "    std_recall = np.std(metrics['recall_scores'])\n",
    "\n",
    "    # Append a row as a dict\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"F1 Score (mean ± std)\": f\"{average_f1:.4f} ± {std_f1:.4f}\",\n",
    "        \"Precision (mean ± std)\": f\"{average_precision:.4f} ± {std_precision:.4f}\",\n",
    "        \"Recall (mean ± std)\": f\"{average_recall:.4f} ± {std_recall:.4f}\",\n",
    "        \"Accuracy (mean ± std)\": f\"{average_accuracy:.4f} ± {std_accuracy:.4f}\",\n",
    "        \"Loss (mean ± std)\": f\"{average_loss:.4f} ± {std_loss:.4f}\"\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a title and the table\n",
    "print(\"\\n--- Average Metrics Across Splits For SO Detection ---\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Step 7: Perform statistical test\n",
    "\n",
    "# Prepare the data for the Friedman test\n",
    "# Each row is a split, each column is a model\n",
    "data_for_friedman = [\n",
    "    model_metrics['one_input_cnn_raw']['f1_scores'],\n",
    "    model_metrics['one_input_cnn_filtered']['f1_scores'],\n",
    "    model_metrics['three_input_cnn_freq']['f1_scores']\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Friedman Test for Comparing F1 Scores Across Splits ---\")\n",
    "\n",
    "# Check if we have enough data points for the Friedman test (at least k=3 models and n>=2 splits/ranks)\n",
    "if len(data_for_friedman[0]) >= 2 and len(data_for_friedman) >= 3:\n",
    "    statistic, p_value = friedmanchisquare(*data_for_friedman)\n",
    "\n",
    "    print(f\"Statistic: {statistic:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"\\nInterpretation: The p-value is less than the significance level (alpha = 0.05).\")\n",
    "        print(\"This suggests that there is a statistically significant difference in the median F1 scores among the three models across the splits.\")\n",
    "        print(\"You may want to perform post-hoc tests (e.g., Wilcoxon signed-rank tests with a multiple comparison correction like Bonferroni or Holm) to determine which specific model pairs are significantly different.\")\n",
    "    else:\n",
    "        print(\"\\nInterpretation: The p-value is greater than the significance level (alpha = 0.05).\")\n",
    "        print(\"This suggests that there is no statistically significant difference in the median F1 scores among the three models across the splits.\")\n",
    "else:\n",
    "    print(\"\\nCannot perform Friedman test: Need at least 3 models and 2 splits with valid F1 scores.\")\n",
    "\n",
    "# You now have the individual scores stored in the model_metrics dictionary\n",
    "# if you want to perform further analysis on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b5d8b-55b1-453b-90d3-1275f9ef7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate and display average metrics for all models across splits\n",
    "# This part is already in the previous cell, but we'll re-calculate averages for the table/plot\n",
    "\n",
    "model_names = list(model_metrics.keys())\n",
    "average_f1s = [np.mean(model_metrics[name]['f1_scores']) for name in model_names]\n",
    "std_f1s = [np.std(model_metrics[name]['f1_scores']) for name in model_names]\n",
    "\n",
    "# Create a summary table (DataFrame)\n",
    "summary_data = {\n",
    "    'Model': model_names,\n",
    "    'Average F1 Score (Label 1)': average_f1s,\n",
    "    'Standard Deviation (F1 Score)': std_f1s\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n--- Summary Table of Average Metrics Across Splits for SO Detection ---\")\n",
    "display(summary_df)\n",
    "\n",
    "# Create a bar plot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(summary_df['Model'], summary_df['Average F1 Score (Label 1)'], yerr=summary_df['Standard Deviation (F1 Score)'], capsize=5, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "\n",
    "# Add the average F1 score on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.4f}', va='bottom', ha='center') # va: vertical alignment\n",
    "\n",
    "plt.ylabel(\"Average F1 Score (Label 1)\")\n",
    "plt.title(\"Average F1 Score (Label 1) per Model Across 5 Splits\")\n",
    "plt.ylim(0, 1.05) # F1 score is between 0 and 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
