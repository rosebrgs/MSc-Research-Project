{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53912b6f-0832-4e91-b068-27583c9a7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista\n",
    "import ipywidgets\n",
    "import ipyevents\n",
    "import pyvistaqt\n",
    "import yasa\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc, precision_score, recall_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import stft\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "\n",
    "import pywt\n",
    "import cv2\n",
    "\n",
    "SEED = 15\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7692555-7c48-480d-a32f-6fe1f0fbcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1dd96-bb6c-47de-9642-b7df2c49f262",
   "metadata": {},
   "source": [
    "## CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23cf70f-8364-46b1-9471-022dd77508a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_downsampled(input_shape=(300,1)):\n",
    "\n",
    "    # linear embedding layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Three convolutional blocks (like having three pattern detectors)\n",
    "\n",
    "    # First convolution block, kernel size of 5\n",
    "    padded1 = tf.keras.layers.ZeroPadding1D(padding=2)(input_layer)\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=10, kernel_size=5, strides=1, padding='valid')(padded1)\n",
    "    # each filter learns a different type of short-time feature\n",
    "    # stride of 1, moves one step at a time\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv1)\n",
    "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # K = 2\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "\n",
    "    # Second convolution block, kernel size of 11\n",
    "    padded2 = tf.keras.layers.ZeroPadding1D(padding=5)(input_layer)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=10, kernel_size=11, strides=1, padding='valid')(padded2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv2)\n",
    "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "\n",
    "    # Third convolution block, kernel size of 21\n",
    "    padded3 = tf.keras.layers.ZeroPadding1D(padding=10)(input_layer)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=10, kernel_size=21, strides=1, padding='valid')(padded3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv3)\n",
    "    conv3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "\n",
    "    # Concatenate the outputs of all blocks\n",
    "    concatenated = tf.keras.layers.Concatenate()([conv1, conv2, conv3])\n",
    "\n",
    "    # GRU Layer\n",
    "    gru = tf.keras.layers.GRU(64)(concatenated)\n",
    "\n",
    "    # Fully connected (dense) layer\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(gru)\n",
    "    # add a Dropout layer to prevent overfitting\n",
    "    #dense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    # Two softmax outputs for dual-task classification\n",
    "    #output_task1 = tf.keras.layers.Dense(2, activation='softmax', name='task1')(dense)\n",
    "    #output_task2 = tf.keras.layers.Dense(2, activation='softmax', name='task2')(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create the model\n",
    "    #model = tf.keras.models.Model(inputs=input_layer, outputs=[output_task1, output_task2])\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    #model.compile(optimizer='adam', loss={'task1': 'categorical_crossentropy', 'task2': 'categorical_crossentropy'}, metrics={'task1': 'accuracy', 'task2': 'accuracy'})\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649e50c-1cdc-4338-b6f1-e4c296d00765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_filtered():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered_so = tf.keras.Input(shape=(300, 1), name='filtered_so_input')\n",
    "    input_filtered_spindles = tf.keras.Input(shape=(300, 1), name='filtered_spindles_input') \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered_so = conv_branch(input_filtered_so)\n",
    "    branch_filtered_spindles = conv_branch(input_filtered_spindles)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered_so = tf.keras.layers.GRU(64)(branch_filtered_so)\n",
    "    gru_filtered_spindles = tf.keras.layers.GRU(64)(branch_filtered_spindles)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered_so, gru_filtered_spindles])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered_so, input_filtered_spindles], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e006ebc-5171-4135-91f3-ee0ae19663cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_freq():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered_so = tf.keras.Input(shape=(300, 1), name='filtered_so_input')\n",
    "    input_filtered_spindles = tf.keras.Input(shape=(300, 1), name='filtered_spindles_input')\n",
    "    input_stft = tf.keras.Input(shape=(13, 1), name='stft_input')  \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered_so = conv_branch(input_filtered_so)\n",
    "    branch_filtered_spindles = conv_branch(input_filtered_spindles)\n",
    "    branch_stft = conv_branch(input_stft)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered_so = tf.keras.layers.GRU(64)(branch_filtered_so)\n",
    "    gru_filtered_spindles = tf.keras.layers.GRU(64)(branch_filtered_spindles)\n",
    "    gru_stft = tf.keras.layers.GRU(64)(branch_stft)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered_so, gru_filtered_spindles, gru_stft])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered_so, input_filtered_spindles, input_stft], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca32c0-040b-4a8a-bd3a-7c9629d3e5c5",
   "metadata": {},
   "source": [
    "## Spindle detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff46e4c-4ed8-4b62-8743-2a668fd2e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spindles_times(eeg_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "    \n",
    "    # 1. Filter between 12 and 16 Hz\n",
    "    \n",
    "    data = eeg_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=12, h_freq=16)\n",
    "    \n",
    "    # 2. Downsample at 100 Hz (100 samples per second)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "    \n",
    "    sfreq = data.info['sfreq']  \n",
    "    channel_data = data.get_data()[0]\n",
    "    # extract the filtered data\n",
    "    \n",
    "    \n",
    "    # 3: Calculate amplitude by applying Hilbert transformation\n",
    "\n",
    "    hilbert_signal = hilbert(channel_data)\n",
    "    # apply hilbert transformation to bandpassed data\n",
    "    # gives analytic signal with amplitude and phase information\n",
    "    envelope = np.abs(hilbert_signal)\n",
    "    # take the absolute part of the hilbert signal\n",
    "    # also the instantaneous power of the signal\n",
    "    # gives the envelope: amplitude modulation\n",
    "    # how strength of oscillations change over time\n",
    "    # size of sliding window\n",
    "    \n",
    "    # 4: Perform smoothing with a sliding window of 0.2 seconds\n",
    "    # this removes high-frequency noise\n",
    "    \n",
    "    sliding_window = int(0.2 * sfreq)\n",
    "    smoothed_envelope = np.convolve(envelope, np.ones(sliding_window) / sliding_window, mode='same')\n",
    "    # convolving envelope with a uniform filter over the sliding window\n",
    "    # convolution takes rolling average of 20 samples at a time\n",
    "    # smooth the signal with the average of values in the window\n",
    "    # in the smoothed envelope, can detect regions with higher amplitude \n",
    "    # which is when a spindle event occurs\n",
    "    # np.ones: creates a filter kernel\n",
    "    # have a filter where the sum of all elements equals 1\n",
    "    # this filter is replaced by the average of the 20 surrounding samples\n",
    "    # convolution between envelope and averaging filter\n",
    "    # mode = 'same': so that output of convolution has same length as original envelope\n",
    "\n",
    "    # 5. Define spindle detection threshold\n",
    "\n",
    "    threshold = np.percentile(smoothed_envelope, 75)\n",
    "    spindle_threshold = smoothed_envelope > threshold\n",
    "    #threshold = np.mean(smoothed_envelope) + 1.5 * np.std(smoothed_envelope)\n",
    "    #spindle_threshold = smoothed_envelope > threshold\n",
    "    # threshold is 75th percentile of the smoothed envelope\n",
    "    # will look at the duration later\n",
    "    \n",
    "    # 6. Detect spindles and define peaks and troughs for visualisation\n",
    "    \n",
    "    spindles = []\n",
    "    # initialize list with spindles\n",
    "    above_threshold = np.where(spindle_threshold)[0]\n",
    "    # returns indices where signal above the threshold\n",
    "    stacked_spindles = []\n",
    "    # initialize list for stacking the spindles for the visualisation\n",
    "    # contains aligned spindles at peak\n",
    "    \n",
    "    if len(above_threshold) > 0:\n",
    "        # checking it's not empty\n",
    "        start_idx = above_threshold[0]\n",
    "        # would be the start of a potential spindle\n",
    "        for i in range(1, len(above_threshold)):\n",
    "            if above_threshold[i] > above_threshold[i - 1] + 1:  \n",
    "                # if above threshold[1] > above_threshold[0] + 1\n",
    "                # because all indices should be separated by 1\n",
    "                # so here detects gaps\n",
    "                # so starting from the second index\n",
    "                # and comparing each index to the one before\n",
    "                end_idx = above_threshold[i - 1]\n",
    "                # so if above condition is true, this is the end of the spindle\n",
    "                duration = (end_idx - start_idx) / sfreq\n",
    "                if 0.5 <= duration <= 3:\n",
    "                    # only keep spindles lasting 0.5 to 3 seconds\n",
    "                    segment = channel_data[start_idx:end_idx]\n",
    "                    # extract EEG segment corresponding to detected spindle\n",
    "                    peak_idx = start_idx + np.argmax(segment) \n",
    "                    # extract the peak of the spindle\n",
    "                    # this will be useful for later\n",
    "                    spindles.append((start_idx / sfreq, end_idx / sfreq))\n",
    "                    # all the spindles are stored in spindles\n",
    "                    \n",
    "                    # Aligning spindles at peak for visualization\n",
    "                    before_peak_idx = max(0, peak_idx - int(1.5 * sfreq))\n",
    "                    # still in the for loop, so this is the peak index of individual peak\n",
    "                    after_peak_idx = min(len(channel_data), peak_idx + int(1.5 * sfreq))\n",
    "                    # extracting 1.5 seconds before and after peak\n",
    "                    # max and min are used for out of bounds situations at the start and end of EEG data\n",
    "                    aligned_segment = channel_data[before_peak_idx:after_peak_idx]\n",
    "                    stacked_spindles.append(aligned_segment)\n",
    "                    # the aligned segment is saved in stacked spindles\n",
    "                \n",
    "                start_idx = above_threshold[i]\n",
    "                # update the start index for the for loop\n",
    "\n",
    "        # then need to process the final spindle\n",
    "        end_idx = above_threshold[-1]\n",
    "        duration = (end_idx - start_idx) / sfreq\n",
    "        if 0.5 <= duration <= 3:\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            peak_idx = start_idx + np.argmax(segment)\n",
    "            spindles.append((start_idx / sfreq, end_idx / sfreq))\n",
    "\n",
    "            before_peak_idx = max(0, peak_idx - int(1.5 * sfreq))\n",
    "            after_peak_idx = min(len(channel_data), peak_idx + int(1.5 * sfreq))\n",
    "            aligned_segment = channel_data[before_peak_idx:after_peak_idx]\n",
    "            stacked_spindles.append(aligned_segment)\n",
    "    \n",
    "    return spindles\n",
    "    \n",
    "\n",
    "def detect_spindles_peaks(eeg_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "    \n",
    "    # 1. Filter between 12 and 16 Hz\n",
    "    \n",
    "    data = eeg_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=12, h_freq=16)\n",
    "    \n",
    "    # 2. Downsample at 100 Hz (100 samples per second)\n",
    "    \n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']  \n",
    "    # update to new sampling frequency\n",
    "    # because used later in the code\n",
    "    channel_data = data.get_data()[0]\n",
    "    # extract the filtered data\n",
    "    \n",
    "    # 3: Calculate amplitude by applying Hilbert transformation\n",
    "\n",
    "    hilbert_signal = hilbert(channel_data)\n",
    "    # apply hilbert transformation to bandpassed data\n",
    "    # gives analytic signal with amplitude and phase information\n",
    "    envelope = np.abs(hilbert_signal)\n",
    "    # take the absolute part of the hilbert signal\n",
    "    # also the instantaneous power of the signal\n",
    "    # gives the envelope: amplitude modulation\n",
    "    # how strength of oscillations change over time\n",
    "    # size of sliding window\n",
    "    \n",
    "    # 4: Perform smoothing with a sliding window of 0.2 seconds\n",
    "    # this removes high-frequency noise\n",
    "    \n",
    "    sliding_window = int(0.2 * sfreq)\n",
    "    smoothed_envelope = np.convolve(envelope, np.ones(sliding_window) / sliding_window, mode='same')\n",
    "    # convolving envelope with a uniform filter over the sliding window\n",
    "    # convolution takes rolling average of 20 samples at a time\n",
    "    # smooth the signal with the average of values in the window\n",
    "    # in the smoothed envelope, can detect regions with higher amplitude \n",
    "    # which is when a spindle event occurs\n",
    "    # np.ones: creates a filter kernel\n",
    "    # have a filter where the sum of all elements equals 1\n",
    "    # this filter is replaced by the average of the 20 surrounding samples\n",
    "    # convolution between envelope and averaging filter\n",
    "    # mode = 'same': so that output of convolution has same length as original envelope\n",
    "\n",
    "    # 5. Define spindle detection threshold\n",
    "\n",
    "    threshold = np.percentile(smoothed_envelope, 75)\n",
    "    spindle_threshold = smoothed_envelope > threshold\n",
    "    # 75th percentile as criteria\n",
    "\n",
    "    #threshold = np.mean(smoothed_envelope) + 1.5 * np.std(smoothed_envelope)\n",
    "    #spindle_threshold = smoothed_envelope > threshold\n",
    "    \n",
    "    # 6. Detect spindles and define peaks and troughs for visualisation\n",
    "    \n",
    "    spindles = []\n",
    "    # initialize list with spindles\n",
    "    above_threshold = np.where(spindle_threshold)[0]\n",
    "    # returns indices where signal above the threshold\n",
    "    stacked_spindles = []\n",
    "    # initialize list for stacking the spindles for the visualisation\n",
    "    # contains aligned spindles at peak\n",
    "    \n",
    "    if len(above_threshold) > 0:\n",
    "        # checking it's not empty\n",
    "        start_idx = above_threshold[0]\n",
    "        # would be the start of a potential spindle\n",
    "        for i in range(1, len(above_threshold)):\n",
    "            if above_threshold[i] > above_threshold[i - 1] + 1:  \n",
    "                # if above threshold[1] > above_threshold[0] + 1\n",
    "                # because all indices should be separated by 1\n",
    "                # so here detects gaps\n",
    "                end_idx = above_threshold[i - 1]\n",
    "                # so if above condition is true, this is the end of the spindle\n",
    "                duration = (end_idx - start_idx) / sfreq\n",
    "                if 0.5 <= duration <= 3:\n",
    "                    # only keep spindles lasting 0.5 to 3 seconds\n",
    "                    segment = channel_data[start_idx:end_idx]\n",
    "                    # extract EEG segment corresponding to detected spindle\n",
    "                    peak_idx = start_idx + np.argmax(segment) \n",
    "                    # extract the peak of the spindle\n",
    "                    # this will be useful for later\n",
    "                    #spindles.append(f\"Spindle detected from {start_idx / sfreq:.2f}s to {end_idx / sfreq:.2f}s, peak at {peak_idx / sfreq:.2f}s\")\n",
    "                    spindles.append((peak_idx / sfreq))\n",
    "                    # all the spindles are stored in spindles\n",
    "                    \n",
    "                    # Aligning spindles at peak for visualization\n",
    "                    before_peak_idx = max(0, peak_idx - int(1.5 * sfreq))\n",
    "                    # still in the for loop, so this is the peak index of individual peak\n",
    "                    after_peak_idx = min(len(channel_data), peak_idx + int(1.5 * sfreq))\n",
    "                    # extracting 1.5 seconds before and after peak\n",
    "                    # max and min are used for out of bounds situations at the start and end of EEG data\n",
    "                    aligned_segment = channel_data[before_peak_idx:after_peak_idx]\n",
    "                    stacked_spindles.append(aligned_segment)\n",
    "                    # the aligned segment is saved in stacked spindles\n",
    "                \n",
    "                start_idx = above_threshold[i]\n",
    "                # update the start index for the for loop\n",
    "\n",
    "        # then need to process the final spindle\n",
    "        end_idx = above_threshold[-1]\n",
    "        duration = (end_idx - start_idx) / sfreq\n",
    "        if 0.5 <= duration <= 3:\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            peak_idx = start_idx + np.argmax(segment)\n",
    "            spindles.append((peak_idx / sfreq))\n",
    "\n",
    "            before_peak_idx = max(0, peak_idx - int(1.5 * sfreq))\n",
    "            after_peak_idx = min(len(channel_data), peak_idx + int(1.5 * sfreq))\n",
    "            aligned_segment = channel_data[before_peak_idx:after_peak_idx]\n",
    "            stacked_spindles.append(aligned_segment)\n",
    "\n",
    "    \n",
    "    return spindles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74010b3-521c-44b2-b095-9b56d318cf3d",
   "metadata": {},
   "source": [
    "## Slow oscillation detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3c8bcc-7f78-48f3-99ab-9bdf104e5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_slow_oscillations_times(combined_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "    #signs = np.sign(current_data)\n",
    "    #pos_to_neg = np.where((signs[:-1] > 0) & (signs[1:] < 0))[0]\n",
    "    # detect +1 to -1\n",
    "    #neg_to_pos = np.where((signs[:-1] <  0) & (signs[1:] > 0))[0]\n",
    "    # detect -1 to +1\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices = []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings)-1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "\n",
    "        # find the negative to positive crossing in between\n",
    "        #mid_crossings = neg_to_pos[(neg_to_pos > start_idx) & (neg_to_pos < end_idx)]\n",
    "\n",
    "        #if len(mid_crossings) != 1:\n",
    "            #continue\n",
    "\n",
    "        #mid_idx = mid_crossings [0]\n",
    "\n",
    "        #duration = (end_idx - start_idx) / sfreq\n",
    "        #if not (0.8 <= duration <= 2.0):\n",
    "  \n",
    "        \n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    # keep lowest negative peaks (under the 25th percentile)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "    # keep largest peak-to-peak amplitude (over 75th percentile)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation\n",
    "\n",
    "def detect_slow_oscillations_peaks(combined_raw, do_filter=True, do_downsample=True, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    slow_oscillations_peaks = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices =  []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings) - 1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            slow_oscillations_peaks.append((negative_peak, positive_peak))\n",
    "\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c0ff76-96fe-4305-ad0b-a90c91e7bbb8",
   "metadata": {},
   "source": [
    "## Coupling detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089aba57-b25a-4ef7-a5b0-28ff7d0abe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_slow_oscillations_spindles_coupling_so_times(combined_raw, do_filter=True, do_downsample=True, downsample_rate=100):\n",
    "    slow_oscillations_peaks = detect_slow_oscillations_peaks(combined_raw, do_filter=do_filter, do_downsample=do_downsample, downsample_rate=downsample_rate)\n",
    "    slow_oscillations_times = detect_slow_oscillations_times(combined_raw, do_filter=do_filter, do_downsample=do_downsample, downsample_rate=downsample_rate)\n",
    "    spindles_peaks = detect_spindles_peaks(combined_raw, do_filter=do_filter, do_downsample=do_downsample, downsample_rate=downsample_rate)\n",
    "\n",
    "    coupling_times = []\n",
    "    coupling_times_so = []\n",
    "\n",
    "    # first detect the coupling events\n",
    "    for (start_time, end_time), (negative_peak, positive_peak) in zip(slow_oscillations_times, slow_oscillations_peaks):\n",
    "        for peak in spindles_peaks:\n",
    "            if negative_peak < peak < end_time:\n",
    "                coupling_times.append(peak)\n",
    "                # if the peak of the spindle is between the negative and positive trough\n",
    "                # add it to list of coupling times\n",
    "\n",
    "    # then calculate the slow oscillation length\n",
    "    for start_time, end_time in slow_oscillations_times:\n",
    "        current_start_time = start_time\n",
    "        current_end_time = end_time\n",
    "        for coupling_peak in coupling_times:\n",
    "            if current_start_time < coupling_peak < current_end_time:\n",
    "                coupling_times_so.append((current_start_time, current_end_time))\n",
    "\n",
    "    return coupling_times_so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cea46-853b-4428-bb10-633e542db5d6",
   "metadata": {},
   "source": [
    "## Epochs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d211ff33-e059-4360-97e5-861d5104e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_length_epochs(raw, duration=3.0, overlap=0.0, preload=True, reject_by_annotation=False):\n",
    "\n",
    "    return mne.make_fixed_length_epochs(\n",
    "        raw,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=preload,\n",
    "        reject_by_annotation=reject_by_annotation\n",
    "    )\n",
    "# function mne.make_fixed_length_epochs takes into account the sampling frequency of the data\n",
    "\n",
    "\n",
    "def label_coupling_epochs_strict(epochs, coupling_starts, coupling_ends, epoch_length_sec=3.0):\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "    for coupling_start, coupling_end in zip(coupling_starts, coupling_ends):\n",
    "        coupling_duration = coupling_end - coupling_start\n",
    "        required_overlap = 0.8 * coupling_duration  \n",
    "        # only label 1 if epoch contains 50% of the SO duration\n",
    "\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "\n",
    "            # Calculate overlap between coupling and epoch\n",
    "            overlap_start = max(coupling_start, epoch_start)\n",
    "            overlap_end = min(coupling_end, epoch_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration >= required_overlap:\n",
    "                epoch_labels[i] = 1\n",
    "\n",
    "    return epoch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fae6b-c6f3-4d41-9d9e-94bf8b069555",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4b0a26-649b-4b9b-ba8f-6b6e306e667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for split_1...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 32700095 =    180.000 ... 65400.190 secs\n",
      "Ready.\n",
      "Reading 0 ... 32610095  =      0.000 ... 65220.190 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_1_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 7845026 =   2940.000 ... 15690.052 secs\n",
      "Ready.\n",
      "Reading 0 ... 6375026  =      0.000 ... 12750.052 secs...\n",
      "Loaded train and test data for split_1\n",
      "Loading data for split_2...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32985091 =   2940.000 ... 65970.182 secs\n",
      "Ready.\n",
      "Reading 0 ... 31515091  =      0.000 ... 63030.182 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_2_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 90000 ... 7560030 =    180.000 ... 15120.060 secs\n",
      "Ready.\n",
      "Reading 0 ... 7470030  =      0.000 ... 14940.060 secs...\n",
      "Loaded train and test data for split_2\n",
      "Loading data for split_3...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32475107 =   2940.000 ... 64950.214 secs\n",
      "Ready.\n",
      "Reading 0 ... 31005107  =      0.000 ... 62010.214 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_3_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 300000 ... 8280014 =    600.000 ... 16560.028 secs\n",
      "Ready.\n",
      "Reading 0 ... 7980014  =      0.000 ... 15960.028 secs...\n",
      "Loaded train and test data for split_3\n",
      "Loading data for split_4...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 32100090 =   2940.000 ... 64200.180 secs\n",
      "Ready.\n",
      "Reading 0 ... 30630090  =      0.000 ... 61260.180 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_4_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 825000 ... 9180031 =   1650.000 ... 18360.062 secs\n",
      "Ready.\n",
      "Reading 0 ... 8355031  =      0.000 ... 16710.062 secs...\n",
      "Loaded train and test data for split_4\n",
      "Loading data for split_5...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 31650104 =   2940.000 ... 63300.208 secs\n",
      "Ready.\n",
      "Reading 0 ... 30180104  =      0.000 ... 60360.208 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_5_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 165000 ... 8970017 =    330.000 ... 17940.034 secs\n",
      "Ready.\n",
      "Reading 0 ... 8805017  =      0.000 ... 17610.034 secs...\n",
      "Loaded train and test data for split_5\n"
     ]
    }
   ],
   "source": [
    "# for 5-fold validation\n",
    "# load the all the files needed that were pre-processed before\n",
    "# from train_1_raw and test_1_raw to train_5_raw and test_5_raw\n",
    "split_files = {\n",
    "    f'split_{i}': {\n",
    "        'train': fr\"C:\\EEG DATA\\combined_sets\\train_{i}_large_raw.fif\",\n",
    "        'test': fr\"C:\\EEG DATA\\combined_sets\\test_{i}_large_raw.fif\"\n",
    "    } for i in range(1, 6) \n",
    "}\n",
    "\n",
    "raw_splits = {}\n",
    "for split_name, files in split_files.items():\n",
    "    print(f\"Loading data for {split_name}...\")\n",
    "    try:\n",
    "        train_raw = mne.io.read_raw_fif(files['train'], preload=True)\n",
    "        test_raw = mne.io.read_raw_fif(files['test'], preload=True)\n",
    "        raw_splits[split_name] = {'train': train_raw, 'test': test_raw}\n",
    "        print(f\"Loaded train and test data for {split_name}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found for {split_name}: {e}\")\n",
    "        # error in case the file does not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {split_name}: {e}\")\n",
    "        # errors in case not loading data\n",
    "\n",
    "        # error statements useful if running this notebook on another laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9b435-9aff-48ce-a113-3ee82572009d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bbfa1-7748-42fa-9987-266c1992dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Split: split_1 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 96 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21740 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4250 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4250 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: one_input_cnn on split_1 ---\n",
      "Training data shapes: (21740, 300, 1), labels=(21740,)\n",
      "Test data shapes: (4250, 300, 1), labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 116ms/step - accuracy: 0.5630 - loss: 0.6829 - val_accuracy: 0.5584 - val_loss: 0.6818\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.6277 - loss: 0.6515 - val_accuracy: 0.6789 - val_loss: 0.6132\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.6702 - loss: 0.6035 - val_accuracy: 0.6391 - val_loss: 0.6110\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - accuracy: 0.6152 - loss: 0.6469 - val_accuracy: 0.5345 - val_loss: 0.6915\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.5911 - loss: 0.6700 - val_accuracy: 0.5964 - val_loss: 0.6575\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.6353 - loss: 0.6422 - val_accuracy: 0.6095 - val_loss: 0.6597\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 110ms/step - accuracy: 0.6527 - loss: 0.6225 - val_accuracy: 0.6693 - val_loss: 0.6105\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 101ms/step - accuracy: 0.7023 - loss: 0.5745 - val_accuracy: 0.6014 - val_loss: 0.6472\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 101ms/step - accuracy: 0.6609 - loss: 0.6227 - val_accuracy: 0.6355 - val_loss: 0.6518\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 99ms/step - accuracy: 0.7397 - loss: 0.5329 - val_accuracy: 0.7397 - val_loss: 0.5217\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 108ms/step - accuracy: 0.7800 - loss: 0.4829 - val_accuracy: 0.6884 - val_loss: 0.5870\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 109ms/step - accuracy: 0.7280 - loss: 0.5342 - val_accuracy: 0.7236 - val_loss: 0.5285\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7749 - loss: 0.4770 - val_accuracy: 0.7705 - val_loss: 0.4696\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7674 - loss: 0.4911 - val_accuracy: 0.6755 - val_loss: 0.5919\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 105ms/step - accuracy: 0.7794 - loss: 0.4674 - val_accuracy: 0.7532 - val_loss: 0.5543\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 104ms/step - accuracy: 0.8151 - loss: 0.3966 - val_accuracy: 0.8172 - val_loss: 0.3977\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 104ms/step - accuracy: 0.8278 - loss: 0.3721 - val_accuracy: 0.8344 - val_loss: 0.3603\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 104ms/step - accuracy: 0.8370 - loss: 0.3565 - val_accuracy: 0.8441 - val_loss: 0.3489\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 105ms/step - accuracy: 0.8425 - loss: 0.3464 - val_accuracy: 0.8496 - val_loss: 0.3335\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 104ms/step - accuracy: 0.8422 - loss: 0.3482 - val_accuracy: 0.8360 - val_loss: 0.3546\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.3464, Accuracy: 0.8409\n",
      "F1 Score for one_input_cnn on split_1: 0.8220\n",
      "Precision for one_input_cnn on split_1: 0.8433\n",
      "Recall for one_input_cnn on split_1: 0.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-Jul-25 14:14:25 | WARNING | From C:\\Users\\roseb\\anaconda3\\envs\\msc_research_project\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model: two_input_cnn on split_1 ---\n",
      "Training data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1)}, labels=(21740,)\n",
      "Test data shapes: {'raw_input': (4250, 300, 1), 'filtered_input': (4250, 300, 1)}, labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 260ms/step - accuracy: 0.6207 - loss: 0.6527 - val_accuracy: 0.4632 - val_loss: 0.7164\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 199ms/step - accuracy: 0.6154 - loss: 0.6052 - val_accuracy: 0.7447 - val_loss: 0.4998\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.7984 - loss: 0.4361 - val_accuracy: 0.8735 - val_loss: 0.2876\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 226ms/step - accuracy: 0.8717 - loss: 0.2918 - val_accuracy: 0.8873 - val_loss: 0.2606\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 233ms/step - accuracy: 0.8879 - loss: 0.2594 - val_accuracy: 0.8786 - val_loss: 0.3194\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 227ms/step - accuracy: 0.8620 - loss: 0.3222 - val_accuracy: 0.8917 - val_loss: 0.2491\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 227ms/step - accuracy: 0.8845 - loss: 0.2664 - val_accuracy: 0.9062 - val_loss: 0.2210\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9026 - loss: 0.2297 - val_accuracy: 0.9144 - val_loss: 0.2092\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 268ms/step - accuracy: 0.8840 - loss: 0.2681 - val_accuracy: 0.9066 - val_loss: 0.2261\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 246ms/step - accuracy: 0.9013 - loss: 0.2379 - val_accuracy: 0.9121 - val_loss: 0.2101\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 250ms/step - accuracy: 0.9099 - loss: 0.2188 - val_accuracy: 0.9195 - val_loss: 0.1966\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 233ms/step - accuracy: 0.9170 - loss: 0.2052 - val_accuracy: 0.9243 - val_loss: 0.1881\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 243ms/step - accuracy: 0.9213 - loss: 0.1966 - val_accuracy: 0.9243 - val_loss: 0.1825\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 249ms/step - accuracy: 0.9240 - loss: 0.1901 - val_accuracy: 0.9278 - val_loss: 0.1776\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 238ms/step - accuracy: 0.9280 - loss: 0.1842 - val_accuracy: 0.9315 - val_loss: 0.1737\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 248ms/step - accuracy: 0.9305 - loss: 0.1793 - val_accuracy: 0.9324 - val_loss: 0.1709\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 251ms/step - accuracy: 0.9334 - loss: 0.1750 - val_accuracy: 0.9349 - val_loss: 0.1682\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 246ms/step - accuracy: 0.9357 - loss: 0.1709 - val_accuracy: 0.9379 - val_loss: 0.1660\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 239ms/step - accuracy: 0.9369 - loss: 0.1671 - val_accuracy: 0.9388 - val_loss: 0.1639\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 241ms/step - accuracy: 0.9371 - loss: 0.1647 - val_accuracy: 0.9384 - val_loss: 0.1630\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.1381, Accuracy: 0.9473\n",
      "F1 Score for two_input_cnn on split_1: 0.9433\n",
      "Precision for two_input_cnn on split_1: 0.9305\n",
      "Recall for two_input_cnn on split_1: 0.9563\n",
      "\n",
      "--- Evaluating Model: three_input_cnn on split_1 ---\n",
      "Training data shapes: {'raw_input': (21740, 300, 1), 'filtered_input': (21740, 300, 1), 'stft_input': (21740, 13, 1)}, labels=(21740,)\n",
      "Test data shapes: {'raw_input': (4250, 300, 1), 'filtered_input': (4250, 300, 1), 'stft_input': (4250, 13, 1)}, labels=(4250,)\n",
      "Building and compiling model...\n",
      "Training the model...\n",
      "Epoch 1/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 330ms/step - accuracy: 0.6337 - loss: 0.6433 - val_accuracy: 0.5754 - val_loss: 0.6584\n",
      "Epoch 2/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 297ms/step - accuracy: 0.6429 - loss: 0.5835 - val_accuracy: 0.7790 - val_loss: 0.5140\n",
      "Epoch 3/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 302ms/step - accuracy: 0.8133 - loss: 0.4447 - val_accuracy: 0.8034 - val_loss: 0.4719\n",
      "Epoch 4/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 290ms/step - accuracy: 0.8587 - loss: 0.3491 - val_accuracy: 0.8804 - val_loss: 0.2839\n",
      "Epoch 5/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 287ms/step - accuracy: 0.8896 - loss: 0.2626 - val_accuracy: 0.8960 - val_loss: 0.2435\n",
      "Epoch 6/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 277ms/step - accuracy: 0.9000 - loss: 0.2330 - val_accuracy: 0.9108 - val_loss: 0.2176\n",
      "Epoch 7/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 274ms/step - accuracy: 0.9093 - loss: 0.2157 - val_accuracy: 0.9241 - val_loss: 0.1903\n",
      "Epoch 8/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 284ms/step - accuracy: 0.9202 - loss: 0.1901 - val_accuracy: 0.9299 - val_loss: 0.1778\n",
      "Epoch 9/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 283ms/step - accuracy: 0.9271 - loss: 0.1789 - val_accuracy: 0.9312 - val_loss: 0.1736\n",
      "Epoch 10/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 285ms/step - accuracy: 0.9310 - loss: 0.1718 - val_accuracy: 0.9319 - val_loss: 0.1714\n",
      "Epoch 11/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 258ms/step - accuracy: 0.9342 - loss: 0.1652 - val_accuracy: 0.9326 - val_loss: 0.1700\n",
      "Epoch 12/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 256ms/step - accuracy: 0.9360 - loss: 0.1608 - val_accuracy: 0.9342 - val_loss: 0.1673\n",
      "Epoch 13/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 250ms/step - accuracy: 0.9382 - loss: 0.1578 - val_accuracy: 0.9370 - val_loss: 0.1675\n",
      "Epoch 14/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 261ms/step - accuracy: 0.9412 - loss: 0.1532 - val_accuracy: 0.9372 - val_loss: 0.1651\n",
      "Epoch 15/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 259ms/step - accuracy: 0.9421 - loss: 0.1496 - val_accuracy: 0.9363 - val_loss: 0.1641\n",
      "Epoch 16/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 260ms/step - accuracy: 0.9452 - loss: 0.1427 - val_accuracy: 0.9388 - val_loss: 0.1637\n",
      "Epoch 17/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 259ms/step - accuracy: 0.9470 - loss: 0.1363 - val_accuracy: 0.9393 - val_loss: 0.1642\n",
      "Epoch 18/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 256ms/step - accuracy: 0.9503 - loss: 0.1289 - val_accuracy: 0.9379 - val_loss: 0.1675\n",
      "Epoch 19/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 254ms/step - accuracy: 0.9529 - loss: 0.1226 - val_accuracy: 0.9368 - val_loss: 0.1697\n",
      "Epoch 20/20\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 257ms/step - accuracy: 0.9527 - loss: 0.1215 - val_accuracy: 0.9354 - val_loss: 0.1730\n",
      "Training finished.\n",
      "Evaluating on split_1's test data...\n",
      "Loss: 0.1328, Accuracy: 0.9471\n",
      "F1 Score for three_input_cnn on split_1: 0.9423\n",
      "Precision for three_input_cnn on split_1: 0.9411\n",
      "Recall for three_input_cnn on split_1: 0.9435\n",
      "\n",
      "--- Processing Split: split_2 ---\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 31 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 92 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "Filtering raw data in 31 contiguous segments\n",
      "Setting up band-pass filter from 12 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 551 samples (1.102 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4980 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4980 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "21010 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 21010 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "4980 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 4980 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "--- Evaluating Model: one_input_cnn on split_2 ---\n"
     ]
    }
   ],
   "source": [
    "# we want to evaluate the models on all these scores\n",
    "model_metrics = {\n",
    "    'raw': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'raw_and_filtered': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    },\n",
    "    'raw_and_filtered_and_stft': {\n",
    "        'f1_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# will evaluate the three models\n",
    "# model 1: raw\n",
    "# model 2: raw + filtered\n",
    "# model 3: raw + filtered + STFT frequency\n",
    "models_to_evaluate = {\n",
    "    'raw': build_cnn_model_downsampled,\n",
    "    'raw_and_filtered': build_multi_input_cnn_model_filtered,\n",
    "    'raw_and_filtered_and_stft': build_multi_input_cnn_model_freq\n",
    "}\n",
    "\n",
    "# then we go through each split\n",
    "# so create a for loop\n",
    "for split_name, raw_data in raw_splits.items():\n",
    "    print(f\"\\n--- Processing Split: {split_name} ---\")\n",
    "    # when running the code for a long time, allows you to know at which stage it's at\n",
    "    train_raw = raw_data['train']\n",
    "    test_raw = raw_data['test']\n",
    "\n",
    "    # this is for each split\n",
    "\n",
    "    # Slow oscillation detection on raw data for raw/three-input model labels\n",
    "    coupling_train_times_raw_downsampled = detect_slow_oscillations_spindles_coupling_so_times(train_raw, do_filter=True, do_downsample=True)\n",
    "    coupling_test_times_raw_downsampled = detect_slow_oscillations_spindles_coupling_so_times(test_raw, do_filter=True, do_downsample=True)\n",
    "\n",
    "    coupling_starts_train_raw_downsampled, coupling_ends_train_raw_downsampled = zip(*coupling_train_times_raw_downsampled) if coupling_train_times_raw_downsampled else([],[])\n",
    "    coupling_starts_test_raw_downsampled, coupling_ends_test_raw_downsampled = zip(*coupling_test_times_raw_downsampled) if coupling_test_times_raw_downsampled else([],[])\n",
    "    # Downsample raw data for one input\n",
    "    train_raw_downsampled = train_raw.copy().resample(100)\n",
    "    test_raw_downsampled = test_raw.copy().resample(100)\n",
    "\n",
    "    # Filtered data for filtered input and downsample\n",
    "    train_filtered_downsampled_so = train_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "    test_filtered_downsampled_so = test_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    # Downsample to 100 Hz\n",
    "    train_filtered_downsampled_so = train_filtered_downsampled_so.resample(100)\n",
    "    test_filtered_downsampled_so = test_filtered_downsampled_so.resample(100)\n",
    "    # resample because already copied before\n",
    "\n",
    "    # Apply bandpass filter between 12 and 16 Hz\n",
    "    train_filtered_downsampled_spindles = train_raw.copy().filter(l_freq=12, h_freq=16)\n",
    "    test_filtered_downsampled_spindles = test_raw.copy().filter(l_freq=12, h_freq=16)\n",
    "\n",
    "    # Downsample to 100 Hz\n",
    "    train_filtered_downsampled_spindles = train_filtered_downsampled_spindles.resample(100)\n",
    "    test_filtered_downsampled_spindles = test_filtered_downsampled_spindles.resample(100)\n",
    "\n",
    "    # so detection for model 2\n",
    "    #coupling_train_times_filtered_downsampled_so = detect_slow_oscillations_spindles_coupling_so_times(train_filtered_downsampled_so, do_filter=False, do_downsample=False)\n",
    "    #coupling_test_times_filtered_downsampled_so = detect_slow_oscillations_spindles_coupling_so_times(test_filtered_downsampled_so, do_filter=False, do_downsample=False)\n",
    "    # since filtering and downsampling before, do not filter and downsample again in function\n",
    "\n",
    "    #coupling_starts_train_filtered_downsampled_so, coupling_ends_train_filtered_downsampled_so = zip(*coupling_train_times_filtered_downsampled_so) if coupling_train_times_filtered_downsampled_so else([],[])\n",
    "    #coupling_starts_test_filtered_downsampled_so, coupling_ends_test_filtered_downsampled_so = zip(*coupling_test_times_filtered_downsampled_so) if coupling_test_times_filtered_downsampled_so else([],[])\n",
    "\n",
    "    # spindle detection for model 2\n",
    "    #coupling_train_times_filtered_downsampled_spindles = detect_slow_oscillations_spindles_coupling_so_times(train_filtered_downsampled_spindles, do_filter=False, do_downsample=False)\n",
    "    #coupling_test_times_filtered_downsampled_spindles = detect_slow_oscillations_spindles_coupling_so_times(test_filtered_downsampled_spindles, do_filter=False, do_downsample=False)\n",
    "\n",
    "    #coupling_starts_train_filtered_downsampled_spindles, coupling_ends_train_filtered_downsampled_spindles = zip(*coupling_train_times_filtered_downsampled_spindles) if coupling_train_times_filtered_downsampled_spindles else([],[])\n",
    "    #coupling_starts_test_filtered_downsampled_spindles, coupling_ends_test_filtered_downsampled_spindles = zip(*coupling_test_times_filtered_downsampled_spindles) if coupling_test_times_filtered_downsampled_spindles else([],[])\n",
    "\n",
    "    # create fixed length epochs, are of 3 seconds each\n",
    "    epochs_train_raw_downsampled = create_fixed_length_epochs(train_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "    epochs_test_raw_downsampled = create_fixed_length_epochs(test_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "    epochs_train_filtered_downsampled_so = create_fixed_length_epochs(train_filtered_downsampled_so)\n",
    "    epochs_test_filtered_downsampled_so = create_fixed_length_epochs(test_filtered_downsampled_so)\n",
    "\n",
    "    epochs_train_filtered_downsampled_spindles = create_fixed_length_epochs(train_filtered_downsampled_spindles)\n",
    "    epochs_test_filtered_downsampled_spindles = create_fixed_length_epochs(test_filtered_downsampled_spindles)\n",
    "    \n",
    "    # STFT input for model 3\n",
    "    # created on epochs from raw downsampled data\n",
    "    epochs_train_stft_downsampled = np.squeeze(np.array(epochs_train_raw_downsampled))\n",
    "    epochs_test_stft_downsampled = np.squeeze(np.array(epochs_test_raw_downsampled))\n",
    "\n",
    "    fs = train_raw_downsampled.info['sfreq']\n",
    "    nperseg = 50\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    X_train_stft_transformed = []\n",
    "    for epoch in epochs_train_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_train_stft_transformed.append(spectrogram)\n",
    "    X_train_stft_transformed = np.array(X_train_stft_transformed)\n",
    "\n",
    "    X_test_stft_transformed = []\n",
    "    for epoch in epochs_test_stft_downsampled:\n",
    "        f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        spectrogram = np.abs(Zxx)\n",
    "        X_test_stft_transformed.append(spectrogram)\n",
    "    X_test_stft_transformed = np.array(X_test_stft_transformed)\n",
    "\n",
    "    # only keep the frequency dimension of STFT\n",
    "    X_train_stft_freq = np.mean(X_train_stft_transformed, axis=1)\n",
    "    X_test_stft_freq = np.mean(X_test_stft_transformed, axis=1)\n",
    "    X_train_stft_freq = X_train_stft_freq[..., np.newaxis] \n",
    "    # to have correct input size for CNN, adds channel dimension\n",
    "    X_test_stft_freq = X_test_stft_freq[..., np.newaxis] \n",
    "\n",
    "    # normalize per epoch\n",
    "    X_train_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_train_stft_freq\n",
    "    ])\n",
    "    X_test_stft_freq_norm = np.array([\n",
    "        (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "        for epoch in X_test_stft_freq\n",
    "    ])\n",
    "\n",
    "\n",
    "    # reshape the epochs for model 1 and model 2\n",
    "    X_train_raw = np.array(epochs_train_raw_downsampled).reshape(len(epochs_train_raw_downsampled), -1, 1)\n",
    "    X_test_raw = np.array(epochs_test_raw_downsampled).reshape(len(epochs_test_raw_downsampled), -1, 1)\n",
    "\n",
    "    X_train_filtered_so = np.array(epochs_train_filtered_downsampled_so).reshape(len(epochs_train_filtered_downsampled_so), -1, 1)\n",
    "    X_test_filtered_so = np.array(epochs_test_filtered_downsampled_so).reshape(len(epochs_test_filtered_downsampled_so), -1, 1)\n",
    "\n",
    "    X_train_filtered_spindles = np.array(epochs_train_filtered_downsampled_spindles).reshape(len(epochs_train_filtered_downsampled_spindles), -1, 1)\n",
    "    X_test_filtered_spindles = np.array(epochs_test_filtered_downsampled_spindles).reshape(len(epochs_test_filtered_downsampled_spindles), -1, 1)\n",
    "\n",
    "\n",
    "    # still in the same split\n",
    "    # now iterate through the models\n",
    "    for model_name, build_model_func in models_to_evaluate.items():\n",
    "        print(f\"\\n--- Evaluating Model: {model_name} on {split_name} ---\")\n",
    "\n",
    "        # here define X and y sets\n",
    "        # y set defined by assigning labels\n",
    "        if model_name == 'raw':\n",
    "            X_train_input = X_train_raw\n",
    "            X_test_input = X_test_raw\n",
    "            y_train = label_coupling_epochs_strict(epochs_train_raw_downsampled, coupling_starts_train_raw_downsampled, coupling_ends_train_raw_downsampled)\n",
    "            y_test = label_coupling_epochs_strict(epochs_test_raw_downsampled, coupling_starts_test_raw_downsampled, coupling_ends_test_raw_downsampled)\n",
    "            input_shape = (X_train_input.shape[1], X_train_input.shape[2])\n",
    "\n",
    "        elif model_name == 'raw_and_filtered':\n",
    "             X_train_input = {\n",
    "                 'raw_input': X_train_raw,\n",
    "                 'filtered_so_input': X_train_filtered_so,\n",
    "                 'filtered_spindles_input': X_train_filtered_spindles\n",
    "             }\n",
    "             X_test_input = {\n",
    "                 'raw_input': X_test_raw,\n",
    "                 'filtered_so_input': X_test_filtered_so,\n",
    "                 'filtered_spindles_input': X_test_filtered_spindles\n",
    "             }\n",
    "             y_train = label_coupling_epochs_strict(epochs_train_raw_downsampled, coupling_starts_train_raw_downsampled, coupling_ends_train_raw_downsampled)\n",
    "             y_test = label_coupling_epochs_strict(epochs_test_raw_downsampled, coupling_starts_test_raw_downsampled, coupling_ends_test_raw_downsampled)\n",
    "             input_shape = None\n",
    "            # when input shape = None, infers it itself\n",
    "\n",
    "        elif model_name == 'raw_and_filtered_and_stft':\n",
    "            X_train_input = {\n",
    "                'raw_input': X_train_raw,\n",
    "                'filtered_so_input': X_train_filtered_so,\n",
    "                'filtered_spindles_input': X_train_filtered_spindles,\n",
    "                'stft_input': X_train_stft_freq_norm \n",
    "            }\n",
    "            X_test_input = {\n",
    "                'raw_input': X_test_raw,\n",
    "                'filtered_so_input': X_test_filtered_so,\n",
    "                'filtered_spindles_input': X_test_filtered_spindles,\n",
    "                'stft_input': X_test_stft_freq_norm\n",
    "            }\n",
    "            # Labels for the three-input model come from the raw downsampled data\n",
    "            y_train = label_coupling_epochs_strict(epochs_train_raw_downsampled, coupling_starts_train_raw_downsampled, coupling_ends_train_raw_downsampled)\n",
    "            y_test = label_coupling_epochs_strict(epochs_test_raw_downsampled, coupling_starts_test_raw_downsampled, coupling_ends_test_raw_downsampled)\n",
    "\n",
    "            input_shape = None \n",
    "\n",
    "        print(f\"Training data shapes: { {k: v.shape for k, v in X_train_input.items()} if isinstance(X_train_input, dict) else X_train_input.shape}, labels={y_train.shape}\")\n",
    "        print(f\"Test data shapes: { {k: v.shape for k, v in X_test_input.items()} if isinstance(X_test_input, dict) else X_test_input.shape}, labels={y_test.shape}\")\n",
    "        # to check whether a dictionary or not \n",
    "        # because it is a dictionary for the three-input model\n",
    "        # but not for the other models\n",
    "\n",
    "\n",
    "        # build the models\n",
    "\n",
    "        print(\"Building and compiling model...\")\n",
    "        if model_name in ['raw']:\n",
    "             model = build_model_func(input_shape)\n",
    "        else:\n",
    "            model = build_model_func()\n",
    "\n",
    "\n",
    "        # define early stopping\n",
    "        # if validation loss does not change after 5 epochs\n",
    "        # stop training\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        # keep 20% of training set as validation\n",
    "        # this is useful to detect overfitting\n",
    "        history = model.fit(\n",
    "            X_train_input,\n",
    "            y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=20, # Adjust epochs as needed\n",
    "            batch_size=128, # Adjust batch size as needed\n",
    "            callbacks=[early_stop], # Optional: Use early stopping\n",
    "        )\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # this evaluates the model on test data of split (unseen data)\n",
    "        # these are the predictions\n",
    "        print(f\"Evaluating on {split_name}'s test data...\")\n",
    "        loss, accuracy = model.evaluate(X_test_input, y_test, verbose=0)\n",
    "        print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        # useful to compare to accuracy and loss of training and validation set\n",
    "        # to detect any overfitting\n",
    "\n",
    "        # now that have the predictions can calculate the F1 score\n",
    "        # and also accuracy and recall\n",
    "        y_pred_proba = model.predict(X_test_input, verbose=0)\n",
    "        y_pred_labels = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        split_f1 = f1_score(y_test, y_pred_labels)\n",
    "        split_precision = precision_score(y_test, y_pred_labels)\n",
    "        split_recall = recall_score(y_test, y_pred_labels)\n",
    "        print(f\"F1 Score for {model_name} on {split_name}: {split_f1:.4f}\")\n",
    "        print(f\"Precision for {model_name} on {split_name}: {split_precision:.4f}\")\n",
    "        print(f\"Recall for {model_name} on {split_name}: {split_recall:.4f}\")\n",
    "\n",
    "        # store all the metrics\n",
    "        # then move on to next step\n",
    "        model_metrics[model_name]['f1_scores'].append(split_f1)\n",
    "        model_metrics[model_name]['precision_scores'].append(split_precision)\n",
    "        model_metrics[model_name]['recall_scores'].append(split_recall)\n",
    "\n",
    "        # clears tensorflow \n",
    "        # this is to free up memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n--- Evaluation finished for all models across all splits ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c467797-0f32-4c23-889d-121f66f5fdf5",
   "metadata": {},
   "source": [
    "## Display average metrics and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467a81e-5d36-4808-a9a5-08c7694ae3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display F1, precision and recall in a data frame\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    average_f1 = np.mean(metrics['f1_scores'])\n",
    "    std_f1 = np.std(metrics['f1_scores'])\n",
    "\n",
    "    average_precision = np.mean(metrics['precision_scores'])\n",
    "    std_precision = np.std(metrics['precision_scores'])\n",
    "\n",
    "    average_recall = np.mean(metrics['recall_scores'])\n",
    "    std_recall = np.std(metrics['recall_scores'])\n",
    "\n",
    "    # the row is appended as a dict\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"F1 Score (mean ± std)\": f\"{average_f1:.4f} ± {std_f1:.4f}\",\n",
    "        \"Precision (mean ± std)\": f\"{average_precision:.4f} ± {std_precision:.4f}\",\n",
    "        \"Recall (mean ± std)\": f\"{average_recall:.4f} ± {std_recall:.4f}\",\n",
    "    })\n",
    "\n",
    "# use pandas to create the data frame\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# add a title and print the table\n",
    "print(\"\\n--- Average Metrics Across Splits For Coupling Detection ---\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# add statistics\n",
    "\n",
    "# definition\n",
    "def compare_models(f1_a, f1_b, model_a_name, model_b_name, alpha=0.05, n_comparisons=1):\n",
    "    # first alpha is adjusted for multiple comparisons\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"\\nBonferroni corrected alpha: {corrected_alpha:.4f} (original alpha={alpha} / {n_comparisons} comparisons)\")\n",
    "\n",
    "    # differences computed\n",
    "    diff = np.array(f1_b) - np.array(f1_a)\n",
    "\n",
    "    # plot distribution to assess normality\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(diff, kde=True, bins=8)\n",
    "    plt.title(f'Distribution of F1 Score Differences: {model_b_name} - {model_a_name}')\n",
    "    plt.xlabel('F1 Score Difference')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # shapiro wilk test to assess normality\n",
    "    # determines which type of t-test to do\n",
    "    w_stat, p_norm = shapiro(diff)\n",
    "    print(f\"Shapiro-Wilk normality test: W = {w_stat:.4f}, p = {p_norm:.4f}\")\n",
    "\n",
    "    # then perform t-test (paired t-test or wilcoxon signed-rank test)\n",
    "    print(f\"\\n--- Statistical Comparison: {model_b_name} vs {model_a_name} ---\")\n",
    "    if p_norm > corrected_alpha:\n",
    "        print(\"Paired t-test (normal distribution)\")\n",
    "        t_stat, p_val = ttest_rel(f1_b, f1_a)\n",
    "        print(f\"t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")\n",
    "    else:\n",
    "        print(\"Wilcoxon signed-rank test (non-normal distribution)\")\n",
    "        w_stat, p_val = wilcoxon(f1_b, f1_a)\n",
    "        print(f\"W-statistic = {w_stat:.4f}, p-value = {p_val:.4f}\")\n",
    "\n",
    "    # then compare with the Bonferroni corrected alpha\n",
    "    if p_val < corrected_alpha:\n",
    "        print(f\"Significant difference at corrected alpha = {corrected_alpha:.4f}\")\n",
    "    else:\n",
    "        print(f\"No significant difference at corrected alpha = {corrected_alpha:.4f}\")\n",
    "\n",
    "# then do the three model comparisons\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered']['f1_scores'],\n",
    "    model_a_name='raw',\n",
    "    model_b_name='raw_and_filtered',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered_and_stft']['f1_scores'],\n",
    "    model_a_name='raw',\n",
    "    model_b_name='raw_and_filtered',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n",
    "compare_models(\n",
    "    f1_a=model_metrics['raw_and_filtered']['f1_scores'],\n",
    "    f1_b=model_metrics['raw_and_filtered_and_stft']['f1_scores'],\n",
    "    model_a_name='raw_and_filtered',\n",
    "    model_b_name='raw_and_filtered_and_stft',\n",
    "    alpha=0.05,\n",
    "    n_comparisons=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b5d8b-55b1-453b-90d3-1275f9ef7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to get nicer visualisations\n",
    "\n",
    "# have a table only with F1 scores\n",
    "model_names = list(model_metrics.keys())\n",
    "average_f1s = [round(np.mean(model_metrics[name]['f1_scores']), 2) for name in model_names]\n",
    "std_f1s = [round(np.std(model_metrics[name]['f1_scores']), 2) for name in model_names]\n",
    "\n",
    "summary_data = {\n",
    "    'Model': model_names,\n",
    "    'Mean F1 Score': average_f1s,\n",
    "    'F1 Score Std Dev': std_f1s\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n--- Summary of F1 Scores Across 5 Folds for Coupling Detection ---\")\n",
    "display(summary_df)\n",
    "\n",
    "# create bar plot with F1 score for each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(summary_df['Model'], summary_df['Mean F1 Score'],\n",
    "               yerr=summary_df['F1 Score Std Dev'], capsize=5,\n",
    "               color=['lightcoral', 'lightgreen', 'skyblue'])\n",
    "\n",
    "# the F1 values are on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', va='bottom', ha='center', fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Mean F1 Score\")\n",
    "plt.title(\"Comparison of Model Performance on Coupling Detection for 5 participants\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
