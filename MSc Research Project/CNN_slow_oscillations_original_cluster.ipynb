{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc9b022-ca2c-4297-8b96-d04d25fa6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista\n",
    "import ipywidgets\n",
    "import ipyevents\n",
    "import pyvistaqt\n",
    "import yasa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import stft\n",
    "\n",
    "SEED = 82\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31678668-47f6-425c-9bca-e35c41a9083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8591655-1d13-489c-b824-7774a0efe70d",
   "metadata": {},
   "source": [
    "### One-input CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9704a0-12f6-4002-81d9-817fc6b69703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model_downsampled(input_shape=(300,1)):\n",
    "\n",
    "    # linear embedding layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Three convolutional blocks (like having three pattern detectors)\n",
    "\n",
    "    # First convolution block, kernel size of 5\n",
    "    padded1 = tf.keras.layers.ZeroPadding1D(padding=2)(input_layer)\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=10, kernel_size=5, strides=1, padding='valid')(padded1)\n",
    "    # each filter learns a different type of short-time feature\n",
    "    # stride of 1, moves one step at a time\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv1)\n",
    "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    # K = 2\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "\n",
    "    # Second convolution block, kernel size of 11\n",
    "    padded2 = tf.keras.layers.ZeroPadding1D(padding=5)(input_layer)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=10, kernel_size=11, strides=1, padding='valid')(padded2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv2)\n",
    "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "\n",
    "    # Third convolution block, kernel size of 21\n",
    "    padded3 = tf.keras.layers.ZeroPadding1D(padding=10)(input_layer)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=10, kernel_size=21, strides=1, padding='valid')(padded3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.01)(conv3)\n",
    "    conv3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "\n",
    "    # Concatenate the outputs of all blocks\n",
    "    concatenated = tf.keras.layers.Concatenate()([conv1, conv2, conv3])\n",
    "\n",
    "    # GRU Layer\n",
    "    gru = tf.keras.layers.GRU(64)(concatenated)\n",
    "\n",
    "    # Fully connected (dense) layer\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(gru)\n",
    "    # add a Dropout layer to prevent overfitting\n",
    "    #dense = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "    # Two softmax outputs for dual-task classification\n",
    "    #output_task1 = tf.keras.layers.Dense(2, activation='softmax', name='task1')(dense)\n",
    "    #output_task2 = tf.keras.layers.Dense(2, activation='softmax', name='task2')(dense)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Create the model\n",
    "    #model = tf.keras.models.Model(inputs=input_layer, outputs=[output_task1, output_task2])\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    #model.compile(optimizer='adam', loss={'task1': 'categorical_crossentropy', 'task2': 'categorical_crossentropy'}, metrics={'task1': 'accuracy', 'task2': 'accuracy'})\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5d4e-6e10-4eea-beb0-4da5c6489791",
   "metadata": {},
   "source": [
    "### Three-input CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97559de-b1e0-428e-ade7-c208f9046b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_time():\n",
    "    \n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered = tf.keras.Input(shape=(300, 1), name='filtered_input')\n",
    "    input_stft = tf.keras.Input(shape=(26, 1), name='stft_input')  \n",
    "    # 300 time points of raw and filtered EEG\n",
    "    # 26 time points from the STFT transform\n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            # applies zero-padding so that the length after convolution stays the same\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            # performs the convolution\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            # applies LeakyReLU \n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            # reduce temporal dimension by 2\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            # normalize the output\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches: each input has its own CNN branch\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered = conv_branch(input_filtered)\n",
    "    branch_stft = conv_branch(input_stft)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    # this captures temporal dependencies\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered = tf.keras.layers.GRU(64)(branch_filtered)\n",
    "    gru_stft = tf.keras.layers.GRU(64)(branch_stft)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    # this leads to one 192-dimensional vector\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered, gru_stft])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered, input_stft], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8944cde-f04b-4c9f-878b-b981bc09e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_input_cnn_model_freq():\n",
    "    # Inputs\n",
    "    input_raw = tf.keras.Input(shape=(300, 1), name='raw_input')\n",
    "    input_filtered = tf.keras.Input(shape=(300, 1), name='filtered_input')\n",
    "    input_stft = tf.keras.Input(shape=(13, 1), name='stft_input')  \n",
    "\n",
    "    def conv_branch(input_layer, kernel_sizes=[5, 11, 21]):\n",
    "        outputs = []\n",
    "        for k in kernel_sizes:\n",
    "            pad = k // 2\n",
    "            x = tf.keras.layers.ZeroPadding1D(padding=pad)(input_layer)\n",
    "            x = tf.keras.layers.Conv1D(filters=10, kernel_size=k, strides=1, padding='valid')(x)\n",
    "            x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n",
    "            x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            outputs.append(x)\n",
    "        return tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "    # Convolutional branches\n",
    "    branch_raw = conv_branch(input_raw)\n",
    "    branch_filtered = conv_branch(input_filtered)\n",
    "    branch_stft = conv_branch(input_stft)\n",
    "\n",
    "    # Each branch through its own GRU\n",
    "    gru_raw = tf.keras.layers.GRU(64)(branch_raw)\n",
    "    gru_filtered = tf.keras.layers.GRU(64)(branch_filtered)\n",
    "    gru_stft = tf.keras.layers.GRU(64)(branch_stft)\n",
    "\n",
    "    # Concatenate GRU outputs (fixed-length vectors)\n",
    "    merged = tf.keras.layers.Concatenate()([gru_raw, gru_filtered, gru_stft])\n",
    "\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=[input_raw, input_filtered, input_stft], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cebca-db67-415d-9405-f44414237b54",
   "metadata": {},
   "source": [
    "### Slow oscillation detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d83e988-0a6d-4f1a-a621-fb9a6c2bdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_slow_oscillations_times(combined_raw, do_filter=True, do_downsample=False, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "    #signs = np.sign(current_data)\n",
    "    #pos_to_neg = np.where((signs[:-1] > 0) & (signs[1:] < 0))[0]\n",
    "    # detect +1 to -1\n",
    "    #neg_to_pos = np.where((signs[:-1] <  0) & (signs[1:] > 0))[0]\n",
    "    # detect -1 to +1\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices = []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings)-1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "\n",
    "        # find the negative to positive crossing in between\n",
    "        #mid_crossings = neg_to_pos[(neg_to_pos > start_idx) & (neg_to_pos < end_idx)]\n",
    "\n",
    "        #if len(mid_crossings) != 1:\n",
    "            #continue\n",
    "\n",
    "        #mid_idx = mid_crossings [0]\n",
    "\n",
    "        #duration = (end_idx - start_idx) / sfreq\n",
    "        #if not (0.8 <= duration <= 2.0):\n",
    "  \n",
    "        \n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    # keep lowest negative peaks (under the 25th percentile)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "    # keep largest peak-to-peak amplitude (over 75th percentile)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation\n",
    "\n",
    "def detect_slow_oscillations_peaks(combined_raw, do_filter=True, do_downsample=True, downsample_rate=100):\n",
    "\n",
    "    # according to methods from Klinzing et al.(2016)\n",
    "\n",
    "    data = combined_raw.copy().pick_channels(['Fz'])\n",
    "\n",
    "    if do_filter:\n",
    "        data.filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "    if do_downsample:\n",
    "        data.resample(downsample_rate)\n",
    "        \n",
    "    sfreq = data.info['sfreq']\n",
    "    channel_data = data.get_data()[0]\n",
    "    \n",
    "    # 3. find all positive-to-negative zero-crossings\n",
    "    \n",
    "    # zero_crossings = np.where( S!= 0)[0]\n",
    "    # can also save this somewhere for further detection of spindles\n",
    "    \n",
    "    S = np.diff(np.sign(channel_data))\n",
    "    # np.sign returns an array with 1 (positive), 0 (zero), -1 (negative)\n",
    "    # np.diff calculates the difference between consecutive elements in an array\n",
    "    # positive value: transition from negative to positive\n",
    "    # negative value: transition from positive to negative\n",
    "    # when it's a zero, means that value stayed the same\n",
    "    zero_crossings = np.where(S < 0)[0]\n",
    "    # -2 is when a positive-to-negative zero-crossing occurs\n",
    "    # goes from 1 to -1 \n",
    "    # -1 - 1 = -2\n",
    "    # [0] extracts the actual array\n",
    "    # extracts the indices of interest from current_data (not S)\n",
    "\n",
    "\n",
    "    # 4. Detect peak potentials in each pair\n",
    "    slow_oscillations = []\n",
    "    slow_oscillations_peaks = []\n",
    "    negative_peaks = []\n",
    "    positive_peaks = []\n",
    "    peak_to_peak_amplitudes = []\n",
    "    candidate_indices =  []\n",
    "\n",
    "    # for loop for each pair\n",
    "    # to collect all the negative and positive peaks\n",
    "    # to further apply criteria\n",
    "    count = 0\n",
    "    for i in range(0, len(zero_crossings) - 1, 1):\n",
    "        # loop through all the zero_crossings\n",
    "        # step of 1 (with step of 2, miss some zero_crossings)\n",
    "        start_idx = zero_crossings[i] + 1\n",
    "        # assigns index of zero-crossing (representing start of potential SO)\n",
    "        # to start_idx\n",
    "        end_idx = zero_crossings[i + 1] + 1\n",
    "        # assigns index of next zero-crossing (representing end of potential SO)\n",
    "        # to end_idx\n",
    "        segment_length = (end_idx - start_idx) / sfreq\n",
    "\n",
    "        # need to add +1 because of way extract segment later\n",
    "\n",
    "        # have identified index for the pair\n",
    "        \n",
    "        # extract data segment between crossings\n",
    "        \n",
    "        # find peaks\n",
    "        if 0.8 <= segment_length <= 2.0:\n",
    "            count += 1\n",
    "            segment = channel_data[start_idx:end_idx]\n",
    "            positive_peak = np.max(segment)\n",
    "            negative_peak = np.min(segment)\n",
    "            peak_to_peak_amplitude = positive_peak - negative_peak\n",
    "\n",
    "        # store values\n",
    "            candidate_indices.append((start_idx, end_idx))\n",
    "            positive_peaks.append(positive_peak)\n",
    "            negative_peaks.append(negative_peak)\n",
    "            peak_to_peak_amplitudes.append(peak_to_peak_amplitude)\n",
    "\n",
    "    # calculate mean values for comparison\n",
    "    #mean_negative_peak = np.mean(negative_peaks)\n",
    "    # mean_negative_peak = np.mean(negative_peaks) if negative_peaks else 0\n",
    "    #mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes)\n",
    "    # mean_peak_to_peak_amplitude = np.mean(peak_to_peak_amplitudes) if peak_to_peak_amplitudes else 0\n",
    "\n",
    "    negative_peak_threshold = np.percentile(negative_peaks, 25)\n",
    "    peak_to_peak_amplitude_threshold = np.percentile(peak_to_peak_amplitudes, 75)\n",
    "\n",
    "    for (start_idx, end_idx), negative_peak, peak_to_peak_amplitude in zip(candidate_indices, negative_peaks, peak_to_peak_amplitudes):\n",
    "        if peak_to_peak_amplitude >= peak_to_peak_amplitude_threshold and negative_peak <= negative_peak_threshold:\n",
    "            slow_oscillations.append((start_idx / sfreq, end_idx / sfreq))\n",
    "            slow_oscillations_peaks.append((negative_peak, positive_peak))\n",
    "\n",
    "            \n",
    "    return slow_oscillations\n",
    "    # returns a list of tuples, in which each tuple represents the start and end times of\n",
    "    # a detected slow oscillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483deb0-0013-4c25-abb4-b1d2ec0ab2a4",
   "metadata": {},
   "source": [
    "### Epochs function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec51639e-3579-47c8-9682-d1b58f13cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_length_epochs(raw, duration=3.0, overlap=0.0, preload=True, reject_by_annotation=False):\n",
    "    #raw_zero = raw.copy().crop(tmin=raw.times[0], tmax=raw.times[-1], include_tmax=True)\n",
    "    # because otherwise does not start at 0\n",
    "    return mne.make_fixed_length_epochs(\n",
    "        #raw_zero,\n",
    "        raw,\n",
    "        duration=duration,\n",
    "        overlap=overlap,\n",
    "        preload=preload,\n",
    "        reject_by_annotation=reject_by_annotation\n",
    "    )\n",
    "# function mne.make_fixed_length_epochs takes into account the sampling frequency of the data\n",
    "\n",
    "# because can change the overlap in the previous function\n",
    "# can also change this function\n",
    "\n",
    "\n",
    "def label_so_epochs(epochs, so_starts, so_ends, epoch_length_sec=3.0):\n",
    "\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    # new np array with the start time of each epoch\n",
    "    # epoch_starts[i] is the start time of each epoch\n",
    "\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "    # initialize all the labels as 0 initially\n",
    "\n",
    "    for start, end in zip(so_starts, so_ends):\n",
    "        # loop through the start and end times of detected spindles by YASA\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            # loop through the one-second epochs that are not labelled yet\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "            # for each epoch, calculate the epoch end time\n",
    "            # which is epoch_start + length of epoch\n",
    "            # so now have the time range of each epoch\n",
    "            if (start < epoch_end) and (end > epoch_start):\n",
    "                # if the spindle started before the epoch ends\n",
    "                # and the spindle ended after the epoch started\n",
    "                epoch_labels[i] = 1\n",
    "                \n",
    "    return epoch_labels\n",
    "\n",
    "def label_so_epochs_strict(epochs, so_starts, so_ends, epoch_length_sec=3.0):\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "    for so_start, so_end in zip(so_starts, so_ends):\n",
    "        so_duration = so_end - so_start\n",
    "        required_overlap = 0.8 * so_duration  \n",
    "        # only label 1 if epoch contains 80% of the SO duration\n",
    "\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "\n",
    "            # Calculate overlap between SO and epoch\n",
    "            overlap_start = max(so_start, epoch_start)\n",
    "            overlap_end = min(so_end, epoch_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration >= required_overlap:\n",
    "                epoch_labels[i] = 1\n",
    "\n",
    "    return epoch_labels\n",
    "\n",
    "def label_so_epochs_moderate(epochs, so_starts, so_ends, epoch_length_sec=3.0):\n",
    "    epoch_starts = np.arange(len(epochs)) * epoch_length_sec\n",
    "    epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "\n",
    "    for so_start, so_end in zip(so_starts, so_ends):\n",
    "        so_duration = so_end - so_start\n",
    "        required_overlap = 0.5 * so_duration  \n",
    "        # only label 1 if epoch contains 80% of the SO duration\n",
    "\n",
    "        for i, epoch_start in enumerate(epoch_starts):\n",
    "            epoch_end = epoch_start + epoch_length_sec\n",
    "\n",
    "            # Calculate overlap between SO and epoch\n",
    "            overlap_start = max(so_start, epoch_start)\n",
    "            overlap_end = min(so_end, epoch_end)\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "\n",
    "            if overlap_duration >= required_overlap:\n",
    "                epoch_labels[i] = 1\n",
    "\n",
    "    return epoch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b804a7-9705-475d-80bd-17f6b019cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_so_epochs(epochs, so_starts, so_ends):\n",
    "#     sfreq = epochs.info['sfreq']\n",
    "#     epoch_starts = epochs.events[:, 0] / sfreq\n",
    "#     # gives the sample index at which each epoch starts\n",
    "#     # divides by sfreq converts these to start times in seconds\n",
    "#     epoch_length_sec = epochs.get_data().shape[2] / sfreq\n",
    "#     # gives the duration of each epoch\n",
    "\n",
    "#     epoch_labels = np.zeros(len(epochs), dtype=int)\n",
    "#     # initialize all the labels as 0 initially\n",
    "\n",
    "#     for start, end in zip(so_starts, so_ends):\n",
    "#         # loop through the start and end times of detected spindles \n",
    "#         for i, epoch_start in enumerate(epoch_starts):\n",
    "#             # loop through the one-second epochs that are not labelled yet\n",
    "#             epoch_end = epoch_start + epoch_length_sec\n",
    "#             # for each epoch, calculate the epoch end time\n",
    "#             # which is epoch_start + length of epoch\n",
    "#             # so now have the time range of each epoch\n",
    "#             if (start < epoch_end) and (end > epoch_start):\n",
    "#                 # if the spindle started before the epoch ends\n",
    "#                 # and the spindle ended after the epoch started\n",
    "#                 epoch_labels[i] = 1\n",
    "                \n",
    "#     return epoch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cf7bd-1037-4880-8ef2-4230ef26b678",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36892fcf-14ae-430f-b254-094f546cff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file C:\\EEG DATA\\combined_sets\\train_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 1470000 ... 23295072 =   2940.000 ... 46590.144 secs\n",
      "Ready.\n",
      "Reading 0 ... 21825072  =      0.000 ... 43650.144 secs...\n",
      "Opening raw data file C:\\EEG DATA\\combined_sets\\test_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 825000 ... 17985049 =   1650.000 ... 35970.098 secs\n",
      "Ready.\n",
      "Reading 0 ... 17160049  =      0.000 ... 34320.098 secs...\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "train_file = r\"C:\\EEG DATA\\combined_sets\\train_raw.fif\"\n",
    "test_file = r\"C:\\EEG DATA\\combined_sets\\test_raw.fif\"\n",
    "\n",
    "# load raw files\n",
    "train_raw = mne.io.read_raw_fif(train_file, preload=True)\n",
    "test_raw = mne.io.read_raw_fif(test_file, preload=True)\n",
    "\n",
    "#train_raw.crop(tmin=train_raw.times[0], include_tmax=True)\n",
    "#test_raw.crop(tmin=test_raw.times[0], include_tmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b307f-84a2-446c-b938-611c31729379",
   "metadata": {},
   "source": [
    "## With raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3cf24-fa5d-4fdb-93f8-0d2305df266a",
   "metadata": {},
   "source": [
    "### Slow oscillation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee961db-2f38-4caa-9900-202fcc3daf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 73 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 50 contiguous segments\n",
      "Setting up band-pass filter from 0.16 - 1.2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.16\n",
      "- Lower transition bandwidth: 0.16 Hz (-6 dB cutoff frequency: 0.08 Hz)\n",
      "- Upper passband edge: 1.25 Hz\n",
      "- Upper transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.25 Hz)\n",
      "- Filter length: 10313 samples (20.626 s)\n",
      "\n",
      "3610\n",
      "3610\n",
      "2150\n",
      "2150\n"
     ]
    }
   ],
   "source": [
    "so_train_times_raw_downsampled = detect_slow_oscillations_times(train_raw, do_filter=True, do_downsample=True)\n",
    "so_test_times_raw_downsampled = detect_slow_oscillations_times(test_raw, do_filter=True, do_downsample=True)\n",
    "\n",
    "so_starts_train_raw_downsampled, so_ends_train_raw_downsampled = zip(*so_train_times_raw_downsampled) if so_train_times_raw_downsampled else([],[])\n",
    "so_starts_test_raw_downsampled, so_ends_test_raw_downsampled = zip(*so_test_times_raw_downsampled) if so_test_times_raw_downsampled else([],[])\n",
    "\n",
    "print(len(so_starts_train_raw_downsampled))\n",
    "print(len(so_ends_train_raw_downsampled))\n",
    "\n",
    "print(len(so_starts_test_raw_downsampled))\n",
    "print(len(so_ends_test_raw_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef4b98-fb86-4dde-9967-6ef90e913258",
   "metadata": {},
   "source": [
    "### Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17bb813-289a-452a-bd9e-6a281da10d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "train_raw_downsampled = train_raw.copy().resample(100)\n",
    "test_raw_downsampled = test_raw.copy().resample(100)\n",
    "\n",
    "print(train_raw_downsampled.info['sfreq'])\n",
    "print(test_raw_downsampled.info['sfreq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff745f7c-59d7-4091-9be4-011840e63287",
   "metadata": {},
   "source": [
    "### Epoch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78bfc1fe-e7c1-41ae-9e1c-95beb2ef5936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "14550 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 14550 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "11440 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 11440 events and 300 original time points ...\n",
      "0 bad epochs dropped\n",
      "(14550, 1, 300)\n",
      "(11440, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "epochs_train_raw_downsampled = create_fixed_length_epochs(train_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "epochs_test_raw_downsampled = create_fixed_length_epochs(test_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "print(epochs_train_raw_downsampled.get_data().shape)\n",
    "print(epochs_test_raw_downsampled.get_data().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769a0784-f513-4108-9d5a-6994da082baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data first 10 SO: (np.float64(7.82), np.float64(83.41), np.float64(108.08), np.float64(262.01), np.float64(274.78), np.float64(308.33), np.float64(330.51), np.float64(378.52), np.float64(394.11), np.float64(478.54))\n",
      "Train labels first 10 epochs: [0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Test data first 10 SO: (np.float64(65.96), np.float64(120.42), np.float64(161.02), np.float64(224.46), np.float64(257.48), np.float64(353.86), np.float64(404.85), np.float64(406.44), np.float64(440.22), np.float64(459.74))\n",
      "Test labels first 10 epochs: [0 0 0 0 0 0 0 0 0 0]\n",
      "CPU times: total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train set\n",
    "\n",
    "epoch_labels_train_raw_downsampled = label_so_epochs(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "\n",
    "print(f\"Train data first 10 SO: {so_starts_train_raw_downsampled[:10]}\")\n",
    "print(f\"Train labels first 10 epochs: {epoch_labels_train_raw_downsampled[:10]}\")\n",
    "\n",
    "# Test set\n",
    "\n",
    "epoch_labels_test_raw_downsampled = label_so_epochs(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "\n",
    "print(f\"\\nTest data first 10 SO: {so_starts_test_raw_downsampled[:10]}\")\n",
    "print(f\"Test labels first 10 epochs: {epoch_labels_test_raw_downsampled[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887f364-bb2e-4aef-91fa-e1dc90c8cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data first 10 SO: (np.float64(7.82), np.float64(83.41), np.float64(108.08), np.float64(262.01), np.float64(274.78), np.float64(308.33), np.float64(330.51), np.float64(378.52), np.float64(394.11), np.float64(478.54))\n",
      "Train labels first 10 epochs: [0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train set\n",
    "\n",
    "epoch_labels_train_raw_downsampled_moderate = label_so_epochs_moderate(epochs_train_raw_downsampled, so_starts_train_raw_downsampled, so_ends_train_raw_downsampled)\n",
    "\n",
    "print(f\"Train data first 10 SO: {so_starts_train_raw_downsampled[:10]}\")\n",
    "print(f\"Train labels first 10 epochs: {epoch_labels_train_raw_downsampled_moderate[:10]}\")\n",
    "\n",
    "# Test set\n",
    "\n",
    "epoch_labels_test_raw_downsampled_moderate = label_so_epochs_moderate(epochs_test_raw_downsampled, so_starts_test_raw_downsampled, so_ends_test_raw_downsampled)\n",
    "\n",
    "print(f\"\\nTest data first 10 SO: {so_starts_test_raw_downsampled[:10]}\")\n",
    "print(f\"Test labels first 10 epochs: {epoch_labels_test_raw_downsampled_moderate[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29dffa-d78d-407a-841d-b018f8faede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train set\n",
    "train_labels = epoch_labels_train_raw_downsampled\n",
    "train_counts = np.bincount(train_labels)\n",
    "train_total = len(train_labels)\n",
    "\n",
    "print(\"\\nTrain label distribution:\")\n",
    "for label, count in enumerate(train_counts):\n",
    "    proportion = count / train_total\n",
    "    print(f\"Label {label}: count = {count}, proportion = {proportion:.2%}\")\n",
    "\n",
    "# Test set\n",
    "test_labels = epoch_labels_test_raw_downsampled\n",
    "test_counts = np.bincount(test_labels)\n",
    "test_total = len(test_labels)\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "for label, count in enumerate(test_counts):\n",
    "    proportion = count / test_total\n",
    "    print(f\"Label {label}: count = {count}, proportion = {proportion:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7344dbc-850f-4d74-87bf-74ed12d3b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train set\n",
    "train_labels = epoch_labels_train_raw_downsampled_moderate\n",
    "train_counts = np.bincount(train_labels)\n",
    "train_total = len(train_labels)\n",
    "\n",
    "print(\"\\nTrain label distribution:\")\n",
    "for label, count in enumerate(train_counts):\n",
    "    proportion = count / train_total\n",
    "    print(f\"Label {label}: count = {count}, proportion = {proportion:.2%}\")\n",
    "\n",
    "# Test set\n",
    "test_labels = epoch_labels_test_raw_downsampled_moderate\n",
    "test_counts = np.bincount(test_labels)\n",
    "test_total = len(test_labels)\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "for label, count in enumerate(test_counts):\n",
    "    proportion = count / test_total\n",
    "    print(f\"Label {label}: count = {count}, proportion = {proportion:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37287d51-199b-446b-bbf0-cd6c8b78b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(epoch_labels_train_raw_downsampled))\n",
    "print(len(epoch_labels_test_raw_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532d567-9081-4468-ab9c-156686a91b2d",
   "metadata": {},
   "source": [
    "### Prepare EEG data for CNN Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78b27c-8d38-4760-a71a-8863aab50d6d",
   "metadata": {},
   "source": [
    "#### X and y train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb866d-b42e-450f-ae56-eb3678f7b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape arrays\n",
    "\n",
    "epochs_train_np_raw_downsampled = np.array(epochs_train_raw_downsampled).reshape(len(epochs_train_raw_downsampled), -1, 1)\n",
    "# number of epochs N, sampling frequency (time dimension automatically inferred), channel dimension\n",
    "epochs_test_np_raw_downsampled = np.array(epochs_test_raw_downsampled).reshape(len(epochs_test_raw_downsampled), -1, 1)\n",
    "                                                                 \n",
    "# Define X and y sets\n",
    "                                                                 \n",
    "X_train_raw_downsampled = epochs_train_np_raw_downsampled\n",
    "y_train_raw_downsampled = epoch_labels_train_raw_downsampled_moderate\n",
    "\n",
    "X_test_raw_downsampled = epochs_test_np_raw_downsampled\n",
    "y_test_raw_downsampled = epoch_labels_test_raw_downsampled_moderate\n",
    "\n",
    "# Print shapes\n",
    "\n",
    "print(f\"X_train shape: {X_train_raw_downsampled.shape}\")\n",
    "print(f\"y_train shape: {y_train_raw_downsampled.shape}\")\n",
    "\n",
    "print(f\"\\nX_test shape: {X_test_raw_downsampled.shape}\")\n",
    "print(f\"y_test shape: {y_test_raw_downsampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f682a9-5692-4257-882e-e770ef510e3b",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4cd57-b932-4271-b481-87d671765064",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      \n",
    "    patience=5,               \n",
    "    restore_best_weights=True \n",
    ")\n",
    "# stop after 5 epochs with no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a74e73-926b-4916-ab56-59a81838077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model architecture\n",
    "input_shape = (300, 1)\n",
    "model_raw_downsampled_so = build_cnn_model_downsampled(input_shape)\n",
    "model_raw_downsampled_so.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d0acf-1f34-48fb-ba0a-4f91bd758381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_info_raw_downsampled_so = model_raw_downsampled_so.fit(X_train_raw_downsampled, y_train_raw_downsampled, validation_split=0.2, epochs=30, batch_size=128, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078b5bc-ee2e-4c3c-a28f-f3d6d77e481f",
   "metadata": {},
   "source": [
    "#### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8605561-28c2-46f1-b6e3-e8537f923c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(training_info):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    axs[0].plot(training_info.history['loss'], label=\"training set\")\n",
    "    axs[0].plot(training_info.history['val_loss'], label=\"validation set\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "    try:\n",
    "        axs[1].plot(training_info.history['accuracy'], label=\"training set\")\n",
    "        axs[1].plot(training_info.history['val_accuracy'], label=\"validation set\")\n",
    "        axs[1].set_xlabel(\"Epoch\")\n",
    "        axs[1].set_ylabel(\"Accuracy\")\n",
    "        axs[1].grid(True)\n",
    "        axs[1].legend()\n",
    "    except:\n",
    "        pass\n",
    "  \n",
    "    fig.suptitle(\"Training history for one-input CNN model with raw EEG data for SO detection\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_info_raw_downsampled_so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a63eb0-d68f-4d80-9f9a-cd373f01c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw_downsampled_so.evaluate(X_test_raw_downsampled, y_test_raw_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5e25e-7859-4be4-9170-66fd00fd19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_raw_downsampled_so.predict(X_test_raw_downsampled)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test_raw_downsampled, y_pred_labels))\n",
    "print(classification_report(y_test_raw_downsampled, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440bead-9f6e-4c1c-8fbd-fe43501db1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = model_raw_downsampled_so.predict(X_test_raw_downsampled)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_raw_downsampled, y_pred_labels)\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Classification report as a dataframe\n",
    "report = classification_report(y_test_raw_downsampled, y_pred_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Confusion matrix plotted as heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for SO Detection \\nusing a one-input CNN with raw EEG data\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report as a table\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=report_df.round(2).values,\n",
    "                 colLabels=report_df.columns,\n",
    "                 rowLabels=report_df.index,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.title(\"Classification Report for SO Detection \\nusing a one-input CNN with raw EEG data\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e013d-1f6f-435c-b825-7550110b60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten in case y_pred has shape (n_samples, 1)\n",
    "y_pred_proba = y_pred.ravel()\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_raw_downsampled, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic for SO Detection \\nusing a one-input CNN with raw EEG data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee697a-43c0-45e7-9789-72faeb4d685a",
   "metadata": {},
   "source": [
    "## With filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8473c-6f84-4407-a891-368862254086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bandpass filter between 0.16 and 1.25 Hz\n",
    "# to compare performance of filtered dataset to unfiltered one\n",
    "train_filtered_downsampled = train_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "test_filtered_downsampled = test_raw.copy().filter(l_freq=0.16, h_freq=1.25)\n",
    "\n",
    "# Downsample to 100 Hz\n",
    "train_filtered_downsampled.resample(100)\n",
    "test_filtered_downsampled.resample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a5cf0-4515-499c-bfa3-fb61eca1efcd",
   "metadata": {},
   "source": [
    "### Slow oscillation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18b3e3-f856-4991-895c-f0c4b1efcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_train_times_filtered_downsampled = detect_slow_oscillations_times(train_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "so_test_times_filtered_downsampled = detect_slow_oscillations_times(test_filtered_downsampled, do_filter=False, do_downsample=False)\n",
    "# since filtering and downsampling before, do not filter and downsample again in function\n",
    "\n",
    "so_starts_train_filtered_downsampled, so_ends_train_filtered_downsampled = zip(*so_train_times_filtered_downsampled) if so_train_times_filtered_downsampled else([],[])\n",
    "so_starts_test_filtered_downsampled, so_ends_test_filtered_downsampled = zip(*so_test_times_filtered_downsampled) if so_test_times_filtered_downsampled else([],[])\n",
    "\n",
    "print(len(so_starts_train_filtered_downsampled))\n",
    "print(len(so_ends_train_filtered_downsampled))\n",
    "\n",
    "print(len(so_starts_test_filtered_downsampled))\n",
    "print(len(so_ends_test_filtered_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190b898-ecc1-44d4-80c5-f0b39611e16d",
   "metadata": {},
   "source": [
    "### Epoch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71e612-2b7a-4e30-a4a7-8929c927ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_filtered_downsampled = create_fixed_length_epochs(train_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "epochs_test_filtered_downsampled = create_fixed_length_epochs(test_filtered_downsampled, duration=3.0, overlap=0.0)\n",
    "print(epochs_train_filtered_downsampled.get_data().shape)\n",
    "print(epochs_test_filtered_downsampled.get_data().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2605182-2805-4628-b245-59b784234d3a",
   "metadata": {},
   "source": [
    "### Labels for 3-second epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401d067-a679-495b-a7b3-2522dc12e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train set\n",
    "\n",
    "epoch_labels_train_filtered_downsampled_moderate = label_so_epochs_moderate(epochs_train_filtered_downsampled, so_starts_train_filtered_downsampled, so_ends_train_filtered_downsampled)\n",
    "\n",
    "print(f\"Train data first 10 slow oscillations: {so_starts_train_filtered_downsampled[:10]}\")\n",
    "print(f\"Train labels first 10 epochs: {epoch_labels_train_filtered_downsampled_moderate[:10]}\")\n",
    "\n",
    "# Test set\n",
    "\n",
    "epoch_labels_test_filtered_downsampled_moderate = label_so_epochs_moderate(epochs_test_filtered_downsampled, so_starts_test_filtered_downsampled, so_ends_test_filtered_downsampled)\n",
    "\n",
    "print(f\"\\nTest data first 10 slow oscillations: {so_starts_test_filtered_downsampled[:10]}\")\n",
    "print(f\"Test labels first 10 epochs: {epoch_labels_test_filtered_downsampled_moderate[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1672dff-67db-4f06-ae9c-5fdd2d7db8c0",
   "metadata": {},
   "source": [
    "### Prepare EEG data for CNN input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc57423-b68a-40cb-8b1e-9e4acfa8f463",
   "metadata": {},
   "source": [
    "##### X and y train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2590082-e8c7-4225-8ffb-869164337d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape arrays\n",
    "\n",
    "epochs_train_np_filtered_downsampled = np.array(epochs_train_filtered_downsampled).reshape(len(epochs_train_filtered_downsampled), -1, 1)\n",
    "# number of epochs N, sampling frequency (time dimension automatically inferred), channel dimension\n",
    "epochs_test_np_filtered_downsampled = np.array(epochs_test_filtered_downsampled).reshape(len(epochs_test_filtered_downsampled), -1, 1)\n",
    "                                                                 \n",
    "# Define X and y sets\n",
    "                                                                 \n",
    "X_train_filtered_downsampled = epochs_train_np_filtered_downsampled\n",
    "y_train_filtered_downsampled = epoch_labels_train_filtered_downsampled_moderate\n",
    "\n",
    "X_test_filtered_downsampled = epochs_test_np_filtered_downsampled\n",
    "y_test_filtered_downsampled = epoch_labels_test_filtered_downsampled_moderate\n",
    "\n",
    "# Print shapes\n",
    "\n",
    "print(f\"X_train shape: {X_train_filtered_downsampled.shape}\")\n",
    "print(f\"y_train shape: {y_train_filtered_downsampled.shape}\")\n",
    "\n",
    "print(f\"\\nX_test shape: {X_test_filtered_downsampled.shape}\")\n",
    "print(f\"y_test shape: {y_test_filtered_downsampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df816a21-9552-48d6-8f49-ec9a83e520bf",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b41732-5333-45ca-9147-403432bcc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      \n",
    "    patience=5,               \n",
    "    restore_best_weights=True \n",
    ")\n",
    "# stop after 5 epochs with no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30a3ae-2fe6-4e10-814c-9b1a3e70d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model architecture\n",
    "input_shape = (300, 1)\n",
    "model_filtered_downsampled = build_cnn_model_downsampled(input_shape)\n",
    "model_filtered_downsampled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44877650-9158-4b7f-8b5f-198c68f148c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# change it to training_info_filtered_downsampled_so next time I run the code\n",
    "\n",
    "training_info_filtered_downsampled = model_filtered_downsampled.fit(X_train_filtered_downsampled, y_train_filtered_downsampled, validation_split=0.2, epochs=30, batch_size=128, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416e2b1-0d92-432a-83a3-f584513c55b9",
   "metadata": {},
   "source": [
    "#### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929c98a-b2c0-4e7f-b625-4c738672b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(training_info):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    axs[0].plot(training_info.history['loss'], label=\"training set\")\n",
    "    axs[0].plot(training_info.history['val_loss'], label=\"validation set\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "    try:\n",
    "        axs[1].plot(training_info.history['accuracy'], label=\"training set\")\n",
    "        axs[1].plot(training_info.history['val_accuracy'], label=\"validation set\")\n",
    "        axs[1].set_xlabel(\"Epoch\")\n",
    "        axs[1].set_ylabel(\"Accuracy\")\n",
    "        axs[1].grid(True)\n",
    "        axs[1].legend()\n",
    "    except:\n",
    "        pass\n",
    "  \n",
    "    fig.suptitle(\"Training history for one-input CNN model with filtered EEG data for SO detection\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_info_filtered_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b77833-d435-40a9-b4db-8a81ce0423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filtered_downsampled.evaluate(X_test_filtered_downsampled, y_test_filtered_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3474e85-7125-4571-aa12-98fcee23f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_filtered_downsampled.predict(X_test_raw_downsampled)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test_raw_downsampled, y_pred_labels))\n",
    "print(classification_report(y_test_raw_downsampled, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff41761-4d76-432c-9008-17f71ab20e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = model_filtered_downsampled.predict(X_test_raw_downsampled)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_filtered_downsampled, y_pred_labels)\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Classification report as a dataframe\n",
    "report = classification_report(y_test_filtered_downsampled, y_pred_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Confusion matrix plotted as heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for SO Detection \\nusing a one-input CNN with filtered EEG data\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report as a table\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=report_df.round(2).values,\n",
    "                 colLabels=report_df.columns,\n",
    "                 rowLabels=report_df.index,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.title(\"Classification Report for SO Detection \\nusing a one-input CNN with filtered EEG data\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee83080-1c34-4c32-9e47-44a8c87b1ba9",
   "metadata": {},
   "source": [
    "## With STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da838a-3f06-4118-9f99-b104f0f0ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_train_times_stft_downsampled = detect_slow_oscillations_times(train_raw, do_filter=True, do_downsample=True)\n",
    "so_test_times_stft_downsampled = detect_slow_oscillations_times(test_raw, do_filter=True, do_downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e04fbe-209c-4f76-be11-f301bda34753",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_starts_train_stft_downsampled, so_ends_train_stft_downsampled = zip(*so_train_times_stft_downsampled) if so_train_times_stft_downsampled else([],[])\n",
    "so_starts_test_stft_downsampled, so_ends_test_stft_downsampled = zip(*so_test_times_stft_downsampled) if so_test_times_stft_downsampled else([],[])\n",
    "\n",
    "print(len(so_starts_train_stft_downsampled))\n",
    "print(len(so_ends_train_stft_downsampled))\n",
    "\n",
    "print(len(so_starts_test_stft_downsampled))\n",
    "print(len(so_ends_test_stft_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c1079-af62-4394-b5b6-e3297336ff70",
   "metadata": {},
   "source": [
    "### Epoch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159d709-e9ad-4175-8a75-effd26ff70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_stft_downsampled = create_fixed_length_epochs(train_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "epochs_test_stft_downsampled = create_fixed_length_epochs(test_raw_downsampled, duration=3.0, overlap=0.0)\n",
    "\n",
    "print(epochs_train_stft_downsampled.get_data().shape)\n",
    "print(epochs_test_stft_downsampled.get_data().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53468828-02fb-4456-9149-18af74069b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train set\n",
    "\n",
    "epoch_labels_train_stft_downsampled_moderate = label_so_epochs_moderate(epochs_train_stft_downsampled, so_starts_train_stft_downsampled, so_ends_train_stft_downsampled)\n",
    "\n",
    "print(f\"Train data first 10 SO: {so_starts_train_stft_downsampled[:10]}\")\n",
    "print(f\"Train labels first 10 epochs: {epoch_labels_train_stft_downsampled_moderate[:10]}\")\n",
    "\n",
    "# Test set\n",
    "\n",
    "epoch_labels_test_stft_downsampled_moderate = label_so_epochs_moderate(epochs_test_stft_downsampled, so_starts_test_stft_downsampled, so_ends_test_stft_downsampled)\n",
    "\n",
    "print(f\"\\nTest data first 10 SO: {so_starts_test_stft_downsampled[:10]}\")\n",
    "print(f\"Test labels first 10 epochs: {epoch_labels_test_stft_downsampled_moderate[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820b130-162a-44e8-8faf-a3cab66a29f5",
   "metadata": {},
   "source": [
    "### Apply STFT to each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc69efa-6440-479e-9429-bf9bb1e42394",
   "metadata": {},
   "source": [
    "##### get rid of channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c66e2f-d28a-4b0e-bfea-ff0fa49f468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_stft_downsampled = np.squeeze(epochs_train_stft_downsampled)\n",
    "epochs_test_stft_downsampled = np.squeeze(epochs_test_stft_downsampled)\n",
    "\n",
    "print(epochs_train_stft_downsampled.shape)\n",
    "print(epochs_test_stft_downsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb0681-d5d8-40c1-92b2-d2be1bf39bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_raw_downsampled.info['sfreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db23da-1b6b-44e6-90cb-977588ebb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = train_raw_downsampled.info['sfreq']  \n",
    "# smaller nperseg means higher time resolution\n",
    "# noverlap must be less than nperseg\n",
    "\n",
    "# I want to focus more on the frequency resolution\n",
    "# at the expense of the time resolution\n",
    "# for spindles, want a 1-2 Hz resolution (frequency bins every 1-2 Hz)\n",
    "# let's start with 2 Hz first \n",
    "nperseg = 50\n",
    "noverlap = nperseg // 2\n",
    "# common practice is to set noverlap to 50% of nperseg\n",
    "\n",
    "\n",
    "epochs_train_stft_transformed_downsampled = []\n",
    "\n",
    "for epoch in epochs_train_stft_downsampled:\n",
    "    f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    spectrogram = np.abs(Zxx)  \n",
    "    epochs_train_stft_transformed_downsampled.append(spectrogram)\n",
    "\n",
    "epochs_test_stft_transformed_downsampled = []\n",
    "\n",
    "for epoch in epochs_test_stft_downsampled:\n",
    "    f, t, Zxx = stft(epoch, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    spectrogram = np.abs(Zxx)  \n",
    "    epochs_test_stft_transformed_downsampled.append(spectrogram)\n",
    "    \n",
    "# convert into numpy arrays\n",
    "epochs_train_stft_transformed_downsampled = np.array(epochs_train_stft_transformed_downsampled)\n",
    "epochs_test_stft_transformed_downsampled = np.array(epochs_test_stft_transformed_downsampled)\n",
    "\n",
    "print(\"Train STFT shape:\", epochs_train_stft_transformed_downsampled.shape)\n",
    "print(\"Test STFT shape:\", epochs_test_stft_transformed_downsampled.shape)\n",
    "\n",
    "# shape is number of epochs, frequency_bins, time_bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e314f-977b-47eb-9d89-9ad8154aadfc",
   "metadata": {},
   "source": [
    "#### Define X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd860e-824e-4ca4-93ce-5c0fd60536ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y sets and reshape\n",
    "\n",
    "X_train_stft_downsampled = epochs_train_stft_transformed_downsampled[..., np.newaxis]  # Shape: (14550, 65, 25, 1)\n",
    "y_train_stft_downsampled = epoch_labels_train_stft_downsampled_moderate\n",
    "\n",
    "X_test_stft_downsampled = epochs_test_stft_transformed_downsampled[..., np.newaxis]    # Shape: (11440, 65, 25, 1)\n",
    "y_test_stft_downsampled = epoch_labels_test_stft_downsampled_moderate\n",
    "                                                                 \n",
    "\n",
    "# Print shapes\n",
    "\n",
    "print(f\"X_train shape: {X_train_stft_downsampled.shape}\")\n",
    "print(f\"y_train shape: {y_train_stft_downsampled.shape}\")\n",
    "\n",
    "print(f\"\\nX_test shape: {X_test_stft_downsampled.shape}\")\n",
    "print(f\"y_test shape: {y_test_stft_downsampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fc617-86d9-49dc-aba8-f4cacd6f24cc",
   "metadata": {},
   "source": [
    "### Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d265443-afdf-4cd0-99ae-766e191462a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this scales the spectrogram in range 0,1\n",
    "# this is min-max normalisation\n",
    "\n",
    "X_train_stft_norm_downsampled = np.array([\n",
    "    (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "    for epoch in X_train_stft_downsampled\n",
    "])\n",
    "\n",
    "X_test_stft_norm_downsampled = np.array([\n",
    "    (epoch - np.min(epoch)) / (np.max(epoch) - np.min(epoch) + 1e-8)\n",
    "    for epoch in X_test_stft_downsampled\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d977b57-4cf0-4dbe-8066-f13ec0cb596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should have values between 0 and 1 \n",
    "\n",
    "print(\"Before normalisation:\")\n",
    "print(\"Max train value:\", np.max(X_train_stft_downsampled))\n",
    "print(\"Min train value:\", np.min(X_train_stft_downsampled))\n",
    "\n",
    "print(\"Max test value:\", np.max(X_test_stft_downsampled))\n",
    "print(\"Min test value:\", np.min(X_test_stft_downsampled))\n",
    "\n",
    "print(\"\\nAfter normalisation:\")\n",
    "print(\"Max train value:\", np.max(X_train_stft_norm_downsampled))\n",
    "print(\"Min train value:\", np.min(X_train_stft_norm_downsampled))\n",
    "\n",
    "print(\"Max test value:\", np.max(X_test_stft_norm_downsampled))\n",
    "print(\"Min test value:\", np.min(X_test_stft_norm_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e1eb7-b479-4099-b2ad-0c3c9a5914eb",
   "metadata": {},
   "source": [
    "## Three-input model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888c666-12e6-44ef-a5f6-19425f2a723c",
   "metadata": {},
   "source": [
    "### Reducing STFT dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c865e-a99d-4831-88ea-6dc0c235f97c",
   "metadata": {},
   "source": [
    "#### Only keeping time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6903f75-9412-4660-b672-63879e0f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stft_time = np.mean(X_train_stft_norm_downsampled, axis=2)\n",
    "# collapses the frequency axis by averaging them\n",
    "X_test_stft_time =  np.mean(X_test_stft_norm_downsampled, axis=2)\n",
    "\n",
    "print(f\"Shape of X_train_stft_time:{X_train_stft_time.shape}\")\n",
    "print(f\"Shape of X_test_stft_time:{X_test_stft_time.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c98405-2c3b-47a7-817d-4f42b050f53f",
   "metadata": {},
   "source": [
    "#### Only keeping frequency axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed58625-1a5c-4172-8387-5b7405662724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stft_freq = np.mean(X_train_stft_norm_downsampled, axis=1) \n",
    "X_test_stft_freq = np.mean(X_test_stft_norm_downsampled, axis=1)\n",
    "\n",
    "print(f\"Shape of X_train_stft_freq:{X_train_stft_freq.shape}\")\n",
    "print(f\"Shape of X_test_stft_freq:{X_test_stft_freq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2efbd1-beff-4ad5-ad21-c965c90942ae",
   "metadata": {},
   "source": [
    "### 3-input model with frequency for the STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc48dd-b78c-4dd7-9e46-75622c13bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict_freq = {\n",
    "    'raw_input': X_train_raw_downsampled,\n",
    "    'filtered_input': X_train_filtered_downsampled,\n",
    "    'stft_input': X_train_stft_freq\n",
    "}\n",
    "\n",
    "X_test_dict_freq = {\n",
    "    'raw_input': X_test_raw_downsampled,\n",
    "    'filtered_input': X_test_filtered_downsampled,\n",
    "    'stft_input': X_test_stft_freq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf7ca7-2bce-4d4e-b5f6-edad3247142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_multi_input_freq = build_multi_input_cnn_model_freq()\n",
    "cnn_model_multi_input_freq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b0747-34b3-4403-ba88-dba2c85019c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      \n",
    "    patience=5,               \n",
    "    restore_best_weights=True \n",
    ")\n",
    "# stop after 5 epochs with no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedd25e-0d34-4cd0-a081-cd048de866ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_info_multiple_inputs_freq = cnn_model_multi_input_freq.fit(X_train_dict_freq, y_train_raw_downsampled, validation_split=0.2, epochs=20, batch_size=128, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6574b-a46a-4fe5-9a32-13ca0a98a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(training_info):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    axs[0].plot(training_info.history['loss'], label=\"training set\")\n",
    "    axs[0].plot(training_info.history['val_loss'], label=\"validation set\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "    try:\n",
    "        axs[1].plot(training_info.history['accuracy'], label=\"training set\")\n",
    "        axs[1].plot(training_info.history['val_accuracy'], label=\"validation set\")\n",
    "        axs[1].set_xlabel(\"Epoch\")\n",
    "        axs[1].set_ylabel(\"Accuracy\")\n",
    "        axs[1].grid(True)\n",
    "        axs[1].legend()\n",
    "    except:\n",
    "        pass\n",
    "  \n",
    "    fig.suptitle(\"Training History for three-input CNN model with frequency component of STFT for SO detection\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_info_multiple_inputs_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d2307-f9f5-4470-9214-8f842474357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_multi_input_freq.evaluate(X_test_dict_freq, y_test_raw_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958467bd-af0e-4f50-8cca-5a934e5a8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model_multi_input_freq.predict(X_test_dict_freq)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test_raw_downsampled, y_pred_labels))\n",
    "print(classification_report(y_test_raw_downsampled, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff917c-8e86-4c74-9475-d1612c35fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = cnn_model_multi_input_freq.predict(X_test_dict_freq)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_raw_downsampled, y_pred_labels)\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Classification report as a dataframe\n",
    "report = classification_report(y_test_raw_downsampled, y_pred_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Confusion matrix plotted as heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for SO Detection \\nusing a three-input CNN with frequency \\ncomponent of STFT\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report as a table\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=report_df.round(2).values,\n",
    "                 colLabels=report_df.columns,\n",
    "                 rowLabels=report_df.index,\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.title(\"Classification Report for SO Detection \\nusing a three-input CNN with frequency \\ncomponent of STFT\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310170a-bb25-4d93-98f4-451ddb987490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten in case y_pred has shape (n_samples, 1)\n",
    "y_pred_proba = y_pred.ravel()\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_raw_downsampled, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic for a three-input CNN model \\nwith the STFT frequency component for SO detection\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
